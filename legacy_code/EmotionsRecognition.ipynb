{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjWvnaQUrZmD"
   },
   "source": [
    "# Emotion classification using the RAVDESS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDNbxj45rkvB"
   },
   "source": [
    "# Analysis\n",
    "\n",
    "We are will first install LibROSA, a python package for music and audio analysis.\n",
    "\n",
    "After the import, we will plot the signal of the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "EgFwaDhMbJVm",
    "outputId": "d1f5d32b-177d-4858-c366-d43a6b8783ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.9.2)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.7.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: numba>=0.45.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (0.55.2)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (21.3)\n",
      "Requirement already satisfied: resampy>=0.2.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (0.3.1)\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from numba>=0.45.1->librosa) (0.38.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from numba>=0.45.1->librosa) (62.3.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from packaging>=20.0->librosa) (3.0.9)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pooch>=1.0->librosa) (2.27.1)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxI4xzngdS-e"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "from librosa import display\n",
    "\n",
    "data, sampling_rate = librosa.load('../features/Actor_01/03-01-01-01-01-01-01.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCtNuVWlr5jL"
   },
   "source": [
    "# Load all files\n",
    "\n",
    "We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AKvuF--gd6F-",
    "outputId": "4fbbbdc4-3bce-47b3-812c-1dd9e3938159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data loaded. Loading time: 652.7888643741608 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "path = '../features/'\n",
    "lst = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "  for file in files:\n",
    "      try:\n",
    "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
    "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
    "        file = int(file[7:8]) - 1 \n",
    "        arr = mfccs, file\n",
    "        lst.append(arr)\n",
    "      # If the file is not valid, skip it\n",
    "      except ValueError:\n",
    "        continue\n",
    "\n",
    "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLSggnF7kKY1"
   },
   "outputs": [],
   "source": [
    "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
    "X, y = zip(*lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VzvBRTJIlIE9",
    "outputId": "6eb806ec-f065-4420-d526-c1fa8d00c26c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2452, 40), (2452,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Agw-3KN1sDhh"
   },
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "To make a first attempt in accomplishing this classification task I chose a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-Xgb5NslTBO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UshLOC1ClWL3"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BnCR52nlXw0"
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "qWyTownblZM0",
    "outputId": "61708923-f907-442b-86d7-b3e5b2840c29"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEuw6TUQlr7C"
   },
   "outputs": [],
   "source": [
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1v0i0V7sMw7"
   },
   "source": [
    "Let's go with our classification report.\n",
    "\n",
    "Before we start, a quick reminder of the classes we are trying to predict:\n",
    "\n",
    "emotions = {\n",
    "    \"neutral\": \"0\",\n",
    "    \"calm\": \"1\",\n",
    "    \"happy\": \"2\",\n",
    "    \"sad\": \"3\",\n",
    "    \"angry\": \"4\", \n",
    "    \"fearful\": \"5\", \n",
    "    \"disgust\": \"6\", \n",
    "    \"surprised\": \"7\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "c4kNSYkAleIv",
    "outputId": "fb407f71-b7de-4a15-cee9-527a69d45c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.46      0.43        57\n",
      "           1       0.63      0.62      0.62       130\n",
      "           2       0.52      0.48      0.50       126\n",
      "           3       0.43      0.41      0.42       123\n",
      "           4       0.55      0.59      0.57       122\n",
      "           5       0.45      0.45      0.45       124\n",
      "           6       0.31      0.27      0.29        63\n",
      "           7       0.39      0.46      0.43        65\n",
      "\n",
      "    accuracy                           0.48       810\n",
      "   macro avg       0.46      0.47      0.46       810\n",
      "weighted avg       0.48      0.48      0.48       810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCVgjLj-gwE2"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jfaTxzZ1w__y"
   },
   "source": [
    "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
    "\n",
    "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcov_DCXgs7v"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eo0ljqzg-KM"
   },
   "outputs": [],
   "source": [
    "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
    "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
    "                                 n_estimators= 22000, random_state= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Tg45qSOfg-26",
    "outputId": "c31df1c1-9342-4485-b7f8-211cc4077e7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10, max_features=&#x27;log2&#x27;, max_leaf_nodes=100,\n",
       "                       min_samples_leaf=3, min_samples_split=20,\n",
       "                       n_estimators=22000, random_state=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, max_features=&#x27;log2&#x27;, max_leaf_nodes=100,\n",
       "                       min_samples_leaf=3, min_samples_split=20,\n",
       "                       n_estimators=22000, random_state=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
       "                       min_samples_leaf=3, min_samples_split=20,\n",
       "                       n_estimators=22000, random_state=5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aM8KU3qxhGBM"
   },
   "outputs": [],
   "source": [
    "predictions = rforest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "296FW5sBdanI",
    "outputId": "a9fbfcc2-f9c5-4a3d-9bc0-2a6f513ba609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.44      0.60        57\n",
      "           1       0.64      0.88      0.74       130\n",
      "           2       0.72      0.51      0.60       126\n",
      "           3       0.55      0.63      0.59       123\n",
      "           4       0.68      0.77      0.72       122\n",
      "           5       0.58      0.54      0.56       124\n",
      "           6       0.47      0.30      0.37        63\n",
      "           7       0.46      0.57      0.51        65\n",
      "\n",
      "    accuracy                           0.62       810\n",
      "   macro avg       0.63      0.58      0.59       810\n",
      "weighted avg       0.63      0.62      0.61       810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9eqMHV3S8i6"
   },
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-QscoyMxQtn"
   },
   "source": [
    "Let's build our neural network!\n",
    "\n",
    "To do so, we need to expand the dimensions of our array, adding a third one using the numpy \"expand_dims\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4i187-Pe-w5"
   },
   "outputs": [],
   "source": [
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vnvoCRX1gQCh",
    "outputId": "cc9d5f67-0c48-443c-e6b1-6798d200529e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1642, 40, 1), (810, 40, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_traincnn.shape, x_testcnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "HZOGIpuefCd3",
    "outputId": "4fe2802a-3147-4725-d79b-3c9cbe993e16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(40,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LphftMIZzUvz"
   },
   "source": [
    "With *model.summary* we can see a recap of what we have build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "pIWPB4Zgfic7",
    "outputId": "49b5d344-637a-452e-c730-76f5471ea889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 40, 128)           768       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 5128      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 87,944\n",
      "Trainable params: 87,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qQSBeBhzcLu"
   },
   "source": [
    "Now we can compile and fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "iNI1znbsfpTx",
    "outputId": "872a1dbe-5206-4d6c-a7ce-943b585f4a9e"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ktdF-nJKfq6F",
    "outputId": "a05f0852-1564-4425-8885-bcea35dd0e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "103/103 [==============================] - 1s 4ms/step - loss: 7.8388 - accuracy: 0.1255 - val_loss: 2.7198 - val_accuracy: 0.1691\n",
      "Epoch 2/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 6.4381 - accuracy: 0.1431 - val_loss: 2.5544 - val_accuracy: 0.1840\n",
      "Epoch 3/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 5.7996 - accuracy: 0.1480 - val_loss: 2.2400 - val_accuracy: 0.2086\n",
      "Epoch 4/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 5.0753 - accuracy: 0.1486 - val_loss: 2.1103 - val_accuracy: 0.2210\n",
      "Epoch 5/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 4.4553 - accuracy: 0.1644 - val_loss: 2.3932 - val_accuracy: 0.1864\n",
      "Epoch 6/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.8973 - accuracy: 0.1815 - val_loss: 2.5567 - val_accuracy: 0.1827\n",
      "Epoch 7/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.4816 - accuracy: 0.1900 - val_loss: 2.0271 - val_accuracy: 0.2778\n",
      "Epoch 8/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.0747 - accuracy: 0.1894 - val_loss: 1.9367 - val_accuracy: 0.2519\n",
      "Epoch 9/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.7887 - accuracy: 0.2077 - val_loss: 1.8353 - val_accuracy: 0.2642\n",
      "Epoch 10/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.5668 - accuracy: 0.2156 - val_loss: 1.8257 - val_accuracy: 0.2901\n",
      "Epoch 11/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.3171 - accuracy: 0.2253 - val_loss: 1.8049 - val_accuracy: 0.2519\n",
      "Epoch 12/1000\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.2182 - accuracy: 0.2375 - val_loss: 1.7885 - val_accuracy: 0.2617\n",
      "Epoch 13/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.1352 - accuracy: 0.2436 - val_loss: 1.7790 - val_accuracy: 0.2877\n",
      "Epoch 14/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.1313 - accuracy: 0.2259 - val_loss: 1.8001 - val_accuracy: 0.3173\n",
      "Epoch 15/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.0251 - accuracy: 0.2637 - val_loss: 1.8219 - val_accuracy: 0.2778\n",
      "Epoch 16/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.9998 - accuracy: 0.2412 - val_loss: 1.7834 - val_accuracy: 0.2951\n",
      "Epoch 17/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.9264 - accuracy: 0.2680 - val_loss: 1.7686 - val_accuracy: 0.3111\n",
      "Epoch 18/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.9203 - accuracy: 0.2710 - val_loss: 1.7676 - val_accuracy: 0.3506\n",
      "Epoch 19/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.8872 - accuracy: 0.2771 - val_loss: 1.7459 - val_accuracy: 0.3728\n",
      "Epoch 20/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.8637 - accuracy: 0.2686 - val_loss: 1.7279 - val_accuracy: 0.3827\n",
      "Epoch 21/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.8376 - accuracy: 0.2838 - val_loss: 1.7110 - val_accuracy: 0.3765\n",
      "Epoch 22/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.8141 - accuracy: 0.3112 - val_loss: 1.7228 - val_accuracy: 0.3457\n",
      "Epoch 23/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.7872 - accuracy: 0.3106 - val_loss: 1.7107 - val_accuracy: 0.3741\n",
      "Epoch 24/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.7930 - accuracy: 0.3167 - val_loss: 1.6920 - val_accuracy: 0.3889\n",
      "Epoch 25/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.7683 - accuracy: 0.3009 - val_loss: 1.6730 - val_accuracy: 0.3778\n",
      "Epoch 26/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.7644 - accuracy: 0.3210 - val_loss: 1.6612 - val_accuracy: 0.3753\n",
      "Epoch 27/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.7457 - accuracy: 0.3289 - val_loss: 1.6509 - val_accuracy: 0.4037\n",
      "Epoch 28/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.7360 - accuracy: 0.3465 - val_loss: 1.6573 - val_accuracy: 0.3852\n",
      "Epoch 29/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.7293 - accuracy: 0.3252 - val_loss: 1.6405 - val_accuracy: 0.3975\n",
      "Epoch 30/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.6991 - accuracy: 0.3392 - val_loss: 1.6346 - val_accuracy: 0.4235\n",
      "Epoch 31/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.7078 - accuracy: 0.3557 - val_loss: 1.6091 - val_accuracy: 0.4148\n",
      "Epoch 32/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.6858 - accuracy: 0.3672 - val_loss: 1.5919 - val_accuracy: 0.4296\n",
      "Epoch 33/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.6721 - accuracy: 0.3453 - val_loss: 1.6011 - val_accuracy: 0.4136\n",
      "Epoch 34/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.6440 - accuracy: 0.3715 - val_loss: 1.5884 - val_accuracy: 0.4395\n",
      "Epoch 35/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.6721 - accuracy: 0.3642 - val_loss: 1.5900 - val_accuracy: 0.3963\n",
      "Epoch 36/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.6400 - accuracy: 0.3788 - val_loss: 1.5758 - val_accuracy: 0.4494\n",
      "Epoch 37/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.6252 - accuracy: 0.3855 - val_loss: 1.5780 - val_accuracy: 0.4333\n",
      "Epoch 38/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.5991 - accuracy: 0.3837 - val_loss: 1.5414 - val_accuracy: 0.4432\n",
      "Epoch 39/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.6121 - accuracy: 0.3959 - val_loss: 1.5332 - val_accuracy: 0.4481\n",
      "Epoch 40/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.6007 - accuracy: 0.3916 - val_loss: 1.5370 - val_accuracy: 0.4432\n",
      "Epoch 41/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.5920 - accuracy: 0.3995 - val_loss: 1.5463 - val_accuracy: 0.4617\n",
      "Epoch 42/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.5918 - accuracy: 0.4062 - val_loss: 1.5485 - val_accuracy: 0.4568\n",
      "Epoch 43/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.5714 - accuracy: 0.4062 - val_loss: 1.5222 - val_accuracy: 0.4617\n",
      "Epoch 44/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.5581 - accuracy: 0.4190 - val_loss: 1.5115 - val_accuracy: 0.4617\n",
      "Epoch 45/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.5471 - accuracy: 0.4147 - val_loss: 1.5084 - val_accuracy: 0.4667\n",
      "Epoch 46/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.5331 - accuracy: 0.4263 - val_loss: 1.4985 - val_accuracy: 0.4728\n",
      "Epoch 47/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.5198 - accuracy: 0.4166 - val_loss: 1.4811 - val_accuracy: 0.4617\n",
      "Epoch 48/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.5210 - accuracy: 0.4318 - val_loss: 1.5063 - val_accuracy: 0.4765\n",
      "Epoch 49/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4994 - accuracy: 0.4342 - val_loss: 1.4744 - val_accuracy: 0.4667\n",
      "Epoch 50/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.5317 - accuracy: 0.4184 - val_loss: 1.4872 - val_accuracy: 0.4617\n",
      "Epoch 51/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4874 - accuracy: 0.4391 - val_loss: 1.4486 - val_accuracy: 0.4765\n",
      "Epoch 52/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4925 - accuracy: 0.4421 - val_loss: 1.4454 - val_accuracy: 0.4975\n",
      "Epoch 53/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4853 - accuracy: 0.4409 - val_loss: 1.4374 - val_accuracy: 0.4728\n",
      "Epoch 54/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4720 - accuracy: 0.4470 - val_loss: 1.4313 - val_accuracy: 0.4864\n",
      "Epoch 55/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4655 - accuracy: 0.4568 - val_loss: 1.4215 - val_accuracy: 0.4877\n",
      "Epoch 56/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 1.4699 - accuracy: 0.4391 - val_loss: 1.4371 - val_accuracy: 0.4840\n",
      "Epoch 57/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4623 - accuracy: 0.4488 - val_loss: 1.4426 - val_accuracy: 0.4778\n",
      "Epoch 58/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4254 - accuracy: 0.4555 - val_loss: 1.4140 - val_accuracy: 0.4840\n",
      "Epoch 59/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4266 - accuracy: 0.4586 - val_loss: 1.4016 - val_accuracy: 0.5111\n",
      "Epoch 60/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4106 - accuracy: 0.4884 - val_loss: 1.3869 - val_accuracy: 0.4963\n",
      "Epoch 61/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4164 - accuracy: 0.4665 - val_loss: 1.3857 - val_accuracy: 0.5037\n",
      "Epoch 62/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4182 - accuracy: 0.4495 - val_loss: 1.3948 - val_accuracy: 0.4852\n",
      "Epoch 63/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4048 - accuracy: 0.4769 - val_loss: 1.3770 - val_accuracy: 0.4988\n",
      "Epoch 64/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4240 - accuracy: 0.4695 - val_loss: 1.3786 - val_accuracy: 0.5037\n",
      "Epoch 65/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.4018 - accuracy: 0.4836 - val_loss: 1.3660 - val_accuracy: 0.5049\n",
      "Epoch 66/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3944 - accuracy: 0.4775 - val_loss: 1.3528 - val_accuracy: 0.5198\n",
      "Epoch 67/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3866 - accuracy: 0.4829 - val_loss: 1.3698 - val_accuracy: 0.5123\n",
      "Epoch 68/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3756 - accuracy: 0.4805 - val_loss: 1.3573 - val_accuracy: 0.5049\n",
      "Epoch 69/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3749 - accuracy: 0.4872 - val_loss: 1.3480 - val_accuracy: 0.5148\n",
      "Epoch 70/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3550 - accuracy: 0.5043 - val_loss: 1.3533 - val_accuracy: 0.5148\n",
      "Epoch 71/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3754 - accuracy: 0.4787 - val_loss: 1.3243 - val_accuracy: 0.5272\n",
      "Epoch 72/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3595 - accuracy: 0.4720 - val_loss: 1.3342 - val_accuracy: 0.5148\n",
      "Epoch 73/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3528 - accuracy: 0.4836 - val_loss: 1.3377 - val_accuracy: 0.5111\n",
      "Epoch 74/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3433 - accuracy: 0.5104 - val_loss: 1.3454 - val_accuracy: 0.4914\n",
      "Epoch 75/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3603 - accuracy: 0.5030 - val_loss: 1.3256 - val_accuracy: 0.5185\n",
      "Epoch 76/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3274 - accuracy: 0.5250 - val_loss: 1.3364 - val_accuracy: 0.5247\n",
      "Epoch 77/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3362 - accuracy: 0.5030 - val_loss: 1.3179 - val_accuracy: 0.5247\n",
      "Epoch 78/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3218 - accuracy: 0.5030 - val_loss: 1.3130 - val_accuracy: 0.5198\n",
      "Epoch 79/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3224 - accuracy: 0.5067 - val_loss: 1.3287 - val_accuracy: 0.4901\n",
      "Epoch 80/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3379 - accuracy: 0.4945 - val_loss: 1.2957 - val_accuracy: 0.5358\n",
      "Epoch 81/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3183 - accuracy: 0.4982 - val_loss: 1.2950 - val_accuracy: 0.5222\n",
      "Epoch 82/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.3054 - accuracy: 0.5225 - val_loss: 1.2929 - val_accuracy: 0.5370\n",
      "Epoch 83/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2946 - accuracy: 0.5250 - val_loss: 1.3204 - val_accuracy: 0.5321\n",
      "Epoch 84/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2946 - accuracy: 0.5341 - val_loss: 1.3093 - val_accuracy: 0.5111\n",
      "Epoch 85/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2883 - accuracy: 0.5262 - val_loss: 1.2991 - val_accuracy: 0.5284\n",
      "Epoch 86/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2860 - accuracy: 0.5061 - val_loss: 1.2937 - val_accuracy: 0.5420\n",
      "Epoch 87/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2734 - accuracy: 0.5262 - val_loss: 1.2806 - val_accuracy: 0.5494\n",
      "Epoch 88/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2840 - accuracy: 0.5122 - val_loss: 1.2795 - val_accuracy: 0.5420\n",
      "Epoch 89/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2686 - accuracy: 0.5286 - val_loss: 1.2692 - val_accuracy: 0.5420\n",
      "Epoch 90/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2694 - accuracy: 0.5432 - val_loss: 1.2753 - val_accuracy: 0.5457\n",
      "Epoch 91/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2740 - accuracy: 0.5140 - val_loss: 1.2705 - val_accuracy: 0.5506\n",
      "Epoch 92/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2520 - accuracy: 0.5201 - val_loss: 1.2665 - val_accuracy: 0.5358\n",
      "Epoch 93/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2685 - accuracy: 0.5244 - val_loss: 1.2822 - val_accuracy: 0.5469\n",
      "Epoch 94/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 1.2649 - accuracy: 0.5359 - val_loss: 1.2513 - val_accuracy: 0.5395\n",
      "Epoch 95/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2610 - accuracy: 0.5396 - val_loss: 1.2391 - val_accuracy: 0.5630\n",
      "Epoch 96/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2462 - accuracy: 0.5487 - val_loss: 1.2438 - val_accuracy: 0.5568\n",
      "Epoch 97/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2197 - accuracy: 0.5463 - val_loss: 1.2670 - val_accuracy: 0.5346\n",
      "Epoch 98/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2268 - accuracy: 0.5445 - val_loss: 1.2503 - val_accuracy: 0.5494\n",
      "Epoch 99/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2309 - accuracy: 0.5414 - val_loss: 1.2345 - val_accuracy: 0.5605\n",
      "Epoch 100/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2294 - accuracy: 0.5402 - val_loss: 1.2326 - val_accuracy: 0.5667\n",
      "Epoch 101/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2198 - accuracy: 0.5414 - val_loss: 1.2386 - val_accuracy: 0.5605\n",
      "Epoch 102/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2159 - accuracy: 0.5463 - val_loss: 1.2263 - val_accuracy: 0.5531\n",
      "Epoch 103/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2033 - accuracy: 0.5658 - val_loss: 1.2349 - val_accuracy: 0.5444\n",
      "Epoch 104/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1988 - accuracy: 0.5579 - val_loss: 1.2109 - val_accuracy: 0.5667\n",
      "Epoch 105/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1968 - accuracy: 0.5572 - val_loss: 1.2298 - val_accuracy: 0.5531\n",
      "Epoch 106/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1987 - accuracy: 0.5585 - val_loss: 1.2402 - val_accuracy: 0.5531\n",
      "Epoch 107/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2045 - accuracy: 0.5621 - val_loss: 1.2027 - val_accuracy: 0.5753\n",
      "Epoch 108/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1892 - accuracy: 0.5548 - val_loss: 1.2098 - val_accuracy: 0.5667\n",
      "Epoch 109/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1960 - accuracy: 0.5585 - val_loss: 1.2130 - val_accuracy: 0.5654\n",
      "Epoch 110/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.2017 - accuracy: 0.5591 - val_loss: 1.2036 - val_accuracy: 0.5728\n",
      "Epoch 111/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1810 - accuracy: 0.5694 - val_loss: 1.2038 - val_accuracy: 0.5704\n",
      "Epoch 112/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1996 - accuracy: 0.5603 - val_loss: 1.2081 - val_accuracy: 0.5630\n",
      "Epoch 113/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1724 - accuracy: 0.5646 - val_loss: 1.2239 - val_accuracy: 0.5617\n",
      "Epoch 114/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1713 - accuracy: 0.5554 - val_loss: 1.2030 - val_accuracy: 0.5753\n",
      "Epoch 115/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1646 - accuracy: 0.5731 - val_loss: 1.1953 - val_accuracy: 0.5679\n",
      "Epoch 116/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1653 - accuracy: 0.5743 - val_loss: 1.2009 - val_accuracy: 0.5679\n",
      "Epoch 117/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1641 - accuracy: 0.5536 - val_loss: 1.1885 - val_accuracy: 0.5827\n",
      "Epoch 118/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1622 - accuracy: 0.5719 - val_loss: 1.2199 - val_accuracy: 0.5444\n",
      "Epoch 119/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1687 - accuracy: 0.5700 - val_loss: 1.2015 - val_accuracy: 0.5481\n",
      "Epoch 120/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1422 - accuracy: 0.5743 - val_loss: 1.1864 - val_accuracy: 0.5716\n",
      "Epoch 121/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1573 - accuracy: 0.5719 - val_loss: 1.1889 - val_accuracy: 0.5815\n",
      "Epoch 122/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1354 - accuracy: 0.5883 - val_loss: 1.1855 - val_accuracy: 0.5741\n",
      "Epoch 123/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1334 - accuracy: 0.5847 - val_loss: 1.1869 - val_accuracy: 0.5765\n",
      "Epoch 124/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1379 - accuracy: 0.5816 - val_loss: 1.1717 - val_accuracy: 0.5802\n",
      "Epoch 125/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1395 - accuracy: 0.5847 - val_loss: 1.1685 - val_accuracy: 0.5790\n",
      "Epoch 126/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1376 - accuracy: 0.5877 - val_loss: 1.1611 - val_accuracy: 0.5938\n",
      "Epoch 127/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1304 - accuracy: 0.5749 - val_loss: 1.1632 - val_accuracy: 0.5852\n",
      "Epoch 128/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1322 - accuracy: 0.5792 - val_loss: 1.1837 - val_accuracy: 0.5654\n",
      "Epoch 129/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1266 - accuracy: 0.5853 - val_loss: 1.1857 - val_accuracy: 0.5728\n",
      "Epoch 130/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1304 - accuracy: 0.5877 - val_loss: 1.1587 - val_accuracy: 0.5877\n",
      "Epoch 131/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 1.1284 - accuracy: 0.5926 - val_loss: 1.1623 - val_accuracy: 0.5926\n",
      "Epoch 132/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1078 - accuracy: 0.5999 - val_loss: 1.1825 - val_accuracy: 0.5617\n",
      "Epoch 133/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1078 - accuracy: 0.5889 - val_loss: 1.1568 - val_accuracy: 0.5852\n",
      "Epoch 134/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1094 - accuracy: 0.5987 - val_loss: 1.1980 - val_accuracy: 0.5494\n",
      "Epoch 135/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1187 - accuracy: 0.5895 - val_loss: 1.1664 - val_accuracy: 0.5827\n",
      "Epoch 136/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0943 - accuracy: 0.6005 - val_loss: 1.1554 - val_accuracy: 0.5765\n",
      "Epoch 137/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1112 - accuracy: 0.5914 - val_loss: 1.1423 - val_accuracy: 0.5864\n",
      "Epoch 138/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0878 - accuracy: 0.6066 - val_loss: 1.1473 - val_accuracy: 0.5741\n",
      "Epoch 139/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0747 - accuracy: 0.6072 - val_loss: 1.1435 - val_accuracy: 0.5926\n",
      "Epoch 140/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.1032 - accuracy: 0.5974 - val_loss: 1.1544 - val_accuracy: 0.5840\n",
      "Epoch 141/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0718 - accuracy: 0.6048 - val_loss: 1.1477 - val_accuracy: 0.5914\n",
      "Epoch 142/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0825 - accuracy: 0.5920 - val_loss: 1.1579 - val_accuracy: 0.5827\n",
      "Epoch 143/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0869 - accuracy: 0.5914 - val_loss: 1.1329 - val_accuracy: 0.5889\n",
      "Epoch 144/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0675 - accuracy: 0.6072 - val_loss: 1.1274 - val_accuracy: 0.5975\n",
      "Epoch 145/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0683 - accuracy: 0.6163 - val_loss: 1.1380 - val_accuracy: 0.5852\n",
      "Epoch 146/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0673 - accuracy: 0.6072 - val_loss: 1.1340 - val_accuracy: 0.5889\n",
      "Epoch 147/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0762 - accuracy: 0.6133 - val_loss: 1.1545 - val_accuracy: 0.5827\n",
      "Epoch 148/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0799 - accuracy: 0.5981 - val_loss: 1.1366 - val_accuracy: 0.5790\n",
      "Epoch 149/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 1.0738 - accuracy: 0.5987 - val_loss: 1.1419 - val_accuracy: 0.5951\n",
      "Epoch 150/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0508 - accuracy: 0.6041 - val_loss: 1.1336 - val_accuracy: 0.5827\n",
      "Epoch 151/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0657 - accuracy: 0.6108 - val_loss: 1.1596 - val_accuracy: 0.5778\n",
      "Epoch 152/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0726 - accuracy: 0.5993 - val_loss: 1.1459 - val_accuracy: 0.5753\n",
      "Epoch 153/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0505 - accuracy: 0.6084 - val_loss: 1.1248 - val_accuracy: 0.5889\n",
      "Epoch 154/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0553 - accuracy: 0.6090 - val_loss: 1.1438 - val_accuracy: 0.5852\n",
      "Epoch 155/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0506 - accuracy: 0.6121 - val_loss: 1.1155 - val_accuracy: 0.5877\n",
      "Epoch 156/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0341 - accuracy: 0.6139 - val_loss: 1.1188 - val_accuracy: 0.5951\n",
      "Epoch 157/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0660 - accuracy: 0.6127 - val_loss: 1.1135 - val_accuracy: 0.6000\n",
      "Epoch 158/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0541 - accuracy: 0.6096 - val_loss: 1.1459 - val_accuracy: 0.5827\n",
      "Epoch 159/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0289 - accuracy: 0.6309 - val_loss: 1.1040 - val_accuracy: 0.5889\n",
      "Epoch 160/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0298 - accuracy: 0.6285 - val_loss: 1.1148 - val_accuracy: 0.5877\n",
      "Epoch 161/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0454 - accuracy: 0.6139 - val_loss: 1.1094 - val_accuracy: 0.5938\n",
      "Epoch 162/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0306 - accuracy: 0.6212 - val_loss: 1.1035 - val_accuracy: 0.5901\n",
      "Epoch 163/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0494 - accuracy: 0.6291 - val_loss: 1.1173 - val_accuracy: 0.5840\n",
      "Epoch 164/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0147 - accuracy: 0.6303 - val_loss: 1.0990 - val_accuracy: 0.5988\n",
      "Epoch 165/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0189 - accuracy: 0.6169 - val_loss: 1.1141 - val_accuracy: 0.5975\n",
      "Epoch 166/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0266 - accuracy: 0.6328 - val_loss: 1.1058 - val_accuracy: 0.5975\n",
      "Epoch 167/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 1.0134 - accuracy: 0.6145 - val_loss: 1.1101 - val_accuracy: 0.5951\n",
      "Epoch 168/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 1.0263 - accuracy: 0.6261 - val_loss: 1.0957 - val_accuracy: 0.6000\n",
      "Epoch 169/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0179 - accuracy: 0.6358 - val_loss: 1.1072 - val_accuracy: 0.6123\n",
      "Epoch 170/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0223 - accuracy: 0.6175 - val_loss: 1.1014 - val_accuracy: 0.5901\n",
      "Epoch 171/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0087 - accuracy: 0.6309 - val_loss: 1.1053 - val_accuracy: 0.5988\n",
      "Epoch 172/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0074 - accuracy: 0.6303 - val_loss: 1.1173 - val_accuracy: 0.5951\n",
      "Epoch 173/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 1.0162 - accuracy: 0.6346 - val_loss: 1.0850 - val_accuracy: 0.6123\n",
      "Epoch 174/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9949 - accuracy: 0.6468 - val_loss: 1.0910 - val_accuracy: 0.6049\n",
      "Epoch 175/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 1.0016 - accuracy: 0.6309 - val_loss: 1.0896 - val_accuracy: 0.6025\n",
      "Epoch 176/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9910 - accuracy: 0.6364 - val_loss: 1.1033 - val_accuracy: 0.5840\n",
      "Epoch 177/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9843 - accuracy: 0.6407 - val_loss: 1.0994 - val_accuracy: 0.6099\n",
      "Epoch 178/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9998 - accuracy: 0.6364 - val_loss: 1.0858 - val_accuracy: 0.6062\n",
      "Epoch 179/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9861 - accuracy: 0.6401 - val_loss: 1.0937 - val_accuracy: 0.5975\n",
      "Epoch 180/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9813 - accuracy: 0.6425 - val_loss: 1.0784 - val_accuracy: 0.6111\n",
      "Epoch 181/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9753 - accuracy: 0.6565 - val_loss: 1.0852 - val_accuracy: 0.6049\n",
      "Epoch 182/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9701 - accuracy: 0.6486 - val_loss: 1.0821 - val_accuracy: 0.5988\n",
      "Epoch 183/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9773 - accuracy: 0.6431 - val_loss: 1.0736 - val_accuracy: 0.6074\n",
      "Epoch 184/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9768 - accuracy: 0.6480 - val_loss: 1.1095 - val_accuracy: 0.5988\n",
      "Epoch 185/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9641 - accuracy: 0.6510 - val_loss: 1.0808 - val_accuracy: 0.5951\n",
      "Epoch 186/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9770 - accuracy: 0.6364 - val_loss: 1.0638 - val_accuracy: 0.6037\n",
      "Epoch 187/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9637 - accuracy: 0.6486 - val_loss: 1.0827 - val_accuracy: 0.6049\n",
      "Epoch 188/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9611 - accuracy: 0.6541 - val_loss: 1.0721 - val_accuracy: 0.6198\n",
      "Epoch 189/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9564 - accuracy: 0.6559 - val_loss: 1.0682 - val_accuracy: 0.6062\n",
      "Epoch 190/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9487 - accuracy: 0.6608 - val_loss: 1.0707 - val_accuracy: 0.5975\n",
      "Epoch 191/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9575 - accuracy: 0.6480 - val_loss: 1.0728 - val_accuracy: 0.5926\n",
      "Epoch 192/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9589 - accuracy: 0.6498 - val_loss: 1.0636 - val_accuracy: 0.6086\n",
      "Epoch 193/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9520 - accuracy: 0.6626 - val_loss: 1.0740 - val_accuracy: 0.6086\n",
      "Epoch 194/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9355 - accuracy: 0.6620 - val_loss: 1.0690 - val_accuracy: 0.6000\n",
      "Epoch 195/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9462 - accuracy: 0.6474 - val_loss: 1.0631 - val_accuracy: 0.6086\n",
      "Epoch 196/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9348 - accuracy: 0.6504 - val_loss: 1.0535 - val_accuracy: 0.6062\n",
      "Epoch 197/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9314 - accuracy: 0.6705 - val_loss: 1.0650 - val_accuracy: 0.6123\n",
      "Epoch 198/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9381 - accuracy: 0.6632 - val_loss: 1.0609 - val_accuracy: 0.6136\n",
      "Epoch 199/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9283 - accuracy: 0.6705 - val_loss: 1.0643 - val_accuracy: 0.6000\n",
      "Epoch 200/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9291 - accuracy: 0.6638 - val_loss: 1.0626 - val_accuracy: 0.6025\n",
      "Epoch 201/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9367 - accuracy: 0.6516 - val_loss: 1.0684 - val_accuracy: 0.6037\n",
      "Epoch 202/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9361 - accuracy: 0.6614 - val_loss: 1.0648 - val_accuracy: 0.6086\n",
      "Epoch 203/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9336 - accuracy: 0.6663 - val_loss: 1.0643 - val_accuracy: 0.6136\n",
      "Epoch 204/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9300 - accuracy: 0.6748 - val_loss: 1.0542 - val_accuracy: 0.5975\n",
      "Epoch 205/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9417 - accuracy: 0.6602 - val_loss: 1.0464 - val_accuracy: 0.6309\n",
      "Epoch 206/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9405 - accuracy: 0.6571 - val_loss: 1.0767 - val_accuracy: 0.6012\n",
      "Epoch 207/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9167 - accuracy: 0.6675 - val_loss: 1.0497 - val_accuracy: 0.6259\n",
      "Epoch 208/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9280 - accuracy: 0.6644 - val_loss: 1.0462 - val_accuracy: 0.6136\n",
      "Epoch 209/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9234 - accuracy: 0.6669 - val_loss: 1.0475 - val_accuracy: 0.6222\n",
      "Epoch 210/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9058 - accuracy: 0.6748 - val_loss: 1.0766 - val_accuracy: 0.6148\n",
      "Epoch 211/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9118 - accuracy: 0.6772 - val_loss: 1.0406 - val_accuracy: 0.6173\n",
      "Epoch 212/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9053 - accuracy: 0.6693 - val_loss: 1.0423 - val_accuracy: 0.6123\n",
      "Epoch 213/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9004 - accuracy: 0.6766 - val_loss: 1.0432 - val_accuracy: 0.6198\n",
      "Epoch 214/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8941 - accuracy: 0.6876 - val_loss: 1.0425 - val_accuracy: 0.6247\n",
      "Epoch 215/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9040 - accuracy: 0.6705 - val_loss: 1.0379 - val_accuracy: 0.6173\n",
      "Epoch 216/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8798 - accuracy: 0.6815 - val_loss: 1.0382 - val_accuracy: 0.6321\n",
      "Epoch 217/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9024 - accuracy: 0.6827 - val_loss: 1.0364 - val_accuracy: 0.6185\n",
      "Epoch 218/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8817 - accuracy: 0.6724 - val_loss: 1.0508 - val_accuracy: 0.6198\n",
      "Epoch 219/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.9065 - accuracy: 0.6717 - val_loss: 1.1035 - val_accuracy: 0.5938\n",
      "Epoch 220/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8940 - accuracy: 0.6888 - val_loss: 1.0342 - val_accuracy: 0.6136\n",
      "Epoch 221/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8825 - accuracy: 0.6888 - val_loss: 1.0313 - val_accuracy: 0.6272\n",
      "Epoch 222/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8911 - accuracy: 0.6784 - val_loss: 1.0395 - val_accuracy: 0.6185\n",
      "Epoch 223/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8860 - accuracy: 0.6894 - val_loss: 1.0381 - val_accuracy: 0.6173\n",
      "Epoch 224/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8858 - accuracy: 0.6839 - val_loss: 1.0332 - val_accuracy: 0.6210\n",
      "Epoch 225/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8674 - accuracy: 0.6900 - val_loss: 1.0294 - val_accuracy: 0.6247\n",
      "Epoch 226/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8761 - accuracy: 0.6827 - val_loss: 1.0272 - val_accuracy: 0.6173\n",
      "Epoch 227/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8775 - accuracy: 0.6827 - val_loss: 1.0318 - val_accuracy: 0.6235\n",
      "Epoch 228/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8922 - accuracy: 0.6845 - val_loss: 1.0262 - val_accuracy: 0.6148\n",
      "Epoch 229/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8831 - accuracy: 0.6876 - val_loss: 1.0436 - val_accuracy: 0.6086\n",
      "Epoch 230/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8719 - accuracy: 0.7022 - val_loss: 1.0639 - val_accuracy: 0.5926\n",
      "Epoch 231/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8935 - accuracy: 0.6644 - val_loss: 1.0470 - val_accuracy: 0.6025\n",
      "Epoch 232/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8888 - accuracy: 0.6827 - val_loss: 1.0374 - val_accuracy: 0.6198\n",
      "Epoch 233/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8663 - accuracy: 0.7040 - val_loss: 1.0100 - val_accuracy: 0.6309\n",
      "Epoch 234/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8596 - accuracy: 0.6851 - val_loss: 1.0289 - val_accuracy: 0.6086\n",
      "Epoch 235/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8538 - accuracy: 0.6888 - val_loss: 1.0147 - val_accuracy: 0.6309\n",
      "Epoch 236/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8494 - accuracy: 0.6937 - val_loss: 1.0404 - val_accuracy: 0.6136\n",
      "Epoch 237/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8642 - accuracy: 0.6882 - val_loss: 1.0212 - val_accuracy: 0.6259\n",
      "Epoch 238/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.8553 - accuracy: 0.6900 - val_loss: 1.0387 - val_accuracy: 0.6074\n",
      "Epoch 239/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8548 - accuracy: 0.6924 - val_loss: 1.0150 - val_accuracy: 0.6235\n",
      "Epoch 240/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8529 - accuracy: 0.6851 - val_loss: 1.0235 - val_accuracy: 0.6111\n",
      "Epoch 241/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.8581 - accuracy: 0.7010 - val_loss: 1.0207 - val_accuracy: 0.6210\n",
      "Epoch 242/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8404 - accuracy: 0.7010 - val_loss: 1.0042 - val_accuracy: 0.6346\n",
      "Epoch 243/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8583 - accuracy: 0.6815 - val_loss: 1.0003 - val_accuracy: 0.6296\n",
      "Epoch 244/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8497 - accuracy: 0.6973 - val_loss: 1.0126 - val_accuracy: 0.6185\n",
      "Epoch 245/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8425 - accuracy: 0.6973 - val_loss: 1.0121 - val_accuracy: 0.6259\n",
      "Epoch 246/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8316 - accuracy: 0.7119 - val_loss: 0.9959 - val_accuracy: 0.6272\n",
      "Epoch 247/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8164 - accuracy: 0.6943 - val_loss: 1.0302 - val_accuracy: 0.6148\n",
      "Epoch 248/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.8339 - accuracy: 0.7022 - val_loss: 1.0191 - val_accuracy: 0.6296\n",
      "Epoch 249/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.8379 - accuracy: 0.6888 - val_loss: 1.0073 - val_accuracy: 0.6383\n",
      "Epoch 250/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8448 - accuracy: 0.6979 - val_loss: 1.0050 - val_accuracy: 0.6272\n",
      "Epoch 251/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8359 - accuracy: 0.7022 - val_loss: 1.0406 - val_accuracy: 0.6173\n",
      "Epoch 252/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.8369 - accuracy: 0.7004 - val_loss: 0.9991 - val_accuracy: 0.6272\n",
      "Epoch 253/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8206 - accuracy: 0.7071 - val_loss: 1.0114 - val_accuracy: 0.6247\n",
      "Epoch 254/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8309 - accuracy: 0.6955 - val_loss: 1.0072 - val_accuracy: 0.6333\n",
      "Epoch 255/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.8142 - accuracy: 0.7077 - val_loss: 1.0080 - val_accuracy: 0.6247\n",
      "Epoch 256/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.8140 - accuracy: 0.7004 - val_loss: 1.0040 - val_accuracy: 0.6358\n",
      "Epoch 257/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.8147 - accuracy: 0.7107 - val_loss: 1.0128 - val_accuracy: 0.6235\n",
      "Epoch 258/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.8332 - accuracy: 0.6894 - val_loss: 0.9931 - val_accuracy: 0.6222\n",
      "Epoch 259/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.8206 - accuracy: 0.7095 - val_loss: 1.0086 - val_accuracy: 0.6160\n",
      "Epoch 260/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.8183 - accuracy: 0.7065 - val_loss: 0.9887 - val_accuracy: 0.6407\n",
      "Epoch 261/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.8192 - accuracy: 0.7119 - val_loss: 0.9966 - val_accuracy: 0.6494\n",
      "Epoch 262/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.8091 - accuracy: 0.7089 - val_loss: 0.9930 - val_accuracy: 0.6395\n",
      "Epoch 263/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8248 - accuracy: 0.7077 - val_loss: 0.9910 - val_accuracy: 0.6235\n",
      "Epoch 264/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8067 - accuracy: 0.7034 - val_loss: 0.9936 - val_accuracy: 0.6346\n",
      "Epoch 265/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8012 - accuracy: 0.7077 - val_loss: 0.9884 - val_accuracy: 0.6370\n",
      "Epoch 266/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.7940 - accuracy: 0.7223 - val_loss: 1.0118 - val_accuracy: 0.6198\n",
      "Epoch 267/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.8077 - accuracy: 0.7125 - val_loss: 0.9988 - val_accuracy: 0.6235\n",
      "Epoch 268/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.8004 - accuracy: 0.7186 - val_loss: 0.9907 - val_accuracy: 0.6210\n",
      "Epoch 269/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7979 - accuracy: 0.7150 - val_loss: 1.0029 - val_accuracy: 0.6247\n",
      "Epoch 270/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.8070 - accuracy: 0.7156 - val_loss: 0.9903 - val_accuracy: 0.6235\n",
      "Epoch 271/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7828 - accuracy: 0.7290 - val_loss: 0.9817 - val_accuracy: 0.6395\n",
      "Epoch 272/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.7967 - accuracy: 0.7107 - val_loss: 1.0026 - val_accuracy: 0.6272\n",
      "Epoch 273/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.7950 - accuracy: 0.6955 - val_loss: 1.0056 - val_accuracy: 0.6160\n",
      "Epoch 274/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7882 - accuracy: 0.7107 - val_loss: 1.0098 - val_accuracy: 0.6210\n",
      "Epoch 275/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.8039 - accuracy: 0.7132 - val_loss: 0.9896 - val_accuracy: 0.6321\n",
      "Epoch 276/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.7910 - accuracy: 0.7217 - val_loss: 0.9935 - val_accuracy: 0.6370\n",
      "Epoch 277/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7856 - accuracy: 0.7077 - val_loss: 0.9969 - val_accuracy: 0.6432\n",
      "Epoch 278/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.7825 - accuracy: 0.7162 - val_loss: 1.0321 - val_accuracy: 0.6235\n",
      "Epoch 279/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7849 - accuracy: 0.7259 - val_loss: 1.0015 - val_accuracy: 0.6235\n",
      "Epoch 280/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.7822 - accuracy: 0.7132 - val_loss: 0.9918 - val_accuracy: 0.6210\n",
      "Epoch 281/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7851 - accuracy: 0.7223 - val_loss: 0.9891 - val_accuracy: 0.6333\n",
      "Epoch 282/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7803 - accuracy: 0.7241 - val_loss: 0.9997 - val_accuracy: 0.6346\n",
      "Epoch 283/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.7751 - accuracy: 0.7199 - val_loss: 0.9990 - val_accuracy: 0.6259\n",
      "Epoch 284/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.7819 - accuracy: 0.7333 - val_loss: 0.9893 - val_accuracy: 0.6259\n",
      "Epoch 285/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.7844 - accuracy: 0.7247 - val_loss: 0.9934 - val_accuracy: 0.6321\n",
      "Epoch 286/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7709 - accuracy: 0.7290 - val_loss: 0.9998 - val_accuracy: 0.6309\n",
      "Epoch 287/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7590 - accuracy: 0.7290 - val_loss: 0.9930 - val_accuracy: 0.6210\n",
      "Epoch 288/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7816 - accuracy: 0.7266 - val_loss: 0.9900 - val_accuracy: 0.6284\n",
      "Epoch 289/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7885 - accuracy: 0.7125 - val_loss: 0.9744 - val_accuracy: 0.6309\n",
      "Epoch 290/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.7737 - accuracy: 0.7247 - val_loss: 0.9810 - val_accuracy: 0.6407\n",
      "Epoch 291/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7549 - accuracy: 0.7375 - val_loss: 0.9791 - val_accuracy: 0.6321\n",
      "Epoch 292/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7569 - accuracy: 0.7381 - val_loss: 0.9779 - val_accuracy: 0.6358\n",
      "Epoch 293/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7574 - accuracy: 0.7199 - val_loss: 1.0004 - val_accuracy: 0.6247\n",
      "Epoch 294/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7763 - accuracy: 0.7247 - val_loss: 0.9761 - val_accuracy: 0.6346\n",
      "Epoch 295/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7709 - accuracy: 0.7223 - val_loss: 0.9720 - val_accuracy: 0.6284\n",
      "Epoch 296/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.7547 - accuracy: 0.7314 - val_loss: 0.9862 - val_accuracy: 0.6395\n",
      "Epoch 297/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.7630 - accuracy: 0.7290 - val_loss: 0.9810 - val_accuracy: 0.6383\n",
      "Epoch 298/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7483 - accuracy: 0.7333 - val_loss: 0.9897 - val_accuracy: 0.6247\n",
      "Epoch 299/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7603 - accuracy: 0.7259 - val_loss: 0.9803 - val_accuracy: 0.6259\n",
      "Epoch 300/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7517 - accuracy: 0.7211 - val_loss: 0.9760 - val_accuracy: 0.6395\n",
      "Epoch 301/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7422 - accuracy: 0.7454 - val_loss: 0.9848 - val_accuracy: 0.6321\n",
      "Epoch 302/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7481 - accuracy: 0.7326 - val_loss: 0.9787 - val_accuracy: 0.6309\n",
      "Epoch 303/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7398 - accuracy: 0.7369 - val_loss: 0.9740 - val_accuracy: 0.6309\n",
      "Epoch 304/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7411 - accuracy: 0.7253 - val_loss: 0.9868 - val_accuracy: 0.6284\n",
      "Epoch 305/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7567 - accuracy: 0.7284 - val_loss: 0.9710 - val_accuracy: 0.6457\n",
      "Epoch 306/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7266 - accuracy: 0.7497 - val_loss: 1.0052 - val_accuracy: 0.6185\n",
      "Epoch 307/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.7342 - accuracy: 0.7387 - val_loss: 0.9592 - val_accuracy: 0.6469\n",
      "Epoch 308/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.7378 - accuracy: 0.7333 - val_loss: 0.9787 - val_accuracy: 0.6420\n",
      "Epoch 309/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.7307 - accuracy: 0.7333 - val_loss: 0.9785 - val_accuracy: 0.6420\n",
      "Epoch 310/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.7318 - accuracy: 0.7333 - val_loss: 0.9585 - val_accuracy: 0.6383\n",
      "Epoch 311/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.7251 - accuracy: 0.7442 - val_loss: 0.9745 - val_accuracy: 0.6235\n",
      "Epoch 312/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.7356 - accuracy: 0.7247 - val_loss: 0.9660 - val_accuracy: 0.6284\n",
      "Epoch 313/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7227 - accuracy: 0.7454 - val_loss: 0.9820 - val_accuracy: 0.6309\n",
      "Epoch 314/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7349 - accuracy: 0.7326 - val_loss: 0.9709 - val_accuracy: 0.6457\n",
      "Epoch 315/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7213 - accuracy: 0.7509 - val_loss: 0.9669 - val_accuracy: 0.6531\n",
      "Epoch 316/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7340 - accuracy: 0.7412 - val_loss: 0.9787 - val_accuracy: 0.6420\n",
      "Epoch 317/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7275 - accuracy: 0.7418 - val_loss: 0.9557 - val_accuracy: 0.6333\n",
      "Epoch 318/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7076 - accuracy: 0.7430 - val_loss: 0.9881 - val_accuracy: 0.6309\n",
      "Epoch 319/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7179 - accuracy: 0.7485 - val_loss: 0.9724 - val_accuracy: 0.6469\n",
      "Epoch 320/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7269 - accuracy: 0.7333 - val_loss: 0.9632 - val_accuracy: 0.6272\n",
      "Epoch 321/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7190 - accuracy: 0.7448 - val_loss: 0.9766 - val_accuracy: 0.6395\n",
      "Epoch 322/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7379 - accuracy: 0.7375 - val_loss: 0.9675 - val_accuracy: 0.6420\n",
      "Epoch 323/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.7588 - val_loss: 0.9900 - val_accuracy: 0.6321\n",
      "Epoch 324/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.7070 - accuracy: 0.7521 - val_loss: 0.9501 - val_accuracy: 0.6420\n",
      "Epoch 325/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7251 - accuracy: 0.7387 - val_loss: 0.9595 - val_accuracy: 0.6457\n",
      "Epoch 326/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7045 - accuracy: 0.7479 - val_loss: 0.9438 - val_accuracy: 0.6531\n",
      "Epoch 327/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7218 - accuracy: 0.7412 - val_loss: 0.9697 - val_accuracy: 0.6494\n",
      "Epoch 328/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.7165 - accuracy: 0.7369 - val_loss: 0.9455 - val_accuracy: 0.6432\n",
      "Epoch 329/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7051 - accuracy: 0.7552 - val_loss: 0.9548 - val_accuracy: 0.6568\n",
      "Epoch 330/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6993 - accuracy: 0.7503 - val_loss: 0.9473 - val_accuracy: 0.6333\n",
      "Epoch 331/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6957 - accuracy: 0.7546 - val_loss: 0.9782 - val_accuracy: 0.6383\n",
      "Epoch 332/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6989 - accuracy: 0.7479 - val_loss: 0.9664 - val_accuracy: 0.6481\n",
      "Epoch 333/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.7061 - accuracy: 0.7491 - val_loss: 0.9438 - val_accuracy: 0.6568\n",
      "Epoch 334/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6986 - accuracy: 0.7491 - val_loss: 0.9491 - val_accuracy: 0.6469\n",
      "Epoch 335/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6990 - accuracy: 0.7546 - val_loss: 0.9680 - val_accuracy: 0.6432\n",
      "Epoch 336/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.7521 - val_loss: 0.9582 - val_accuracy: 0.6420\n",
      "Epoch 337/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6889 - accuracy: 0.7649 - val_loss: 0.9517 - val_accuracy: 0.6407\n",
      "Epoch 338/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7023 - accuracy: 0.7497 - val_loss: 0.9413 - val_accuracy: 0.6494\n",
      "Epoch 339/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6824 - accuracy: 0.7600 - val_loss: 0.9625 - val_accuracy: 0.6346\n",
      "Epoch 340/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6967 - accuracy: 0.7442 - val_loss: 0.9740 - val_accuracy: 0.6358\n",
      "Epoch 341/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.7558 - val_loss: 0.9406 - val_accuracy: 0.6556\n",
      "Epoch 342/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6656 - accuracy: 0.7619 - val_loss: 0.9505 - val_accuracy: 0.6506\n",
      "Epoch 343/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.6961 - accuracy: 0.7661 - val_loss: 0.9586 - val_accuracy: 0.6506\n",
      "Epoch 344/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6796 - accuracy: 0.7613 - val_loss: 0.9552 - val_accuracy: 0.6457\n",
      "Epoch 345/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.7515 - val_loss: 0.9752 - val_accuracy: 0.6370\n",
      "Epoch 346/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6846 - accuracy: 0.7613 - val_loss: 0.9842 - val_accuracy: 0.6333\n",
      "Epoch 347/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6761 - accuracy: 0.7613 - val_loss: 0.9578 - val_accuracy: 0.6383\n",
      "Epoch 348/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.7570 - val_loss: 0.9382 - val_accuracy: 0.6395\n",
      "Epoch 349/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6727 - accuracy: 0.7497 - val_loss: 0.9509 - val_accuracy: 0.6457\n",
      "Epoch 350/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.7649 - val_loss: 0.9473 - val_accuracy: 0.6395\n",
      "Epoch 351/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.6660 - accuracy: 0.7649 - val_loss: 0.9465 - val_accuracy: 0.6432\n",
      "Epoch 352/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6894 - accuracy: 0.7503 - val_loss: 0.9591 - val_accuracy: 0.6457\n",
      "Epoch 353/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6879 - accuracy: 0.7424 - val_loss: 0.9559 - val_accuracy: 0.6469\n",
      "Epoch 354/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6680 - accuracy: 0.7643 - val_loss: 0.9504 - val_accuracy: 0.6420\n",
      "Epoch 355/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6693 - accuracy: 0.7546 - val_loss: 0.9888 - val_accuracy: 0.6235\n",
      "Epoch 356/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6602 - accuracy: 0.7570 - val_loss: 0.9216 - val_accuracy: 0.6519\n",
      "Epoch 357/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6660 - accuracy: 0.7661 - val_loss: 0.9234 - val_accuracy: 0.6593\n",
      "Epoch 358/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.7655 - val_loss: 0.9486 - val_accuracy: 0.6481\n",
      "Epoch 359/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6611 - accuracy: 0.7655 - val_loss: 0.9510 - val_accuracy: 0.6506\n",
      "Epoch 360/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6564 - accuracy: 0.7600 - val_loss: 0.9289 - val_accuracy: 0.6407\n",
      "Epoch 361/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6655 - accuracy: 0.7631 - val_loss: 0.9398 - val_accuracy: 0.6432\n",
      "Epoch 362/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6566 - accuracy: 0.7667 - val_loss: 0.9481 - val_accuracy: 0.6358\n",
      "Epoch 363/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6704 - accuracy: 0.7741 - val_loss: 0.9253 - val_accuracy: 0.6519\n",
      "Epoch 364/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6646 - accuracy: 0.7734 - val_loss: 0.9545 - val_accuracy: 0.6432\n",
      "Epoch 365/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6472 - accuracy: 0.7692 - val_loss: 0.9405 - val_accuracy: 0.6296\n",
      "Epoch 366/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6645 - accuracy: 0.7540 - val_loss: 0.9477 - val_accuracy: 0.6358\n",
      "Epoch 367/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6437 - accuracy: 0.7704 - val_loss: 0.9554 - val_accuracy: 0.6358\n",
      "Epoch 368/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6434 - accuracy: 0.7716 - val_loss: 0.9437 - val_accuracy: 0.6444\n",
      "Epoch 369/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6474 - accuracy: 0.7674 - val_loss: 0.9505 - val_accuracy: 0.6506\n",
      "Epoch 370/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6414 - accuracy: 0.7667 - val_loss: 0.9539 - val_accuracy: 0.6420\n",
      "Epoch 371/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6650 - accuracy: 0.7661 - val_loss: 0.9351 - val_accuracy: 0.6543\n",
      "Epoch 372/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6375 - accuracy: 0.7728 - val_loss: 0.9388 - val_accuracy: 0.6432\n",
      "Epoch 373/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6486 - accuracy: 0.7759 - val_loss: 0.9377 - val_accuracy: 0.6617\n",
      "Epoch 374/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6462 - accuracy: 0.7832 - val_loss: 0.9383 - val_accuracy: 0.6383\n",
      "Epoch 375/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6446 - accuracy: 0.7747 - val_loss: 0.9497 - val_accuracy: 0.6309\n",
      "Epoch 376/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6503 - accuracy: 0.7741 - val_loss: 0.9323 - val_accuracy: 0.6531\n",
      "Epoch 377/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6196 - accuracy: 0.7808 - val_loss: 0.9452 - val_accuracy: 0.6457\n",
      "Epoch 378/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6332 - accuracy: 0.7728 - val_loss: 0.9332 - val_accuracy: 0.6642\n",
      "Epoch 379/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.6337 - accuracy: 0.7808 - val_loss: 0.9585 - val_accuracy: 0.6432\n",
      "Epoch 380/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6278 - accuracy: 0.7868 - val_loss: 0.9268 - val_accuracy: 0.6543\n",
      "Epoch 381/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6244 - accuracy: 0.7674 - val_loss: 0.9466 - val_accuracy: 0.6358\n",
      "Epoch 382/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6178 - accuracy: 0.7856 - val_loss: 0.9370 - val_accuracy: 0.6457\n",
      "Epoch 383/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6335 - accuracy: 0.7747 - val_loss: 0.9589 - val_accuracy: 0.6457\n",
      "Epoch 384/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6265 - accuracy: 0.7734 - val_loss: 0.9495 - val_accuracy: 0.6580\n",
      "Epoch 385/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6264 - accuracy: 0.7862 - val_loss: 0.9460 - val_accuracy: 0.6383\n",
      "Epoch 386/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6329 - accuracy: 0.7692 - val_loss: 0.9369 - val_accuracy: 0.6432\n",
      "Epoch 387/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6104 - accuracy: 0.7856 - val_loss: 0.9176 - val_accuracy: 0.6605\n",
      "Epoch 388/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6302 - accuracy: 0.7759 - val_loss: 0.9365 - val_accuracy: 0.6519\n",
      "Epoch 389/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6187 - accuracy: 0.7765 - val_loss: 0.9368 - val_accuracy: 0.6543\n",
      "Epoch 390/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6238 - accuracy: 0.7826 - val_loss: 0.9280 - val_accuracy: 0.6432\n",
      "Epoch 391/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6157 - accuracy: 0.7808 - val_loss: 0.9221 - val_accuracy: 0.6457\n",
      "Epoch 392/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6116 - accuracy: 0.7929 - val_loss: 0.9262 - val_accuracy: 0.6679\n",
      "Epoch 393/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6293 - accuracy: 0.7789 - val_loss: 0.9422 - val_accuracy: 0.6506\n",
      "Epoch 394/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6237 - accuracy: 0.7795 - val_loss: 0.9242 - val_accuracy: 0.6593\n",
      "Epoch 395/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6162 - accuracy: 0.7923 - val_loss: 0.9322 - val_accuracy: 0.6568\n",
      "Epoch 396/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6224 - accuracy: 0.7783 - val_loss: 0.9577 - val_accuracy: 0.6432\n",
      "Epoch 397/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6154 - accuracy: 0.7759 - val_loss: 0.9314 - val_accuracy: 0.6556\n",
      "Epoch 398/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6215 - accuracy: 0.7741 - val_loss: 0.9315 - val_accuracy: 0.6593\n",
      "Epoch 399/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6109 - accuracy: 0.7820 - val_loss: 0.9239 - val_accuracy: 0.6630\n",
      "Epoch 400/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6007 - accuracy: 0.7862 - val_loss: 0.9133 - val_accuracy: 0.6593\n",
      "Epoch 401/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6164 - accuracy: 0.7698 - val_loss: 0.9165 - val_accuracy: 0.6444\n",
      "Epoch 402/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5853 - accuracy: 0.7856 - val_loss: 0.9305 - val_accuracy: 0.6469\n",
      "Epoch 403/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6168 - accuracy: 0.7777 - val_loss: 0.9606 - val_accuracy: 0.6358\n",
      "Epoch 404/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5889 - accuracy: 0.7875 - val_loss: 0.9330 - val_accuracy: 0.6407\n",
      "Epoch 405/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6010 - accuracy: 0.7832 - val_loss: 0.9231 - val_accuracy: 0.6593\n",
      "Epoch 406/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6179 - accuracy: 0.7820 - val_loss: 0.9262 - val_accuracy: 0.6543\n",
      "Epoch 407/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5847 - accuracy: 0.7984 - val_loss: 0.9362 - val_accuracy: 0.6494\n",
      "Epoch 408/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6095 - accuracy: 0.7960 - val_loss: 0.9198 - val_accuracy: 0.6506\n",
      "Epoch 409/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5977 - accuracy: 0.7856 - val_loss: 0.9171 - val_accuracy: 0.6519\n",
      "Epoch 410/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5961 - accuracy: 0.7887 - val_loss: 0.9229 - val_accuracy: 0.6457\n",
      "Epoch 411/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6088 - accuracy: 0.7887 - val_loss: 0.9353 - val_accuracy: 0.6469\n",
      "Epoch 412/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5983 - accuracy: 0.7862 - val_loss: 0.9329 - val_accuracy: 0.6654\n",
      "Epoch 413/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6154 - accuracy: 0.7996 - val_loss: 0.9349 - val_accuracy: 0.6519\n",
      "Epoch 414/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5750 - accuracy: 0.8027 - val_loss: 0.9369 - val_accuracy: 0.6543\n",
      "Epoch 415/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5961 - accuracy: 0.7972 - val_loss: 0.9089 - val_accuracy: 0.6605\n",
      "Epoch 416/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6099 - accuracy: 0.7905 - val_loss: 0.9137 - val_accuracy: 0.6642\n",
      "Epoch 417/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5850 - accuracy: 0.7820 - val_loss: 0.9308 - val_accuracy: 0.6605\n",
      "Epoch 418/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5993 - accuracy: 0.7917 - val_loss: 0.9208 - val_accuracy: 0.6642\n",
      "Epoch 419/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.6063 - accuracy: 0.7844 - val_loss: 0.9416 - val_accuracy: 0.6432\n",
      "Epoch 420/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5985 - accuracy: 0.7850 - val_loss: 0.9444 - val_accuracy: 0.6358\n",
      "Epoch 421/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5798 - accuracy: 0.7984 - val_loss: 0.9328 - val_accuracy: 0.6630\n",
      "Epoch 422/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5861 - accuracy: 0.7923 - val_loss: 0.9117 - val_accuracy: 0.6630\n",
      "Epoch 423/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5729 - accuracy: 0.7954 - val_loss: 0.9120 - val_accuracy: 0.6457\n",
      "Epoch 424/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5866 - accuracy: 0.7844 - val_loss: 0.9224 - val_accuracy: 0.6494\n",
      "Epoch 425/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5710 - accuracy: 0.7972 - val_loss: 0.9172 - val_accuracy: 0.6543\n",
      "Epoch 426/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5753 - accuracy: 0.8021 - val_loss: 0.9408 - val_accuracy: 0.6321\n",
      "Epoch 427/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5949 - accuracy: 0.7917 - val_loss: 0.9068 - val_accuracy: 0.6617\n",
      "Epoch 428/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5867 - accuracy: 0.7911 - val_loss: 0.9125 - val_accuracy: 0.6519\n",
      "Epoch 429/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5598 - accuracy: 0.8136 - val_loss: 0.9153 - val_accuracy: 0.6716\n",
      "Epoch 430/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5893 - accuracy: 0.7862 - val_loss: 0.9357 - val_accuracy: 0.6494\n",
      "Epoch 431/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5787 - accuracy: 0.8002 - val_loss: 0.9422 - val_accuracy: 0.6457\n",
      "Epoch 432/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5700 - accuracy: 0.8033 - val_loss: 0.9191 - val_accuracy: 0.6506\n",
      "Epoch 433/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5650 - accuracy: 0.8076 - val_loss: 0.9101 - val_accuracy: 0.6506\n",
      "Epoch 434/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5667 - accuracy: 0.7990 - val_loss: 0.9138 - val_accuracy: 0.6593\n",
      "Epoch 435/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.5717 - accuracy: 0.8033 - val_loss: 0.9150 - val_accuracy: 0.6568\n",
      "Epoch 436/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5757 - accuracy: 0.8021 - val_loss: 0.9358 - val_accuracy: 0.6556\n",
      "Epoch 437/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5757 - accuracy: 0.7893 - val_loss: 0.9115 - val_accuracy: 0.6593\n",
      "Epoch 438/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5623 - accuracy: 0.8088 - val_loss: 0.9174 - val_accuracy: 0.6605\n",
      "Epoch 439/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5632 - accuracy: 0.7948 - val_loss: 0.9190 - val_accuracy: 0.6481\n",
      "Epoch 440/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5705 - accuracy: 0.7881 - val_loss: 0.9605 - val_accuracy: 0.6457\n",
      "Epoch 441/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5826 - accuracy: 0.7978 - val_loss: 0.9077 - val_accuracy: 0.6617\n",
      "Epoch 442/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5518 - accuracy: 0.7996 - val_loss: 0.9313 - val_accuracy: 0.6519\n",
      "Epoch 443/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5560 - accuracy: 0.8021 - val_loss: 0.9223 - val_accuracy: 0.6580\n",
      "Epoch 444/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5639 - accuracy: 0.7996 - val_loss: 0.9238 - val_accuracy: 0.6531\n",
      "Epoch 445/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5457 - accuracy: 0.8082 - val_loss: 0.9377 - val_accuracy: 0.6531\n",
      "Epoch 446/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5574 - accuracy: 0.8130 - val_loss: 0.9315 - val_accuracy: 0.6605\n",
      "Epoch 447/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5527 - accuracy: 0.8100 - val_loss: 0.9008 - val_accuracy: 0.6654\n",
      "Epoch 448/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5643 - accuracy: 0.7954 - val_loss: 0.9190 - val_accuracy: 0.6580\n",
      "Epoch 449/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5523 - accuracy: 0.8167 - val_loss: 0.9252 - val_accuracy: 0.6630\n",
      "Epoch 450/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5458 - accuracy: 0.8094 - val_loss: 0.9252 - val_accuracy: 0.6593\n",
      "Epoch 451/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5539 - accuracy: 0.8149 - val_loss: 0.9177 - val_accuracy: 0.6519\n",
      "Epoch 452/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5559 - accuracy: 0.7978 - val_loss: 0.9096 - val_accuracy: 0.6432\n",
      "Epoch 453/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5495 - accuracy: 0.8002 - val_loss: 0.9137 - val_accuracy: 0.6667\n",
      "Epoch 454/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5404 - accuracy: 0.8051 - val_loss: 0.9173 - val_accuracy: 0.6617\n",
      "Epoch 455/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5630 - accuracy: 0.7996 - val_loss: 0.9099 - val_accuracy: 0.6691\n",
      "Epoch 456/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5529 - accuracy: 0.8057 - val_loss: 0.9071 - val_accuracy: 0.6605\n",
      "Epoch 457/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5470 - accuracy: 0.8106 - val_loss: 0.9223 - val_accuracy: 0.6568\n",
      "Epoch 458/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5451 - accuracy: 0.8051 - val_loss: 0.9099 - val_accuracy: 0.6642\n",
      "Epoch 459/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5445 - accuracy: 0.8155 - val_loss: 0.9230 - val_accuracy: 0.6617\n",
      "Epoch 460/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5337 - accuracy: 0.8027 - val_loss: 0.9085 - val_accuracy: 0.6667\n",
      "Epoch 461/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5385 - accuracy: 0.8009 - val_loss: 0.9000 - val_accuracy: 0.6654\n",
      "Epoch 462/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5331 - accuracy: 0.8143 - val_loss: 0.9018 - val_accuracy: 0.6667\n",
      "Epoch 463/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5368 - accuracy: 0.8088 - val_loss: 0.8996 - val_accuracy: 0.6642\n",
      "Epoch 464/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5392 - accuracy: 0.8222 - val_loss: 0.9025 - val_accuracy: 0.6395\n",
      "Epoch 465/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5226 - accuracy: 0.8197 - val_loss: 0.8977 - val_accuracy: 0.6630\n",
      "Epoch 466/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5496 - accuracy: 0.8033 - val_loss: 0.9093 - val_accuracy: 0.6568\n",
      "Epoch 467/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5325 - accuracy: 0.8076 - val_loss: 0.9241 - val_accuracy: 0.6667\n",
      "Epoch 468/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.5392 - accuracy: 0.8069 - val_loss: 0.8893 - val_accuracy: 0.6765\n",
      "Epoch 469/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5307 - accuracy: 0.8143 - val_loss: 0.9063 - val_accuracy: 0.6716\n",
      "Epoch 470/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5266 - accuracy: 0.8100 - val_loss: 0.9123 - val_accuracy: 0.6593\n",
      "Epoch 471/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5226 - accuracy: 0.8130 - val_loss: 0.9063 - val_accuracy: 0.6728\n",
      "Epoch 472/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5357 - accuracy: 0.8100 - val_loss: 0.9144 - val_accuracy: 0.6617\n",
      "Epoch 473/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5110 - accuracy: 0.8252 - val_loss: 0.9149 - val_accuracy: 0.6691\n",
      "Epoch 474/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5469 - accuracy: 0.8027 - val_loss: 0.8963 - val_accuracy: 0.6704\n",
      "Epoch 475/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5282 - accuracy: 0.8185 - val_loss: 0.9015 - val_accuracy: 0.6642\n",
      "Epoch 476/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5161 - accuracy: 0.8203 - val_loss: 0.9095 - val_accuracy: 0.6679\n",
      "Epoch 477/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5219 - accuracy: 0.8112 - val_loss: 0.9120 - val_accuracy: 0.6531\n",
      "Epoch 478/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5390 - accuracy: 0.8082 - val_loss: 0.9207 - val_accuracy: 0.6568\n",
      "Epoch 479/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5189 - accuracy: 0.8118 - val_loss: 0.9037 - val_accuracy: 0.6630\n",
      "Epoch 480/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5267 - accuracy: 0.8264 - val_loss: 0.9060 - val_accuracy: 0.6617\n",
      "Epoch 481/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.5163 - accuracy: 0.8246 - val_loss: 0.9073 - val_accuracy: 0.6679\n",
      "Epoch 482/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.8100 - val_loss: 0.8989 - val_accuracy: 0.6728\n",
      "Epoch 483/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5239 - accuracy: 0.8100 - val_loss: 0.8882 - val_accuracy: 0.6728\n",
      "Epoch 484/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5167 - accuracy: 0.8276 - val_loss: 0.9031 - val_accuracy: 0.6593\n",
      "Epoch 485/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5077 - accuracy: 0.8136 - val_loss: 0.8906 - val_accuracy: 0.6790\n",
      "Epoch 486/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5058 - accuracy: 0.8130 - val_loss: 0.9053 - val_accuracy: 0.6580\n",
      "Epoch 487/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5110 - accuracy: 0.8106 - val_loss: 0.9108 - val_accuracy: 0.6580\n",
      "Epoch 488/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5239 - accuracy: 0.8082 - val_loss: 0.8962 - val_accuracy: 0.6728\n",
      "Epoch 489/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5277 - accuracy: 0.8100 - val_loss: 0.9021 - val_accuracy: 0.6642\n",
      "Epoch 490/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5047 - accuracy: 0.8264 - val_loss: 0.8927 - val_accuracy: 0.6778\n",
      "Epoch 491/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4946 - accuracy: 0.8337 - val_loss: 0.8897 - val_accuracy: 0.6593\n",
      "Epoch 492/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5083 - accuracy: 0.8130 - val_loss: 0.8815 - val_accuracy: 0.6654\n",
      "Epoch 493/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4968 - accuracy: 0.8264 - val_loss: 0.8978 - val_accuracy: 0.6679\n",
      "Epoch 494/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.8234 - val_loss: 0.9059 - val_accuracy: 0.6654\n",
      "Epoch 495/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5272 - accuracy: 0.8136 - val_loss: 0.8929 - val_accuracy: 0.6840\n",
      "Epoch 496/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5206 - accuracy: 0.8234 - val_loss: 0.9258 - val_accuracy: 0.6642\n",
      "Epoch 497/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4817 - accuracy: 0.8392 - val_loss: 0.9028 - val_accuracy: 0.6691\n",
      "Epoch 498/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5102 - accuracy: 0.8216 - val_loss: 0.8847 - val_accuracy: 0.6840\n",
      "Epoch 499/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4935 - accuracy: 0.8276 - val_loss: 0.8820 - val_accuracy: 0.6778\n",
      "Epoch 500/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4829 - accuracy: 0.8386 - val_loss: 0.9065 - val_accuracy: 0.6679\n",
      "Epoch 501/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4997 - accuracy: 0.8222 - val_loss: 0.8938 - val_accuracy: 0.6728\n",
      "Epoch 502/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5185 - accuracy: 0.8100 - val_loss: 0.8896 - val_accuracy: 0.6778\n",
      "Epoch 503/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4928 - accuracy: 0.8136 - val_loss: 0.9197 - val_accuracy: 0.6494\n",
      "Epoch 504/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4785 - accuracy: 0.8319 - val_loss: 0.8947 - val_accuracy: 0.6765\n",
      "Epoch 505/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5139 - accuracy: 0.8252 - val_loss: 0.8958 - val_accuracy: 0.6716\n",
      "Epoch 506/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5142 - accuracy: 0.8210 - val_loss: 0.8975 - val_accuracy: 0.6704\n",
      "Epoch 507/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5078 - accuracy: 0.8270 - val_loss: 0.8795 - val_accuracy: 0.6765\n",
      "Epoch 508/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4802 - accuracy: 0.8356 - val_loss: 0.9342 - val_accuracy: 0.6679\n",
      "Epoch 509/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4792 - accuracy: 0.8307 - val_loss: 0.8976 - val_accuracy: 0.6716\n",
      "Epoch 510/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4960 - accuracy: 0.8191 - val_loss: 0.9038 - val_accuracy: 0.6741\n",
      "Epoch 511/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.8270 - val_loss: 0.9154 - val_accuracy: 0.6568\n",
      "Epoch 512/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4904 - accuracy: 0.8197 - val_loss: 0.8971 - val_accuracy: 0.6679\n",
      "Epoch 513/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4905 - accuracy: 0.8252 - val_loss: 0.8972 - val_accuracy: 0.6716\n",
      "Epoch 514/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4738 - accuracy: 0.8374 - val_loss: 0.8967 - val_accuracy: 0.6741\n",
      "Epoch 515/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4797 - accuracy: 0.8325 - val_loss: 0.8943 - val_accuracy: 0.6765\n",
      "Epoch 516/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.5088 - accuracy: 0.8276 - val_loss: 0.8968 - val_accuracy: 0.6679\n",
      "Epoch 517/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.4850 - accuracy: 0.8331 - val_loss: 0.9138 - val_accuracy: 0.6543\n",
      "Epoch 518/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4918 - accuracy: 0.8337 - val_loss: 0.9142 - val_accuracy: 0.6654\n",
      "Epoch 519/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4888 - accuracy: 0.8325 - val_loss: 0.8944 - val_accuracy: 0.6827\n",
      "Epoch 520/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4833 - accuracy: 0.8264 - val_loss: 0.8959 - val_accuracy: 0.6667\n",
      "Epoch 521/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4826 - accuracy: 0.8270 - val_loss: 0.8954 - val_accuracy: 0.6667\n",
      "Epoch 522/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4761 - accuracy: 0.8423 - val_loss: 0.8879 - val_accuracy: 0.6642\n",
      "Epoch 523/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4759 - accuracy: 0.8307 - val_loss: 0.9185 - val_accuracy: 0.6617\n",
      "Epoch 524/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4881 - accuracy: 0.8222 - val_loss: 0.9056 - val_accuracy: 0.6519\n",
      "Epoch 525/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4757 - accuracy: 0.8258 - val_loss: 0.9301 - val_accuracy: 0.6457\n",
      "Epoch 526/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4779 - accuracy: 0.8258 - val_loss: 0.9057 - val_accuracy: 0.6605\n",
      "Epoch 527/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4892 - accuracy: 0.8289 - val_loss: 0.9107 - val_accuracy: 0.6617\n",
      "Epoch 528/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4881 - accuracy: 0.8350 - val_loss: 0.8948 - val_accuracy: 0.6753\n",
      "Epoch 529/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4746 - accuracy: 0.8398 - val_loss: 0.9107 - val_accuracy: 0.6765\n",
      "Epoch 530/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4784 - accuracy: 0.8295 - val_loss: 0.8971 - val_accuracy: 0.6679\n",
      "Epoch 531/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4837 - accuracy: 0.8185 - val_loss: 0.9055 - val_accuracy: 0.6556\n",
      "Epoch 532/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.8423 - val_loss: 0.8990 - val_accuracy: 0.6679\n",
      "Epoch 533/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4709 - accuracy: 0.8313 - val_loss: 0.9055 - val_accuracy: 0.6790\n",
      "Epoch 534/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4743 - accuracy: 0.8392 - val_loss: 0.8876 - val_accuracy: 0.6679\n",
      "Epoch 535/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4653 - accuracy: 0.8380 - val_loss: 0.9332 - val_accuracy: 0.6481\n",
      "Epoch 536/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4902 - accuracy: 0.8276 - val_loss: 0.8929 - val_accuracy: 0.6728\n",
      "Epoch 537/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4615 - accuracy: 0.8429 - val_loss: 0.9041 - val_accuracy: 0.6679\n",
      "Epoch 538/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4615 - accuracy: 0.8276 - val_loss: 0.8898 - val_accuracy: 0.6753\n",
      "Epoch 539/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4683 - accuracy: 0.8368 - val_loss: 0.8920 - val_accuracy: 0.6593\n",
      "Epoch 540/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4503 - accuracy: 0.8459 - val_loss: 0.9147 - val_accuracy: 0.6778\n",
      "Epoch 541/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4497 - accuracy: 0.8398 - val_loss: 0.8993 - val_accuracy: 0.6753\n",
      "Epoch 542/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4593 - accuracy: 0.8307 - val_loss: 0.8979 - val_accuracy: 0.6704\n",
      "Epoch 543/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4746 - accuracy: 0.8343 - val_loss: 0.8847 - val_accuracy: 0.6704\n",
      "Epoch 544/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4432 - accuracy: 0.8471 - val_loss: 0.8943 - val_accuracy: 0.6741\n",
      "Epoch 545/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4539 - accuracy: 0.8471 - val_loss: 0.8980 - val_accuracy: 0.6741\n",
      "Epoch 546/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4720 - accuracy: 0.8276 - val_loss: 0.8951 - val_accuracy: 0.6691\n",
      "Epoch 547/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4464 - accuracy: 0.8380 - val_loss: 0.8844 - val_accuracy: 0.6889\n",
      "Epoch 548/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4551 - accuracy: 0.8356 - val_loss: 0.8935 - val_accuracy: 0.6704\n",
      "Epoch 549/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4471 - accuracy: 0.8471 - val_loss: 0.8889 - val_accuracy: 0.6741\n",
      "Epoch 550/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4632 - accuracy: 0.8423 - val_loss: 0.8837 - val_accuracy: 0.6778\n",
      "Epoch 551/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4630 - accuracy: 0.8435 - val_loss: 0.9008 - val_accuracy: 0.6753\n",
      "Epoch 552/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4660 - accuracy: 0.8410 - val_loss: 0.9115 - val_accuracy: 0.6691\n",
      "Epoch 553/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.4361 - accuracy: 0.8490 - val_loss: 0.8982 - val_accuracy: 0.6531\n",
      "Epoch 554/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4416 - accuracy: 0.8471 - val_loss: 0.8869 - val_accuracy: 0.6654\n",
      "Epoch 555/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4446 - accuracy: 0.8514 - val_loss: 0.9140 - val_accuracy: 0.6815\n",
      "Epoch 556/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4757 - accuracy: 0.8392 - val_loss: 0.8911 - val_accuracy: 0.6605\n",
      "Epoch 557/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4439 - accuracy: 0.8496 - val_loss: 0.8756 - val_accuracy: 0.6840\n",
      "Epoch 558/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4473 - accuracy: 0.8441 - val_loss: 0.8873 - val_accuracy: 0.6642\n",
      "Epoch 559/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4390 - accuracy: 0.8435 - val_loss: 0.9075 - val_accuracy: 0.6741\n",
      "Epoch 560/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4697 - accuracy: 0.8362 - val_loss: 0.8907 - val_accuracy: 0.6765\n",
      "Epoch 561/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4481 - accuracy: 0.8398 - val_loss: 0.8934 - val_accuracy: 0.6679\n",
      "Epoch 562/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4477 - accuracy: 0.8520 - val_loss: 0.8956 - val_accuracy: 0.6704\n",
      "Epoch 563/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4411 - accuracy: 0.8502 - val_loss: 0.9064 - val_accuracy: 0.6568\n",
      "Epoch 564/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4300 - accuracy: 0.8484 - val_loss: 0.8895 - val_accuracy: 0.6728\n",
      "Epoch 565/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4467 - accuracy: 0.8520 - val_loss: 0.9105 - val_accuracy: 0.6790\n",
      "Epoch 566/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.8368 - val_loss: 0.9013 - val_accuracy: 0.6691\n",
      "Epoch 567/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4429 - accuracy: 0.8459 - val_loss: 0.8929 - val_accuracy: 0.6741\n",
      "Epoch 568/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4528 - accuracy: 0.8374 - val_loss: 0.9088 - val_accuracy: 0.6728\n",
      "Epoch 569/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4334 - accuracy: 0.8484 - val_loss: 0.8803 - val_accuracy: 0.6790\n",
      "Epoch 570/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4468 - accuracy: 0.8368 - val_loss: 0.8791 - val_accuracy: 0.6679\n",
      "Epoch 571/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4615 - accuracy: 0.8368 - val_loss: 0.8938 - val_accuracy: 0.6741\n",
      "Epoch 572/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4495 - accuracy: 0.8447 - val_loss: 0.8840 - val_accuracy: 0.6741\n",
      "Epoch 573/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4407 - accuracy: 0.8404 - val_loss: 0.9001 - val_accuracy: 0.6617\n",
      "Epoch 574/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.8392 - val_loss: 0.8808 - val_accuracy: 0.6914\n",
      "Epoch 575/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4453 - accuracy: 0.8398 - val_loss: 0.9025 - val_accuracy: 0.6691\n",
      "Epoch 576/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4335 - accuracy: 0.8471 - val_loss: 0.8773 - val_accuracy: 0.6815\n",
      "Epoch 577/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4201 - accuracy: 0.8575 - val_loss: 0.9128 - val_accuracy: 0.6704\n",
      "Epoch 578/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8484 - val_loss: 0.8861 - val_accuracy: 0.6716\n",
      "Epoch 579/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4204 - accuracy: 0.8587 - val_loss: 0.8912 - val_accuracy: 0.6654\n",
      "Epoch 580/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4377 - accuracy: 0.8453 - val_loss: 0.8881 - val_accuracy: 0.6765\n",
      "Epoch 581/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4204 - accuracy: 0.8551 - val_loss: 0.9235 - val_accuracy: 0.6741\n",
      "Epoch 582/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4221 - accuracy: 0.8569 - val_loss: 0.9040 - val_accuracy: 0.6617\n",
      "Epoch 583/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4419 - accuracy: 0.8496 - val_loss: 0.8724 - val_accuracy: 0.6827\n",
      "Epoch 584/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4321 - accuracy: 0.8514 - val_loss: 0.8827 - val_accuracy: 0.6852\n",
      "Epoch 585/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4460 - accuracy: 0.8496 - val_loss: 0.8978 - val_accuracy: 0.6815\n",
      "Epoch 586/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4377 - accuracy: 0.8404 - val_loss: 0.9004 - val_accuracy: 0.6679\n",
      "Epoch 587/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4324 - accuracy: 0.8502 - val_loss: 0.8773 - val_accuracy: 0.6802\n",
      "Epoch 588/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4114 - accuracy: 0.8575 - val_loss: 0.8957 - val_accuracy: 0.6778\n",
      "Epoch 589/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4139 - accuracy: 0.8605 - val_loss: 0.8913 - val_accuracy: 0.6741\n",
      "Epoch 590/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.4146 - accuracy: 0.8636 - val_loss: 0.9240 - val_accuracy: 0.6704\n",
      "Epoch 591/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4435 - accuracy: 0.8453 - val_loss: 0.9160 - val_accuracy: 0.6519\n",
      "Epoch 592/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4193 - accuracy: 0.8624 - val_loss: 0.8912 - val_accuracy: 0.6753\n",
      "Epoch 593/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4073 - accuracy: 0.8642 - val_loss: 0.8869 - val_accuracy: 0.6691\n",
      "Epoch 594/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4099 - accuracy: 0.8557 - val_loss: 0.8852 - val_accuracy: 0.6840\n",
      "Epoch 595/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4223 - accuracy: 0.8532 - val_loss: 0.9015 - val_accuracy: 0.6765\n",
      "Epoch 596/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4340 - accuracy: 0.8502 - val_loss: 0.9043 - val_accuracy: 0.6605\n",
      "Epoch 597/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4119 - accuracy: 0.8599 - val_loss: 0.9038 - val_accuracy: 0.6778\n",
      "Epoch 598/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.4184 - accuracy: 0.8471 - val_loss: 0.8763 - val_accuracy: 0.6852\n",
      "Epoch 599/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4135 - accuracy: 0.8569 - val_loss: 0.9080 - val_accuracy: 0.6716\n",
      "Epoch 600/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4212 - accuracy: 0.8520 - val_loss: 0.8962 - val_accuracy: 0.6617\n",
      "Epoch 601/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.4154 - accuracy: 0.8593 - val_loss: 0.9052 - val_accuracy: 0.6580\n",
      "Epoch 602/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4152 - accuracy: 0.8581 - val_loss: 0.8971 - val_accuracy: 0.6704\n",
      "Epoch 603/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.4058 - accuracy: 0.8654 - val_loss: 0.8921 - val_accuracy: 0.6877\n",
      "Epoch 604/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4137 - accuracy: 0.8593 - val_loss: 0.8913 - val_accuracy: 0.6691\n",
      "Epoch 605/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4121 - accuracy: 0.8544 - val_loss: 0.8869 - val_accuracy: 0.6679\n",
      "Epoch 606/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4231 - accuracy: 0.8484 - val_loss: 0.8925 - val_accuracy: 0.6728\n",
      "Epoch 607/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4237 - accuracy: 0.8532 - val_loss: 0.8789 - val_accuracy: 0.6926\n",
      "Epoch 608/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4081 - accuracy: 0.8599 - val_loss: 0.8713 - val_accuracy: 0.6753\n",
      "Epoch 609/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4046 - accuracy: 0.8587 - val_loss: 0.9142 - val_accuracy: 0.6580\n",
      "Epoch 610/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4211 - accuracy: 0.8581 - val_loss: 0.8974 - val_accuracy: 0.6778\n",
      "Epoch 611/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4051 - accuracy: 0.8605 - val_loss: 0.9025 - val_accuracy: 0.6864\n",
      "Epoch 612/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4230 - accuracy: 0.8441 - val_loss: 0.8963 - val_accuracy: 0.6741\n",
      "Epoch 613/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.3824 - accuracy: 0.8709 - val_loss: 0.8663 - val_accuracy: 0.6852\n",
      "Epoch 614/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3985 - accuracy: 0.8630 - val_loss: 0.9207 - val_accuracy: 0.6519\n",
      "Epoch 615/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.4063 - accuracy: 0.8605 - val_loss: 0.8885 - val_accuracy: 0.6938\n",
      "Epoch 616/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4093 - accuracy: 0.8636 - val_loss: 0.9137 - val_accuracy: 0.6654\n",
      "Epoch 617/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4068 - accuracy: 0.8611 - val_loss: 0.8828 - val_accuracy: 0.6889\n",
      "Epoch 618/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4156 - accuracy: 0.8538 - val_loss: 0.8889 - val_accuracy: 0.6679\n",
      "Epoch 619/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3831 - accuracy: 0.8563 - val_loss: 0.9117 - val_accuracy: 0.6765\n",
      "Epoch 620/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.4164 - accuracy: 0.8648 - val_loss: 0.8809 - val_accuracy: 0.6901\n",
      "Epoch 621/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.3963 - accuracy: 0.8642 - val_loss: 0.9102 - val_accuracy: 0.6765\n",
      "Epoch 622/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3918 - accuracy: 0.8739 - val_loss: 0.9104 - val_accuracy: 0.6728\n",
      "Epoch 623/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3933 - accuracy: 0.8605 - val_loss: 0.8741 - val_accuracy: 0.6889\n",
      "Epoch 624/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3996 - accuracy: 0.8660 - val_loss: 0.9270 - val_accuracy: 0.6642\n",
      "Epoch 625/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3859 - accuracy: 0.8666 - val_loss: 0.8648 - val_accuracy: 0.6852\n",
      "Epoch 626/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4108 - accuracy: 0.8569 - val_loss: 0.8976 - val_accuracy: 0.6679\n",
      "Epoch 627/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4095 - accuracy: 0.8593 - val_loss: 0.8772 - val_accuracy: 0.6716\n",
      "Epoch 628/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4083 - accuracy: 0.8557 - val_loss: 0.8813 - val_accuracy: 0.6790\n",
      "Epoch 629/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3896 - accuracy: 0.8618 - val_loss: 0.8918 - val_accuracy: 0.6827\n",
      "Epoch 630/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3900 - accuracy: 0.8691 - val_loss: 0.8949 - val_accuracy: 0.6753\n",
      "Epoch 631/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4068 - accuracy: 0.8581 - val_loss: 0.8910 - val_accuracy: 0.6741\n",
      "Epoch 632/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4073 - accuracy: 0.8666 - val_loss: 0.9017 - val_accuracy: 0.6679\n",
      "Epoch 633/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8666 - val_loss: 0.8848 - val_accuracy: 0.6778\n",
      "Epoch 634/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3903 - accuracy: 0.8636 - val_loss: 0.8792 - val_accuracy: 0.6951\n",
      "Epoch 635/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3799 - accuracy: 0.8654 - val_loss: 0.8943 - val_accuracy: 0.6926\n",
      "Epoch 636/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3975 - accuracy: 0.8630 - val_loss: 0.8860 - val_accuracy: 0.6753\n",
      "Epoch 637/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3761 - accuracy: 0.8672 - val_loss: 0.8937 - val_accuracy: 0.6840\n",
      "Epoch 638/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3800 - accuracy: 0.8709 - val_loss: 0.8976 - val_accuracy: 0.6741\n",
      "Epoch 639/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3934 - accuracy: 0.8611 - val_loss: 0.9010 - val_accuracy: 0.6840\n",
      "Epoch 640/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3863 - accuracy: 0.8733 - val_loss: 0.8940 - val_accuracy: 0.6765\n",
      "Epoch 641/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3974 - accuracy: 0.8611 - val_loss: 0.8990 - val_accuracy: 0.6716\n",
      "Epoch 642/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3906 - accuracy: 0.8642 - val_loss: 0.8781 - val_accuracy: 0.6765\n",
      "Epoch 643/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.4078 - accuracy: 0.8599 - val_loss: 0.8710 - val_accuracy: 0.6852\n",
      "Epoch 644/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3856 - accuracy: 0.8557 - val_loss: 0.8730 - val_accuracy: 0.6815\n",
      "Epoch 645/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3834 - accuracy: 0.8703 - val_loss: 0.8953 - val_accuracy: 0.6864\n",
      "Epoch 646/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3856 - accuracy: 0.8581 - val_loss: 0.8731 - val_accuracy: 0.6901\n",
      "Epoch 647/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8715 - val_loss: 0.8877 - val_accuracy: 0.6778\n",
      "Epoch 648/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3845 - accuracy: 0.8630 - val_loss: 0.9005 - val_accuracy: 0.6642\n",
      "Epoch 649/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3730 - accuracy: 0.8703 - val_loss: 0.8976 - val_accuracy: 0.6716\n",
      "Epoch 650/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3685 - accuracy: 0.8733 - val_loss: 0.9095 - val_accuracy: 0.6728\n",
      "Epoch 651/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3752 - accuracy: 0.8672 - val_loss: 0.8865 - val_accuracy: 0.6864\n",
      "Epoch 652/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3806 - accuracy: 0.8715 - val_loss: 0.8837 - val_accuracy: 0.6840\n",
      "Epoch 653/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3951 - accuracy: 0.8642 - val_loss: 0.8726 - val_accuracy: 0.6790\n",
      "Epoch 654/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3856 - accuracy: 0.8703 - val_loss: 0.8860 - val_accuracy: 0.6741\n",
      "Epoch 655/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3710 - accuracy: 0.8703 - val_loss: 0.8990 - val_accuracy: 0.6889\n",
      "Epoch 656/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.3741 - accuracy: 0.8739 - val_loss: 0.9154 - val_accuracy: 0.6728\n",
      "Epoch 657/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.3674 - accuracy: 0.8745 - val_loss: 0.8955 - val_accuracy: 0.6778\n",
      "Epoch 658/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3661 - accuracy: 0.8776 - val_loss: 0.8832 - val_accuracy: 0.6901\n",
      "Epoch 659/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3512 - accuracy: 0.8825 - val_loss: 0.8809 - val_accuracy: 0.6790\n",
      "Epoch 660/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3769 - accuracy: 0.8697 - val_loss: 0.8760 - val_accuracy: 0.6716\n",
      "Epoch 661/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3767 - accuracy: 0.8770 - val_loss: 0.8943 - val_accuracy: 0.6741\n",
      "Epoch 662/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3579 - accuracy: 0.8819 - val_loss: 0.8828 - val_accuracy: 0.6914\n",
      "Epoch 663/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3766 - accuracy: 0.8691 - val_loss: 0.8827 - val_accuracy: 0.6914\n",
      "Epoch 664/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3819 - accuracy: 0.8709 - val_loss: 0.8753 - val_accuracy: 0.6815\n",
      "Epoch 665/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.8654 - val_loss: 0.8879 - val_accuracy: 0.6827\n",
      "Epoch 666/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3970 - accuracy: 0.8587 - val_loss: 0.9001 - val_accuracy: 0.6679\n",
      "Epoch 667/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3612 - accuracy: 0.8733 - val_loss: 0.8760 - val_accuracy: 0.6802\n",
      "Epoch 668/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8782 - val_loss: 0.8886 - val_accuracy: 0.6815\n",
      "Epoch 669/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3564 - accuracy: 0.8782 - val_loss: 0.9030 - val_accuracy: 0.6593\n",
      "Epoch 670/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.8758 - val_loss: 0.8784 - val_accuracy: 0.6790\n",
      "Epoch 671/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3603 - accuracy: 0.8788 - val_loss: 0.8723 - val_accuracy: 0.6951\n",
      "Epoch 672/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3837 - accuracy: 0.8636 - val_loss: 0.9031 - val_accuracy: 0.6914\n",
      "Epoch 673/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3745 - accuracy: 0.8709 - val_loss: 0.8702 - val_accuracy: 0.6889\n",
      "Epoch 674/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3584 - accuracy: 0.8697 - val_loss: 0.8921 - val_accuracy: 0.6901\n",
      "Epoch 675/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3582 - accuracy: 0.8715 - val_loss: 0.9018 - val_accuracy: 0.6802\n",
      "Epoch 676/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3451 - accuracy: 0.8861 - val_loss: 0.8824 - val_accuracy: 0.6827\n",
      "Epoch 677/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3496 - accuracy: 0.8831 - val_loss: 0.8737 - val_accuracy: 0.6901\n",
      "Epoch 678/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3719 - accuracy: 0.8739 - val_loss: 0.8841 - val_accuracy: 0.6741\n",
      "Epoch 679/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3531 - accuracy: 0.8825 - val_loss: 0.8906 - val_accuracy: 0.6852\n",
      "Epoch 680/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3650 - accuracy: 0.8727 - val_loss: 0.8973 - val_accuracy: 0.6877\n",
      "Epoch 681/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3503 - accuracy: 0.8825 - val_loss: 0.8934 - val_accuracy: 0.6753\n",
      "Epoch 682/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8715 - val_loss: 0.8920 - val_accuracy: 0.6827\n",
      "Epoch 683/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3465 - accuracy: 0.8831 - val_loss: 0.8994 - val_accuracy: 0.6877\n",
      "Epoch 684/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3706 - accuracy: 0.8806 - val_loss: 0.8947 - val_accuracy: 0.6864\n",
      "Epoch 685/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3821 - accuracy: 0.8709 - val_loss: 0.8859 - val_accuracy: 0.6840\n",
      "Epoch 686/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3475 - accuracy: 0.8825 - val_loss: 0.8850 - val_accuracy: 0.6815\n",
      "Epoch 687/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.8782 - val_loss: 0.9067 - val_accuracy: 0.6642\n",
      "Epoch 688/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3587 - accuracy: 0.8745 - val_loss: 0.8813 - val_accuracy: 0.6864\n",
      "Epoch 689/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3551 - accuracy: 0.8825 - val_loss: 0.8906 - val_accuracy: 0.6840\n",
      "Epoch 690/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3514 - accuracy: 0.8782 - val_loss: 0.9177 - val_accuracy: 0.6728\n",
      "Epoch 691/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3649 - accuracy: 0.8648 - val_loss: 0.9160 - val_accuracy: 0.6704\n",
      "Epoch 692/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.3464 - accuracy: 0.8819 - val_loss: 0.8898 - val_accuracy: 0.7086\n",
      "Epoch 693/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.3335 - accuracy: 0.8898 - val_loss: 0.8823 - val_accuracy: 0.6852\n",
      "Epoch 694/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8904 - val_loss: 0.9059 - val_accuracy: 0.6716\n",
      "Epoch 695/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3485 - accuracy: 0.8794 - val_loss: 0.8921 - val_accuracy: 0.6790\n",
      "Epoch 696/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3510 - accuracy: 0.8800 - val_loss: 0.8923 - val_accuracy: 0.6951\n",
      "Epoch 697/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3384 - accuracy: 0.8800 - val_loss: 0.8903 - val_accuracy: 0.6938\n",
      "Epoch 698/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3565 - accuracy: 0.8770 - val_loss: 0.8879 - val_accuracy: 0.6877\n",
      "Epoch 699/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3557 - accuracy: 0.8758 - val_loss: 0.8886 - val_accuracy: 0.6901\n",
      "Epoch 700/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.8922 - val_loss: 0.8711 - val_accuracy: 0.6938\n",
      "Epoch 701/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3527 - accuracy: 0.8764 - val_loss: 0.8908 - val_accuracy: 0.6889\n",
      "Epoch 702/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3521 - accuracy: 0.8867 - val_loss: 0.9003 - val_accuracy: 0.6617\n",
      "Epoch 703/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3475 - accuracy: 0.8831 - val_loss: 0.8876 - val_accuracy: 0.6802\n",
      "Epoch 704/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3471 - accuracy: 0.8758 - val_loss: 0.8812 - val_accuracy: 0.6877\n",
      "Epoch 705/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.3465 - accuracy: 0.8831 - val_loss: 0.8767 - val_accuracy: 0.6852\n",
      "Epoch 706/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3342 - accuracy: 0.8898 - val_loss: 0.8895 - val_accuracy: 0.6901\n",
      "Epoch 707/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8794 - val_loss: 0.8742 - val_accuracy: 0.6765\n",
      "Epoch 708/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3600 - accuracy: 0.8800 - val_loss: 0.8692 - val_accuracy: 0.6975\n",
      "Epoch 709/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8721 - val_loss: 0.8801 - val_accuracy: 0.7037\n",
      "Epoch 710/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3475 - accuracy: 0.8794 - val_loss: 0.8848 - val_accuracy: 0.6988\n",
      "Epoch 711/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3510 - accuracy: 0.8752 - val_loss: 0.8697 - val_accuracy: 0.6852\n",
      "Epoch 712/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.8910 - val_loss: 0.9045 - val_accuracy: 0.6938\n",
      "Epoch 713/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3465 - accuracy: 0.8892 - val_loss: 0.8757 - val_accuracy: 0.6778\n",
      "Epoch 714/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3492 - accuracy: 0.8764 - val_loss: 0.8876 - val_accuracy: 0.6765\n",
      "Epoch 715/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8886 - val_loss: 0.8974 - val_accuracy: 0.6827\n",
      "Epoch 716/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3385 - accuracy: 0.8873 - val_loss: 0.8842 - val_accuracy: 0.6840\n",
      "Epoch 717/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3165 - accuracy: 0.8849 - val_loss: 0.8869 - val_accuracy: 0.6840\n",
      "Epoch 718/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3340 - accuracy: 0.8904 - val_loss: 0.8866 - val_accuracy: 0.6889\n",
      "Epoch 719/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3301 - accuracy: 0.8867 - val_loss: 0.8956 - val_accuracy: 0.6827\n",
      "Epoch 720/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8770 - val_loss: 0.8844 - val_accuracy: 0.6815\n",
      "Epoch 721/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8946 - val_loss: 0.8907 - val_accuracy: 0.6790\n",
      "Epoch 722/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3566 - accuracy: 0.8788 - val_loss: 0.8825 - val_accuracy: 0.6840\n",
      "Epoch 723/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3366 - accuracy: 0.8819 - val_loss: 0.8912 - val_accuracy: 0.6642\n",
      "Epoch 724/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3328 - accuracy: 0.8867 - val_loss: 0.8675 - val_accuracy: 0.6877\n",
      "Epoch 725/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3219 - accuracy: 0.8892 - val_loss: 0.8761 - val_accuracy: 0.6938\n",
      "Epoch 726/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3187 - accuracy: 0.8971 - val_loss: 0.8780 - val_accuracy: 0.6988\n",
      "Epoch 727/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.3190 - accuracy: 0.8873 - val_loss: 0.9302 - val_accuracy: 0.6778\n",
      "Epoch 728/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3219 - accuracy: 0.8940 - val_loss: 0.9081 - val_accuracy: 0.6790\n",
      "Epoch 729/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8825 - val_loss: 0.8822 - val_accuracy: 0.6926\n",
      "Epoch 730/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3307 - accuracy: 0.8843 - val_loss: 0.8764 - val_accuracy: 0.6840\n",
      "Epoch 731/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8843 - val_loss: 0.9077 - val_accuracy: 0.6889\n",
      "Epoch 732/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.8910 - val_loss: 0.8995 - val_accuracy: 0.6728\n",
      "Epoch 733/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3469 - accuracy: 0.8764 - val_loss: 0.8842 - val_accuracy: 0.6975\n",
      "Epoch 734/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3126 - accuracy: 0.9007 - val_loss: 0.8942 - val_accuracy: 0.6914\n",
      "Epoch 735/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8934 - val_loss: 0.8892 - val_accuracy: 0.6815\n",
      "Epoch 736/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3202 - accuracy: 0.8879 - val_loss: 0.9047 - val_accuracy: 0.6901\n",
      "Epoch 737/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3201 - accuracy: 0.8971 - val_loss: 0.8941 - val_accuracy: 0.6877\n",
      "Epoch 738/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8928 - val_loss: 0.8629 - val_accuracy: 0.6975\n",
      "Epoch 739/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3286 - accuracy: 0.8879 - val_loss: 0.8725 - val_accuracy: 0.6951\n",
      "Epoch 740/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3270 - accuracy: 0.8934 - val_loss: 0.8731 - val_accuracy: 0.6901\n",
      "Epoch 741/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3041 - accuracy: 0.9019 - val_loss: 0.8884 - val_accuracy: 0.6802\n",
      "Epoch 742/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3205 - accuracy: 0.8825 - val_loss: 0.8884 - val_accuracy: 0.6914\n",
      "Epoch 743/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3115 - accuracy: 0.8855 - val_loss: 0.8991 - val_accuracy: 0.6827\n",
      "Epoch 744/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.3039 - accuracy: 0.8971 - val_loss: 0.8853 - val_accuracy: 0.6852\n",
      "Epoch 745/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3069 - accuracy: 0.8916 - val_loss: 0.8931 - val_accuracy: 0.6914\n",
      "Epoch 746/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.3228 - accuracy: 0.8794 - val_loss: 0.8955 - val_accuracy: 0.6914\n",
      "Epoch 747/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3168 - accuracy: 0.8989 - val_loss: 0.9025 - val_accuracy: 0.6741\n",
      "Epoch 748/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8934 - val_loss: 0.8930 - val_accuracy: 0.6938\n",
      "Epoch 749/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.3147 - accuracy: 0.8995 - val_loss: 0.8755 - val_accuracy: 0.6988\n",
      "Epoch 750/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3275 - accuracy: 0.8855 - val_loss: 0.9016 - val_accuracy: 0.6938\n",
      "Epoch 751/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3091 - accuracy: 0.8928 - val_loss: 0.8952 - val_accuracy: 0.6827\n",
      "Epoch 752/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3311 - accuracy: 0.8806 - val_loss: 0.9059 - val_accuracy: 0.6728\n",
      "Epoch 753/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8776 - val_loss: 0.8668 - val_accuracy: 0.7012\n",
      "Epoch 754/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2982 - accuracy: 0.9038 - val_loss: 0.9094 - val_accuracy: 0.6827\n",
      "Epoch 755/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8837 - val_loss: 0.8818 - val_accuracy: 0.7062\n",
      "Epoch 756/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.8873 - val_loss: 0.8765 - val_accuracy: 0.6901\n",
      "Epoch 757/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2995 - accuracy: 0.9032 - val_loss: 0.8946 - val_accuracy: 0.7012\n",
      "Epoch 758/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8916 - val_loss: 0.8934 - val_accuracy: 0.6778\n",
      "Epoch 759/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3129 - accuracy: 0.8849 - val_loss: 0.8876 - val_accuracy: 0.6815\n",
      "Epoch 760/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2905 - accuracy: 0.9013 - val_loss: 0.8990 - val_accuracy: 0.6827\n",
      "Epoch 761/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.3074 - accuracy: 0.8959 - val_loss: 0.8708 - val_accuracy: 0.6951\n",
      "Epoch 762/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.3059 - accuracy: 0.8940 - val_loss: 0.9099 - val_accuracy: 0.6765\n",
      "Epoch 763/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2976 - accuracy: 0.8977 - val_loss: 0.9286 - val_accuracy: 0.6815\n",
      "Epoch 764/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.3142 - accuracy: 0.8886 - val_loss: 0.8865 - val_accuracy: 0.6889\n",
      "Epoch 765/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2987 - accuracy: 0.9001 - val_loss: 0.9139 - val_accuracy: 0.6778\n",
      "Epoch 766/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3303 - accuracy: 0.8812 - val_loss: 0.8846 - val_accuracy: 0.6877\n",
      "Epoch 767/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2990 - accuracy: 0.8928 - val_loss: 0.8928 - val_accuracy: 0.6926\n",
      "Epoch 768/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3104 - accuracy: 0.8952 - val_loss: 0.9013 - val_accuracy: 0.6975\n",
      "Epoch 769/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3109 - accuracy: 0.8983 - val_loss: 0.9073 - val_accuracy: 0.6852\n",
      "Epoch 770/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2966 - accuracy: 0.9019 - val_loss: 0.9080 - val_accuracy: 0.6790\n",
      "Epoch 771/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.3041 - accuracy: 0.8879 - val_loss: 0.9040 - val_accuracy: 0.6914\n",
      "Epoch 772/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3221 - accuracy: 0.8861 - val_loss: 0.8792 - val_accuracy: 0.6840\n",
      "Epoch 773/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3144 - accuracy: 0.8886 - val_loss: 0.8798 - val_accuracy: 0.6938\n",
      "Epoch 774/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8849 - val_loss: 0.8710 - val_accuracy: 0.6840\n",
      "Epoch 775/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2961 - accuracy: 0.8995 - val_loss: 0.8943 - val_accuracy: 0.6926\n",
      "Epoch 776/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3154 - accuracy: 0.8922 - val_loss: 0.8790 - val_accuracy: 0.6765\n",
      "Epoch 777/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3178 - accuracy: 0.8946 - val_loss: 0.8973 - val_accuracy: 0.6864\n",
      "Epoch 778/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2942 - accuracy: 0.9001 - val_loss: 0.8949 - val_accuracy: 0.7000\n",
      "Epoch 779/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2919 - accuracy: 0.9050 - val_loss: 0.8884 - val_accuracy: 0.6790\n",
      "Epoch 780/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2981 - accuracy: 0.9038 - val_loss: 0.9038 - val_accuracy: 0.6914\n",
      "Epoch 781/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3058 - accuracy: 0.8989 - val_loss: 0.8896 - val_accuracy: 0.6877\n",
      "Epoch 782/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.8898 - val_loss: 0.9069 - val_accuracy: 0.6889\n",
      "Epoch 783/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.8940 - val_loss: 0.9038 - val_accuracy: 0.6914\n",
      "Epoch 784/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2840 - accuracy: 0.9044 - val_loss: 0.8964 - val_accuracy: 0.6938\n",
      "Epoch 785/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3021 - accuracy: 0.8977 - val_loss: 0.8920 - val_accuracy: 0.6951\n",
      "Epoch 786/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3001 - accuracy: 0.8989 - val_loss: 0.8926 - val_accuracy: 0.7012\n",
      "Epoch 787/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2906 - accuracy: 0.9123 - val_loss: 0.8988 - val_accuracy: 0.6802\n",
      "Epoch 788/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2883 - accuracy: 0.9056 - val_loss: 0.9012 - val_accuracy: 0.6877\n",
      "Epoch 789/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2909 - accuracy: 0.9074 - val_loss: 0.8800 - val_accuracy: 0.6901\n",
      "Epoch 790/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2862 - accuracy: 0.9013 - val_loss: 0.9067 - val_accuracy: 0.6716\n",
      "Epoch 791/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3166 - accuracy: 0.8867 - val_loss: 0.8920 - val_accuracy: 0.7000\n",
      "Epoch 792/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2886 - accuracy: 0.9019 - val_loss: 0.8852 - val_accuracy: 0.6852\n",
      "Epoch 793/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2760 - accuracy: 0.9062 - val_loss: 0.9060 - val_accuracy: 0.6852\n",
      "Epoch 794/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2843 - accuracy: 0.9044 - val_loss: 0.9086 - val_accuracy: 0.6889\n",
      "Epoch 795/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2905 - accuracy: 0.8922 - val_loss: 0.8895 - val_accuracy: 0.6852\n",
      "Epoch 796/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3146 - accuracy: 0.8861 - val_loss: 0.8889 - val_accuracy: 0.6963\n",
      "Epoch 797/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2912 - accuracy: 0.8959 - val_loss: 0.9161 - val_accuracy: 0.6889\n",
      "Epoch 798/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2686 - accuracy: 0.9026 - val_loss: 0.8942 - val_accuracy: 0.6827\n",
      "Epoch 799/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2974 - accuracy: 0.9013 - val_loss: 0.8836 - val_accuracy: 0.6951\n",
      "Epoch 800/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2920 - accuracy: 0.8989 - val_loss: 0.8840 - val_accuracy: 0.6901\n",
      "Epoch 801/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2916 - accuracy: 0.9026 - val_loss: 0.8809 - val_accuracy: 0.6938\n",
      "Epoch 802/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3000 - accuracy: 0.9050 - val_loss: 0.9013 - val_accuracy: 0.6827\n",
      "Epoch 803/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2865 - accuracy: 0.8983 - val_loss: 0.9021 - val_accuracy: 0.6951\n",
      "Epoch 804/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2775 - accuracy: 0.9056 - val_loss: 0.9042 - val_accuracy: 0.6864\n",
      "Epoch 805/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2961 - accuracy: 0.9019 - val_loss: 0.9208 - val_accuracy: 0.6889\n",
      "Epoch 806/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3015 - accuracy: 0.8940 - val_loss: 0.9200 - val_accuracy: 0.6815\n",
      "Epoch 807/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2874 - accuracy: 0.8995 - val_loss: 0.9196 - val_accuracy: 0.6790\n",
      "Epoch 808/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2924 - accuracy: 0.9032 - val_loss: 0.9217 - val_accuracy: 0.6938\n",
      "Epoch 809/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2648 - accuracy: 0.9123 - val_loss: 0.8947 - val_accuracy: 0.6877\n",
      "Epoch 810/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2702 - accuracy: 0.9001 - val_loss: 0.8980 - val_accuracy: 0.6963\n",
      "Epoch 811/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2659 - accuracy: 0.9117 - val_loss: 0.9050 - val_accuracy: 0.6790\n",
      "Epoch 812/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2934 - accuracy: 0.9038 - val_loss: 0.9076 - val_accuracy: 0.6765\n",
      "Epoch 813/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2930 - accuracy: 0.9013 - val_loss: 0.8973 - val_accuracy: 0.6790\n",
      "Epoch 814/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2736 - accuracy: 0.9111 - val_loss: 0.8941 - val_accuracy: 0.7012\n",
      "Epoch 815/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2829 - accuracy: 0.9062 - val_loss: 0.8977 - val_accuracy: 0.6914\n",
      "Epoch 816/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2879 - accuracy: 0.9056 - val_loss: 0.8904 - val_accuracy: 0.6914\n",
      "Epoch 817/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2809 - accuracy: 0.9080 - val_loss: 0.8976 - val_accuracy: 0.7025\n",
      "Epoch 818/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2867 - accuracy: 0.8995 - val_loss: 0.8863 - val_accuracy: 0.6926\n",
      "Epoch 819/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.9190 - val_loss: 0.9025 - val_accuracy: 0.7025\n",
      "Epoch 820/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2672 - accuracy: 0.9080 - val_loss: 0.8906 - val_accuracy: 0.7049\n",
      "Epoch 821/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2786 - accuracy: 0.9074 - val_loss: 0.8799 - val_accuracy: 0.7037\n",
      "Epoch 822/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2770 - accuracy: 0.9062 - val_loss: 0.9042 - val_accuracy: 0.6765\n",
      "Epoch 823/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2657 - accuracy: 0.9038 - val_loss: 0.9207 - val_accuracy: 0.6926\n",
      "Epoch 824/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2854 - accuracy: 0.9032 - val_loss: 0.8999 - val_accuracy: 0.6889\n",
      "Epoch 825/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.2730 - accuracy: 0.9093 - val_loss: 0.9012 - val_accuracy: 0.6914\n",
      "Epoch 826/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2827 - accuracy: 0.9105 - val_loss: 0.8824 - val_accuracy: 0.6877\n",
      "Epoch 827/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2845 - accuracy: 0.8989 - val_loss: 0.9070 - val_accuracy: 0.6914\n",
      "Epoch 828/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2856 - accuracy: 0.9038 - val_loss: 0.9171 - val_accuracy: 0.6901\n",
      "Epoch 829/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.9099 - val_loss: 0.9052 - val_accuracy: 0.6901\n",
      "Epoch 830/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2757 - accuracy: 0.9086 - val_loss: 0.8979 - val_accuracy: 0.6901\n",
      "Epoch 831/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.9099 - val_loss: 0.9258 - val_accuracy: 0.6877\n",
      "Epoch 832/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2505 - accuracy: 0.9263 - val_loss: 0.9092 - val_accuracy: 0.6877\n",
      "Epoch 833/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2562 - accuracy: 0.9147 - val_loss: 0.9108 - val_accuracy: 0.6852\n",
      "Epoch 834/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2785 - accuracy: 0.9050 - val_loss: 0.9164 - val_accuracy: 0.7000\n",
      "Epoch 835/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2611 - accuracy: 0.9123 - val_loss: 0.9020 - val_accuracy: 0.6815\n",
      "Epoch 836/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2946 - accuracy: 0.9007 - val_loss: 0.9101 - val_accuracy: 0.6790\n",
      "Epoch 837/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2615 - accuracy: 0.9153 - val_loss: 0.8922 - val_accuracy: 0.6901\n",
      "Epoch 838/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2851 - accuracy: 0.9001 - val_loss: 0.9050 - val_accuracy: 0.6802\n",
      "Epoch 839/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.2684 - accuracy: 0.9141 - val_loss: 0.9089 - val_accuracy: 0.6951\n",
      "Epoch 840/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2809 - accuracy: 0.9062 - val_loss: 0.8898 - val_accuracy: 0.6901\n",
      "Epoch 841/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2659 - accuracy: 0.9153 - val_loss: 0.8874 - val_accuracy: 0.6901\n",
      "Epoch 842/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2447 - accuracy: 0.9220 - val_loss: 0.8879 - val_accuracy: 0.6926\n",
      "Epoch 843/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2554 - accuracy: 0.9147 - val_loss: 0.8875 - val_accuracy: 0.6988\n",
      "Epoch 844/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2647 - accuracy: 0.9032 - val_loss: 0.9121 - val_accuracy: 0.6827\n",
      "Epoch 845/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2736 - accuracy: 0.9111 - val_loss: 0.9082 - val_accuracy: 0.6963\n",
      "Epoch 846/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2747 - accuracy: 0.9093 - val_loss: 0.8970 - val_accuracy: 0.6840\n",
      "Epoch 847/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2557 - accuracy: 0.9269 - val_loss: 0.9170 - val_accuracy: 0.6963\n",
      "Epoch 848/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2645 - accuracy: 0.9044 - val_loss: 0.9042 - val_accuracy: 0.6975\n",
      "Epoch 849/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2749 - accuracy: 0.9080 - val_loss: 0.8957 - val_accuracy: 0.6938\n",
      "Epoch 850/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2655 - accuracy: 0.9080 - val_loss: 0.8926 - val_accuracy: 0.6975\n",
      "Epoch 851/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2596 - accuracy: 0.9099 - val_loss: 0.9063 - val_accuracy: 0.6840\n",
      "Epoch 852/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2677 - accuracy: 0.9086 - val_loss: 0.9138 - val_accuracy: 0.6963\n",
      "Epoch 853/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2798 - accuracy: 0.9111 - val_loss: 0.9056 - val_accuracy: 0.6926\n",
      "Epoch 854/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2685 - accuracy: 0.9068 - val_loss: 0.9102 - val_accuracy: 0.6864\n",
      "Epoch 855/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.9135 - val_loss: 0.9066 - val_accuracy: 0.6975\n",
      "Epoch 856/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2648 - accuracy: 0.9086 - val_loss: 0.9385 - val_accuracy: 0.6877\n",
      "Epoch 857/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.9135 - val_loss: 0.9164 - val_accuracy: 0.6889\n",
      "Epoch 858/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2480 - accuracy: 0.9233 - val_loss: 0.9153 - val_accuracy: 0.6827\n",
      "Epoch 859/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2599 - accuracy: 0.9160 - val_loss: 0.9081 - val_accuracy: 0.6901\n",
      "Epoch 860/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2662 - accuracy: 0.9032 - val_loss: 0.9255 - val_accuracy: 0.6691\n",
      "Epoch 861/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.9129 - val_loss: 0.9049 - val_accuracy: 0.6975\n",
      "Epoch 862/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2596 - accuracy: 0.9111 - val_loss: 0.9241 - val_accuracy: 0.6827\n",
      "Epoch 863/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2720 - accuracy: 0.9068 - val_loss: 0.8797 - val_accuracy: 0.6963\n",
      "Epoch 864/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2648 - accuracy: 0.9153 - val_loss: 0.9135 - val_accuracy: 0.6963\n",
      "Epoch 865/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2691 - accuracy: 0.9105 - val_loss: 0.8956 - val_accuracy: 0.6951\n",
      "Epoch 866/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2630 - accuracy: 0.9184 - val_loss: 0.8860 - val_accuracy: 0.7062\n",
      "Epoch 867/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2621 - accuracy: 0.9093 - val_loss: 0.9439 - val_accuracy: 0.6938\n",
      "Epoch 868/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2665 - accuracy: 0.9086 - val_loss: 0.8858 - val_accuracy: 0.6951\n",
      "Epoch 869/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2667 - accuracy: 0.9099 - val_loss: 0.8827 - val_accuracy: 0.6901\n",
      "Epoch 870/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2652 - accuracy: 0.9111 - val_loss: 0.9113 - val_accuracy: 0.7012\n",
      "Epoch 871/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2464 - accuracy: 0.9172 - val_loss: 0.9314 - val_accuracy: 0.6704\n",
      "Epoch 872/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.9141 - val_loss: 0.9537 - val_accuracy: 0.7037\n",
      "Epoch 873/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2676 - accuracy: 0.9160 - val_loss: 0.9305 - val_accuracy: 0.6840\n",
      "Epoch 874/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2516 - accuracy: 0.9123 - val_loss: 0.9144 - val_accuracy: 0.6877\n",
      "Epoch 875/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2442 - accuracy: 0.9233 - val_loss: 0.9049 - val_accuracy: 0.6889\n",
      "Epoch 876/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2637 - accuracy: 0.9117 - val_loss: 0.9206 - val_accuracy: 0.6827\n",
      "Epoch 877/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2501 - accuracy: 0.9074 - val_loss: 0.8944 - val_accuracy: 0.6889\n",
      "Epoch 878/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2574 - accuracy: 0.9111 - val_loss: 0.8969 - val_accuracy: 0.7012\n",
      "Epoch 879/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2630 - accuracy: 0.9123 - val_loss: 0.9289 - val_accuracy: 0.6827\n",
      "Epoch 880/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2494 - accuracy: 0.9172 - val_loss: 0.9063 - val_accuracy: 0.6975\n",
      "Epoch 881/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2506 - accuracy: 0.9178 - val_loss: 0.9403 - val_accuracy: 0.6864\n",
      "Epoch 882/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2549 - accuracy: 0.9141 - val_loss: 0.8733 - val_accuracy: 0.7037\n",
      "Epoch 883/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2526 - accuracy: 0.9184 - val_loss: 0.9084 - val_accuracy: 0.6864\n",
      "Epoch 884/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2368 - accuracy: 0.9220 - val_loss: 0.9130 - val_accuracy: 0.6864\n",
      "Epoch 885/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2433 - accuracy: 0.9214 - val_loss: 0.9461 - val_accuracy: 0.6877\n",
      "Epoch 886/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2644 - accuracy: 0.9129 - val_loss: 0.9255 - val_accuracy: 0.6926\n",
      "Epoch 887/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2389 - accuracy: 0.9294 - val_loss: 0.9151 - val_accuracy: 0.6741\n",
      "Epoch 888/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2502 - accuracy: 0.9202 - val_loss: 0.9161 - val_accuracy: 0.6877\n",
      "Epoch 889/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2362 - accuracy: 0.9074 - val_loss: 0.9111 - val_accuracy: 0.6926\n",
      "Epoch 890/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2574 - accuracy: 0.9123 - val_loss: 0.8988 - val_accuracy: 0.6975\n",
      "Epoch 891/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2514 - accuracy: 0.9196 - val_loss: 0.8954 - val_accuracy: 0.6877\n",
      "Epoch 892/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2602 - accuracy: 0.9086 - val_loss: 0.9335 - val_accuracy: 0.6827\n",
      "Epoch 893/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2388 - accuracy: 0.9160 - val_loss: 0.9131 - val_accuracy: 0.6741\n",
      "Epoch 894/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2339 - accuracy: 0.9202 - val_loss: 0.9323 - val_accuracy: 0.6901\n",
      "Epoch 895/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2479 - accuracy: 0.9147 - val_loss: 0.9177 - val_accuracy: 0.6975\n",
      "Epoch 896/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.2391 - accuracy: 0.9202 - val_loss: 0.9368 - val_accuracy: 0.6877\n",
      "Epoch 897/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2244 - accuracy: 0.9233 - val_loss: 0.9162 - val_accuracy: 0.6877\n",
      "Epoch 898/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2591 - accuracy: 0.9166 - val_loss: 0.9014 - val_accuracy: 0.6926\n",
      "Epoch 899/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2374 - accuracy: 0.9239 - val_loss: 0.9238 - val_accuracy: 0.6864\n",
      "Epoch 900/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2339 - accuracy: 0.9239 - val_loss: 0.9197 - val_accuracy: 0.6889\n",
      "Epoch 901/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2257 - accuracy: 0.9324 - val_loss: 0.9360 - val_accuracy: 0.6988\n",
      "Epoch 902/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.2316 - accuracy: 0.9251 - val_loss: 0.9301 - val_accuracy: 0.6951\n",
      "Epoch 903/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2354 - accuracy: 0.9227 - val_loss: 0.9964 - val_accuracy: 0.6728\n",
      "Epoch 904/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.2337 - accuracy: 0.9160 - val_loss: 0.9291 - val_accuracy: 0.6840\n",
      "Epoch 905/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2404 - accuracy: 0.9220 - val_loss: 0.9254 - val_accuracy: 0.7012\n",
      "Epoch 906/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2492 - accuracy: 0.9129 - val_loss: 0.9410 - val_accuracy: 0.6889\n",
      "Epoch 907/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2434 - accuracy: 0.9184 - val_loss: 0.9223 - val_accuracy: 0.6840\n",
      "Epoch 908/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2302 - accuracy: 0.9220 - val_loss: 0.9274 - val_accuracy: 0.6778\n",
      "Epoch 909/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2353 - accuracy: 0.9190 - val_loss: 0.9169 - val_accuracy: 0.6938\n",
      "Epoch 910/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2574 - accuracy: 0.9153 - val_loss: 0.9095 - val_accuracy: 0.6864\n",
      "Epoch 911/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2362 - accuracy: 0.9184 - val_loss: 0.9058 - val_accuracy: 0.6864\n",
      "Epoch 912/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2216 - accuracy: 0.9190 - val_loss: 0.9049 - val_accuracy: 0.6914\n",
      "Epoch 913/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.2295 - accuracy: 0.9196 - val_loss: 0.9200 - val_accuracy: 0.7049\n",
      "Epoch 914/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2421 - accuracy: 0.9245 - val_loss: 0.9036 - val_accuracy: 0.6963\n",
      "Epoch 915/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2375 - accuracy: 0.9147 - val_loss: 0.9162 - val_accuracy: 0.7025\n",
      "Epoch 916/1000\n",
      "103/103 [==============================] - 0s 5ms/step - loss: 0.2299 - accuracy: 0.9257 - val_loss: 0.9395 - val_accuracy: 0.7000\n",
      "Epoch 917/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2407 - accuracy: 0.9251 - val_loss: 0.9434 - val_accuracy: 0.6840\n",
      "Epoch 918/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2284 - accuracy: 0.9227 - val_loss: 0.9221 - val_accuracy: 0.6963\n",
      "Epoch 919/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2378 - accuracy: 0.9190 - val_loss: 0.9164 - val_accuracy: 0.6815\n",
      "Epoch 920/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2312 - accuracy: 0.9294 - val_loss: 0.9151 - val_accuracy: 0.6951\n",
      "Epoch 921/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2369 - accuracy: 0.9141 - val_loss: 0.9269 - val_accuracy: 0.6852\n",
      "Epoch 922/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2187 - accuracy: 0.9312 - val_loss: 0.9122 - val_accuracy: 0.7037\n",
      "Epoch 923/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2397 - accuracy: 0.9220 - val_loss: 0.9254 - val_accuracy: 0.6802\n",
      "Epoch 924/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2408 - accuracy: 0.9239 - val_loss: 0.9157 - val_accuracy: 0.7025\n",
      "Epoch 925/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2291 - accuracy: 0.9220 - val_loss: 0.8827 - val_accuracy: 0.6938\n",
      "Epoch 926/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9257 - val_loss: 0.9153 - val_accuracy: 0.6951\n",
      "Epoch 927/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2280 - accuracy: 0.9233 - val_loss: 0.9338 - val_accuracy: 0.6827\n",
      "Epoch 928/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2263 - accuracy: 0.9300 - val_loss: 0.9227 - val_accuracy: 0.6988\n",
      "Epoch 929/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2419 - accuracy: 0.9196 - val_loss: 0.9231 - val_accuracy: 0.6951\n",
      "Epoch 930/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.9263 - val_loss: 0.9271 - val_accuracy: 0.6938\n",
      "Epoch 931/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.9233 - val_loss: 0.9343 - val_accuracy: 0.6877\n",
      "Epoch 932/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2291 - accuracy: 0.9239 - val_loss: 0.9388 - val_accuracy: 0.6802\n",
      "Epoch 933/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2259 - accuracy: 0.9251 - val_loss: 0.9351 - val_accuracy: 0.6988\n",
      "Epoch 934/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2262 - accuracy: 0.9275 - val_loss: 0.9644 - val_accuracy: 0.6877\n",
      "Epoch 935/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2354 - accuracy: 0.9147 - val_loss: 0.9205 - val_accuracy: 0.7037\n",
      "Epoch 936/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2346 - accuracy: 0.9245 - val_loss: 0.9322 - val_accuracy: 0.6988\n",
      "Epoch 937/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2211 - accuracy: 0.9220 - val_loss: 0.8986 - val_accuracy: 0.7086\n",
      "Epoch 938/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2286 - accuracy: 0.9281 - val_loss: 0.9149 - val_accuracy: 0.6914\n",
      "Epoch 939/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2276 - accuracy: 0.9245 - val_loss: 0.9296 - val_accuracy: 0.6988\n",
      "Epoch 940/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2341 - accuracy: 0.9306 - val_loss: 0.9250 - val_accuracy: 0.6901\n",
      "Epoch 941/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2546 - accuracy: 0.9147 - val_loss: 0.9333 - val_accuracy: 0.6963\n",
      "Epoch 942/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2156 - accuracy: 0.9318 - val_loss: 0.9304 - val_accuracy: 0.7012\n",
      "Epoch 943/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2083 - accuracy: 0.9263 - val_loss: 0.9545 - val_accuracy: 0.6765\n",
      "Epoch 944/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2065 - accuracy: 0.9348 - val_loss: 0.9266 - val_accuracy: 0.7025\n",
      "Epoch 945/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2185 - accuracy: 0.9227 - val_loss: 0.9339 - val_accuracy: 0.6951\n",
      "Epoch 946/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2263 - accuracy: 0.9227 - val_loss: 0.9349 - val_accuracy: 0.6765\n",
      "Epoch 947/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2291 - accuracy: 0.9160 - val_loss: 0.9323 - val_accuracy: 0.7012\n",
      "Epoch 948/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2085 - accuracy: 0.9330 - val_loss: 0.9234 - val_accuracy: 0.7049\n",
      "Epoch 949/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2272 - accuracy: 0.9275 - val_loss: 0.8959 - val_accuracy: 0.6963\n",
      "Epoch 950/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.9336 - val_loss: 0.9257 - val_accuracy: 0.6951\n",
      "Epoch 951/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2271 - accuracy: 0.9220 - val_loss: 0.9284 - val_accuracy: 0.7025\n",
      "Epoch 952/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2071 - accuracy: 0.9361 - val_loss: 0.9451 - val_accuracy: 0.6864\n",
      "Epoch 953/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2189 - accuracy: 0.9227 - val_loss: 0.9068 - val_accuracy: 0.6926\n",
      "Epoch 954/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2143 - accuracy: 0.9257 - val_loss: 0.9260 - val_accuracy: 0.6889\n",
      "Epoch 955/1000\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.2234 - accuracy: 0.9220 - val_loss: 0.9388 - val_accuracy: 0.6852\n",
      "Epoch 956/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2197 - accuracy: 0.9275 - val_loss: 0.9093 - val_accuracy: 0.7049\n",
      "Epoch 957/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2342 - accuracy: 0.9111 - val_loss: 0.9455 - val_accuracy: 0.6790\n",
      "Epoch 958/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2230 - accuracy: 0.9245 - val_loss: 0.9423 - val_accuracy: 0.6901\n",
      "Epoch 959/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2021 - accuracy: 0.9336 - val_loss: 0.9384 - val_accuracy: 0.6938\n",
      "Epoch 960/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.9294 - val_loss: 0.9419 - val_accuracy: 0.6975\n",
      "Epoch 961/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2148 - accuracy: 0.9251 - val_loss: 0.9088 - val_accuracy: 0.7074\n",
      "Epoch 962/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2179 - accuracy: 0.9233 - val_loss: 0.9140 - val_accuracy: 0.6901\n",
      "Epoch 963/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2031 - accuracy: 0.9330 - val_loss: 0.9302 - val_accuracy: 0.7000\n",
      "Epoch 964/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2309 - accuracy: 0.9263 - val_loss: 0.9374 - val_accuracy: 0.6926\n",
      "Epoch 965/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2175 - accuracy: 0.9239 - val_loss: 0.9293 - val_accuracy: 0.7049\n",
      "Epoch 966/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2036 - accuracy: 0.9294 - val_loss: 0.9494 - val_accuracy: 0.6914\n",
      "Epoch 967/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.9196 - val_loss: 0.9672 - val_accuracy: 0.6864\n",
      "Epoch 968/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2167 - accuracy: 0.9257 - val_loss: 0.9467 - val_accuracy: 0.6877\n",
      "Epoch 969/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2294 - accuracy: 0.9227 - val_loss: 0.9265 - val_accuracy: 0.6951\n",
      "Epoch 970/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9300 - val_loss: 0.9159 - val_accuracy: 0.6951\n",
      "Epoch 971/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.1954 - accuracy: 0.9348 - val_loss: 0.9276 - val_accuracy: 0.6951\n",
      "Epoch 972/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2158 - accuracy: 0.9300 - val_loss: 0.9083 - val_accuracy: 0.7062\n",
      "Epoch 973/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2105 - accuracy: 0.9287 - val_loss: 0.9083 - val_accuracy: 0.6988\n",
      "Epoch 974/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2203 - accuracy: 0.9202 - val_loss: 0.9288 - val_accuracy: 0.6938\n",
      "Epoch 975/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2069 - accuracy: 0.9257 - val_loss: 0.9283 - val_accuracy: 0.7099\n",
      "Epoch 976/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.9367 - val_loss: 0.9404 - val_accuracy: 0.6901\n",
      "Epoch 977/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2128 - accuracy: 0.9281 - val_loss: 0.9283 - val_accuracy: 0.6975\n",
      "Epoch 978/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2261 - accuracy: 0.9227 - val_loss: 0.9018 - val_accuracy: 0.7123\n",
      "Epoch 979/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2268 - accuracy: 0.9196 - val_loss: 0.8979 - val_accuracy: 0.7037\n",
      "Epoch 980/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2184 - accuracy: 0.9208 - val_loss: 0.9056 - val_accuracy: 0.6938\n",
      "Epoch 981/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2156 - accuracy: 0.9269 - val_loss: 0.9620 - val_accuracy: 0.6864\n",
      "Epoch 982/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2045 - accuracy: 0.9336 - val_loss: 0.9181 - val_accuracy: 0.7000\n",
      "Epoch 983/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2041 - accuracy: 0.9312 - val_loss: 0.9452 - val_accuracy: 0.7037\n",
      "Epoch 984/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2076 - accuracy: 0.9263 - val_loss: 0.9571 - val_accuracy: 0.7049\n",
      "Epoch 985/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2117 - accuracy: 0.9281 - val_loss: 0.9557 - val_accuracy: 0.6951\n",
      "Epoch 986/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2179 - accuracy: 0.9300 - val_loss: 0.9273 - val_accuracy: 0.7062\n",
      "Epoch 987/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2272 - accuracy: 0.9330 - val_loss: 0.9245 - val_accuracy: 0.6951\n",
      "Epoch 988/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9257 - val_loss: 0.9336 - val_accuracy: 0.6926\n",
      "Epoch 989/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2120 - accuracy: 0.9263 - val_loss: 0.9568 - val_accuracy: 0.6877\n",
      "Epoch 990/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2021 - accuracy: 0.9312 - val_loss: 0.9568 - val_accuracy: 0.6852\n",
      "Epoch 991/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.1986 - accuracy: 0.9367 - val_loss: 0.9383 - val_accuracy: 0.6827\n",
      "Epoch 992/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.1989 - accuracy: 0.9373 - val_loss: 0.9389 - val_accuracy: 0.6852\n",
      "Epoch 993/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2148 - accuracy: 0.9287 - val_loss: 0.9366 - val_accuracy: 0.6901\n",
      "Epoch 994/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.1993 - accuracy: 0.9428 - val_loss: 0.9417 - val_accuracy: 0.6975\n",
      "Epoch 995/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2102 - accuracy: 0.9330 - val_loss: 0.9651 - val_accuracy: 0.6815\n",
      "Epoch 996/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2040 - accuracy: 0.9336 - val_loss: 0.9517 - val_accuracy: 0.7037\n",
      "Epoch 997/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9361 - val_loss: 0.9304 - val_accuracy: 0.7160\n",
      "Epoch 998/1000\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2009 - accuracy: 0.9312 - val_loss: 0.9514 - val_accuracy: 0.6827\n",
      "Epoch 999/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9342 - val_loss: 0.9158 - val_accuracy: 0.7025\n",
      "Epoch 1000/1000\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.2084 - accuracy: 0.9367 - val_loss: 0.9404 - val_accuracy: 0.7074\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFytY6LDzgJ0"
   },
   "source": [
    "Let's plot the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "TFz4ClZov9gZ",
    "outputId": "e3fdf6e2-f249-4b36-a063-683c88ab705f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vf1W7LgP2DA5"
   },
   "source": [
    "\n",
    "\n",
    "And now let's plot the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "8yyFBt7ASPUe",
    "outputId": "d149ff38-7f2f-4eb4-d62e-08d8683ede2d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['accuracy'])\n",
    "plt.plot(cnnhistory.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gaZONl1mD8XD"
   },
   "source": [
    "Let's now create a classification report to review the f1-score of the model per class.\n",
    "To do so, we have to:\n",
    "- Create a variable predictions that will contain the model.predict_classes outcome\n",
    "- Convert our y_test (array of strings with our classes) to an array of int called new_Ytest, otherwise it will not be comparable to the predictions by the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EO25uIL-9vqx"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_testcnn)\n",
    "predictions=np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1i06grlBBSrn",
    "outputId": "af34893b-827c-4355-a92a-17b53b80ad3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 1, 1, 2, 1, 4, 4, 4, 3, 6, 7, 2, 1, 5, 4, 4, 5, 1, 7, 4,\n",
       "       3, 1, 6, 7, 5, 6, 7, 7, 4, 5, 0, 7, 0, 4, 1, 3, 4, 1, 3, 3, 2, 1,\n",
       "       2, 4, 5, 1, 6, 3, 6, 4, 6, 5, 2, 4, 5, 0, 5, 2, 1, 3, 5, 3, 3, 1,\n",
       "       2, 4, 1, 3, 4, 0, 6, 5, 5, 1, 2, 0, 5, 3, 6, 3, 3, 1, 4, 7, 0, 4,\n",
       "       5, 5, 1, 5, 4, 2, 2, 1, 1, 3, 4, 7, 6, 1, 1, 7, 1, 3, 7, 2, 3, 4,\n",
       "       7, 5, 4, 5, 1, 1, 0, 1, 7, 1, 1, 1, 6, 0, 2, 2, 3, 2, 2, 3, 5, 0,\n",
       "       6, 7, 0, 7, 3, 7, 7, 3, 3, 1, 3, 6, 4, 4, 5, 2, 2, 4, 6, 7, 3, 7,\n",
       "       6, 5, 3, 4, 5, 4, 4, 1, 4, 5, 1, 1, 6, 0, 5, 1, 5, 2, 1, 5, 1, 5,\n",
       "       1, 7, 7, 2, 3, 1, 1, 2, 3, 1, 1, 4, 7, 1, 3, 0, 5, 1, 0, 5, 1, 1,\n",
       "       4, 6, 1, 4, 3, 3, 7, 4, 3, 1, 3, 4, 7, 3, 3, 3, 5, 0, 2, 1, 1, 6,\n",
       "       6, 3, 5, 3, 4, 2, 1, 1, 2, 4, 3, 7, 5, 5, 2, 6, 6, 5, 6, 1, 3, 7,\n",
       "       1, 2, 3, 0, 5, 5, 3, 3, 5, 5, 2, 4, 3, 4, 6, 2, 3, 0, 3, 5, 2, 3,\n",
       "       1, 7, 1, 2, 7, 7, 1, 6, 6, 1, 1, 2, 0, 0, 2, 1, 4, 7, 3, 2, 6, 5,\n",
       "       2, 3, 5, 0, 4, 0, 2, 1, 5, 6, 6, 5, 2, 4, 5, 3, 3, 1, 6, 5, 7, 5,\n",
       "       1, 6, 2, 0, 7, 5, 7, 1, 3, 2, 7, 3, 5, 2, 3, 1, 4, 7, 6, 1, 4, 5,\n",
       "       2, 7, 6, 2, 4, 1, 1, 4, 5, 3, 3, 1, 4, 2, 1, 7, 5, 2, 1, 5, 2, 7,\n",
       "       1, 2, 3, 5, 1, 7, 2, 1, 0, 0, 2, 6, 4, 6, 6, 1, 3, 1, 5, 1, 2, 2,\n",
       "       3, 2, 3, 3, 3, 1, 4, 4, 1, 5, 2, 0, 7, 3, 4, 4, 2, 4, 4, 4, 2, 2,\n",
       "       0, 2, 7, 2, 2, 7, 3, 7, 3, 7, 3, 5, 1, 5, 5, 5, 3, 5, 0, 5, 4, 4,\n",
       "       1, 6, 5, 7, 3, 0, 1, 1, 3, 0, 1, 4, 4, 7, 0, 3, 7, 4, 3, 6, 7, 4,\n",
       "       2, 4, 6, 2, 7, 2, 3, 5, 5, 1, 1, 6, 7, 1, 1, 1, 4, 1, 5, 3, 4, 4,\n",
       "       5, 6, 4, 6, 4, 2, 2, 0, 4, 3, 3, 7, 1, 7, 4, 7, 3, 6, 6, 5, 5, 0,\n",
       "       5, 6, 4, 4, 3, 1, 2, 3, 7, 2, 1, 1, 3, 7, 3, 7, 2, 3, 7, 2, 7, 5,\n",
       "       7, 3, 1, 1, 5, 3, 1, 2, 1, 3, 6, 4, 5, 3, 1, 1, 5, 3, 4, 7, 5, 7,\n",
       "       4, 5, 1, 4, 1, 4, 5, 2, 7, 2, 6, 0, 3, 2, 1, 7, 3, 2, 1, 2, 5, 2,\n",
       "       2, 5, 7, 5, 1, 1, 3, 5, 2, 2, 3, 2, 4, 1, 4, 3, 7, 1, 6, 5, 1, 7,\n",
       "       5, 3, 5, 7, 5, 2, 2, 1, 2, 5, 6, 0, 2, 2, 3, 5, 7, 5, 4, 4, 7, 1,\n",
       "       2, 4, 5, 3, 2, 0, 7, 3, 4, 4, 4, 5, 0, 3, 6, 6, 1, 5, 4, 7, 5, 3,\n",
       "       7, 2, 5, 5, 2, 0, 0, 3, 5, 5, 0, 3, 3, 4, 1, 1, 3, 4, 7, 7, 5, 6,\n",
       "       7, 3, 6, 4, 0, 1, 5, 3, 5, 0, 1, 5, 6, 3, 4, 1, 1, 3, 7, 2, 0, 4,\n",
       "       2, 1, 5, 0, 6, 7, 1, 3, 6, 5, 1, 4, 1, 2, 7, 3, 1, 5, 6, 1, 3, 4,\n",
       "       4, 3, 4, 5, 2, 1, 5, 1, 4, 6, 1, 7, 3, 4, 3, 1, 1, 2, 3, 7, 1, 3,\n",
       "       3, 2, 4, 4, 7, 2, 4, 1, 1, 0, 7, 6, 5, 0, 2, 3, 6, 3, 2, 1, 5, 5,\n",
       "       3, 5, 2, 1, 2, 5, 3, 3, 4, 5, 0, 0, 1, 5, 5, 6, 1, 0, 7, 6, 2, 5,\n",
       "       3, 6, 5, 7, 5, 1, 7, 6, 4, 5, 5, 5, 4, 4, 3, 4, 1, 5, 1, 5, 2, 0,\n",
       "       3, 3, 2, 5, 4, 7, 0, 6, 6, 3, 5, 1, 2, 4, 2, 5, 1, 5, 5, 2, 6, 4,\n",
       "       3, 2, 0, 1, 5, 3, 5, 4, 2, 0, 1, 4, 5, 7, 2, 7, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HUHshx93CM_6",
    "outputId": "5b33758e-9a1a-403d-9679-19c0e6a2c0e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 1, 1, 1, 1, 2, 4, 4, 0, 6, 5, 2, 1, 3, 2, 4, 5, 3, 6, 4,\n",
       "       0, 1, 6, 4, 2, 6, 7, 7, 4, 5, 3, 7, 4, 4, 1, 3, 4, 3, 3, 5, 2, 2,\n",
       "       7, 4, 5, 3, 6, 3, 1, 4, 6, 3, 1, 4, 5, 0, 5, 2, 1, 3, 5, 3, 1, 3,\n",
       "       2, 4, 6, 3, 4, 2, 6, 5, 5, 2, 2, 0, 5, 3, 7, 3, 3, 1, 4, 4, 0, 4,\n",
       "       3, 5, 1, 5, 4, 2, 2, 1, 3, 5, 4, 7, 6, 2, 1, 7, 3, 2, 7, 2, 3, 4,\n",
       "       7, 4, 4, 5, 1, 1, 0, 1, 7, 1, 2, 1, 6, 0, 2, 2, 3, 2, 2, 3, 5, 0,\n",
       "       4, 6, 0, 7, 3, 7, 7, 1, 5, 1, 2, 7, 4, 4, 5, 2, 2, 4, 6, 7, 7, 2,\n",
       "       6, 5, 3, 6, 2, 4, 4, 1, 4, 4, 3, 1, 6, 0, 5, 1, 5, 2, 1, 5, 1, 2,\n",
       "       1, 7, 2, 2, 6, 1, 2, 2, 1, 1, 1, 4, 7, 1, 3, 3, 5, 3, 0, 5, 1, 0,\n",
       "       4, 1, 1, 4, 3, 3, 7, 4, 3, 1, 3, 4, 7, 0, 5, 3, 5, 0, 2, 1, 1, 2,\n",
       "       6, 1, 5, 3, 4, 2, 3, 0, 2, 4, 3, 5, 5, 4, 2, 7, 4, 5, 3, 3, 0, 7,\n",
       "       1, 5, 3, 0, 4, 3, 3, 3, 2, 5, 2, 4, 2, 4, 3, 2, 3, 0, 3, 4, 5, 3,\n",
       "       1, 3, 1, 5, 2, 7, 1, 7, 4, 1, 1, 2, 0, 0, 2, 1, 4, 5, 5, 2, 6, 5,\n",
       "       5, 3, 3, 0, 4, 0, 2, 1, 5, 6, 6, 5, 1, 4, 2, 0, 0, 1, 6, 3, 7, 0,\n",
       "       1, 7, 2, 1, 2, 1, 6, 1, 3, 2, 6, 3, 5, 5, 3, 3, 4, 7, 7, 1, 4, 5,\n",
       "       2, 5, 6, 2, 4, 1, 1, 4, 5, 3, 0, 1, 4, 2, 1, 7, 5, 2, 1, 5, 2, 7,\n",
       "       1, 2, 3, 5, 1, 7, 2, 4, 0, 4, 3, 6, 4, 6, 6, 1, 3, 1, 5, 1, 2, 2,\n",
       "       3, 2, 3, 3, 3, 1, 4, 4, 1, 2, 2, 0, 7, 5, 4, 4, 2, 4, 4, 4, 7, 4,\n",
       "       1, 2, 7, 2, 2, 3, 7, 7, 3, 4, 5, 5, 1, 5, 3, 5, 3, 4, 1, 5, 4, 4,\n",
       "       3, 6, 5, 7, 3, 0, 1, 0, 3, 0, 1, 4, 4, 7, 0, 1, 3, 2, 3, 6, 2, 4,\n",
       "       2, 6, 4, 2, 7, 2, 3, 5, 1, 1, 1, 7, 5, 0, 1, 1, 4, 1, 5, 3, 4, 4,\n",
       "       5, 6, 4, 6, 4, 2, 2, 0, 5, 3, 5, 7, 3, 7, 4, 3, 5, 6, 6, 5, 5, 0,\n",
       "       5, 6, 4, 4, 3, 1, 2, 3, 6, 2, 0, 1, 3, 4, 3, 5, 7, 5, 3, 2, 2, 5,\n",
       "       7, 0, 1, 1, 2, 3, 6, 2, 1, 3, 1, 4, 5, 3, 1, 1, 5, 7, 5, 7, 4, 7,\n",
       "       5, 4, 1, 4, 1, 6, 5, 2, 3, 2, 6, 0, 3, 2, 4, 7, 5, 2, 1, 2, 5, 2,\n",
       "       2, 2, 0, 5, 1, 1, 3, 6, 5, 2, 3, 2, 4, 2, 4, 1, 7, 3, 7, 5, 1, 1,\n",
       "       2, 3, 3, 3, 5, 2, 2, 1, 2, 5, 6, 0, 6, 5, 3, 4, 7, 5, 4, 2, 4, 1,\n",
       "       4, 4, 2, 3, 2, 2, 2, 3, 4, 4, 4, 5, 0, 3, 6, 6, 1, 5, 2, 7, 4, 3,\n",
       "       7, 4, 5, 5, 2, 0, 0, 1, 5, 5, 4, 3, 5, 2, 1, 1, 3, 4, 7, 6, 2, 6,\n",
       "       7, 3, 6, 4, 2, 1, 5, 5, 6, 0, 1, 5, 3, 4, 4, 1, 1, 4, 4, 4, 3, 7,\n",
       "       2, 1, 5, 1, 6, 7, 1, 5, 1, 5, 1, 0, 1, 2, 3, 3, 1, 3, 4, 1, 3, 4,\n",
       "       5, 3, 6, 6, 6, 0, 5, 1, 4, 3, 1, 1, 3, 4, 3, 1, 6, 2, 3, 7, 1, 3,\n",
       "       3, 2, 4, 2, 5, 2, 6, 1, 1, 0, 7, 6, 2, 0, 2, 3, 7, 3, 5, 1, 5, 5,\n",
       "       5, 6, 5, 1, 2, 5, 3, 4, 4, 5, 0, 0, 1, 5, 5, 0, 2, 0, 7, 6, 2, 5,\n",
       "       5, 4, 5, 7, 5, 1, 7, 6, 4, 5, 5, 5, 4, 4, 3, 4, 1, 5, 1, 5, 2, 0,\n",
       "       3, 3, 2, 5, 4, 6, 0, 2, 6, 6, 5, 1, 2, 4, 2, 3, 1, 5, 5, 2, 1, 4,\n",
       "       5, 7, 0, 1, 2, 3, 5, 4, 2, 0, 6, 4, 5, 0, 2, 7, 3, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMxojpvWCxOs"
   },
   "outputs": [],
   "source": [
    "new_Ytest = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W07EQaC8DE6i",
    "outputId": "9e7d7f0f-8cd3-4068-e42e-4e23c22f8a71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 1, 1, 1, 1, 2, 4, 4, 0, 6, 5, 2, 1, 3, 2, 4, 5, 3, 6, 4,\n",
       "       0, 1, 6, 4, 2, 6, 7, 7, 4, 5, 3, 7, 4, 4, 1, 3, 4, 3, 3, 5, 2, 2,\n",
       "       7, 4, 5, 3, 6, 3, 1, 4, 6, 3, 1, 4, 5, 0, 5, 2, 1, 3, 5, 3, 1, 3,\n",
       "       2, 4, 6, 3, 4, 2, 6, 5, 5, 2, 2, 0, 5, 3, 7, 3, 3, 1, 4, 4, 0, 4,\n",
       "       3, 5, 1, 5, 4, 2, 2, 1, 3, 5, 4, 7, 6, 2, 1, 7, 3, 2, 7, 2, 3, 4,\n",
       "       7, 4, 4, 5, 1, 1, 0, 1, 7, 1, 2, 1, 6, 0, 2, 2, 3, 2, 2, 3, 5, 0,\n",
       "       4, 6, 0, 7, 3, 7, 7, 1, 5, 1, 2, 7, 4, 4, 5, 2, 2, 4, 6, 7, 7, 2,\n",
       "       6, 5, 3, 6, 2, 4, 4, 1, 4, 4, 3, 1, 6, 0, 5, 1, 5, 2, 1, 5, 1, 2,\n",
       "       1, 7, 2, 2, 6, 1, 2, 2, 1, 1, 1, 4, 7, 1, 3, 3, 5, 3, 0, 5, 1, 0,\n",
       "       4, 1, 1, 4, 3, 3, 7, 4, 3, 1, 3, 4, 7, 0, 5, 3, 5, 0, 2, 1, 1, 2,\n",
       "       6, 1, 5, 3, 4, 2, 3, 0, 2, 4, 3, 5, 5, 4, 2, 7, 4, 5, 3, 3, 0, 7,\n",
       "       1, 5, 3, 0, 4, 3, 3, 3, 2, 5, 2, 4, 2, 4, 3, 2, 3, 0, 3, 4, 5, 3,\n",
       "       1, 3, 1, 5, 2, 7, 1, 7, 4, 1, 1, 2, 0, 0, 2, 1, 4, 5, 5, 2, 6, 5,\n",
       "       5, 3, 3, 0, 4, 0, 2, 1, 5, 6, 6, 5, 1, 4, 2, 0, 0, 1, 6, 3, 7, 0,\n",
       "       1, 7, 2, 1, 2, 1, 6, 1, 3, 2, 6, 3, 5, 5, 3, 3, 4, 7, 7, 1, 4, 5,\n",
       "       2, 5, 6, 2, 4, 1, 1, 4, 5, 3, 0, 1, 4, 2, 1, 7, 5, 2, 1, 5, 2, 7,\n",
       "       1, 2, 3, 5, 1, 7, 2, 4, 0, 4, 3, 6, 4, 6, 6, 1, 3, 1, 5, 1, 2, 2,\n",
       "       3, 2, 3, 3, 3, 1, 4, 4, 1, 2, 2, 0, 7, 5, 4, 4, 2, 4, 4, 4, 7, 4,\n",
       "       1, 2, 7, 2, 2, 3, 7, 7, 3, 4, 5, 5, 1, 5, 3, 5, 3, 4, 1, 5, 4, 4,\n",
       "       3, 6, 5, 7, 3, 0, 1, 0, 3, 0, 1, 4, 4, 7, 0, 1, 3, 2, 3, 6, 2, 4,\n",
       "       2, 6, 4, 2, 7, 2, 3, 5, 1, 1, 1, 7, 5, 0, 1, 1, 4, 1, 5, 3, 4, 4,\n",
       "       5, 6, 4, 6, 4, 2, 2, 0, 5, 3, 5, 7, 3, 7, 4, 3, 5, 6, 6, 5, 5, 0,\n",
       "       5, 6, 4, 4, 3, 1, 2, 3, 6, 2, 0, 1, 3, 4, 3, 5, 7, 5, 3, 2, 2, 5,\n",
       "       7, 0, 1, 1, 2, 3, 6, 2, 1, 3, 1, 4, 5, 3, 1, 1, 5, 7, 5, 7, 4, 7,\n",
       "       5, 4, 1, 4, 1, 6, 5, 2, 3, 2, 6, 0, 3, 2, 4, 7, 5, 2, 1, 2, 5, 2,\n",
       "       2, 2, 0, 5, 1, 1, 3, 6, 5, 2, 3, 2, 4, 2, 4, 1, 7, 3, 7, 5, 1, 1,\n",
       "       2, 3, 3, 3, 5, 2, 2, 1, 2, 5, 6, 0, 6, 5, 3, 4, 7, 5, 4, 2, 4, 1,\n",
       "       4, 4, 2, 3, 2, 2, 2, 3, 4, 4, 4, 5, 0, 3, 6, 6, 1, 5, 2, 7, 4, 3,\n",
       "       7, 4, 5, 5, 2, 0, 0, 1, 5, 5, 4, 3, 5, 2, 1, 1, 3, 4, 7, 6, 2, 6,\n",
       "       7, 3, 6, 4, 2, 1, 5, 5, 6, 0, 1, 5, 3, 4, 4, 1, 1, 4, 4, 4, 3, 7,\n",
       "       2, 1, 5, 1, 6, 7, 1, 5, 1, 5, 1, 0, 1, 2, 3, 3, 1, 3, 4, 1, 3, 4,\n",
       "       5, 3, 6, 6, 6, 0, 5, 1, 4, 3, 1, 1, 3, 4, 3, 1, 6, 2, 3, 7, 1, 3,\n",
       "       3, 2, 4, 2, 5, 2, 6, 1, 1, 0, 7, 6, 2, 0, 2, 3, 7, 3, 5, 1, 5, 5,\n",
       "       5, 6, 5, 1, 2, 5, 3, 4, 4, 5, 0, 0, 1, 5, 5, 0, 2, 0, 7, 6, 2, 5,\n",
       "       5, 4, 5, 7, 5, 1, 7, 6, 4, 5, 5, 5, 4, 4, 3, 4, 1, 5, 1, 5, 2, 0,\n",
       "       3, 3, 2, 5, 4, 6, 0, 2, 6, 6, 5, 1, 2, 4, 2, 3, 1, 5, 5, 2, 1, 4,\n",
       "       5, 7, 0, 1, 2, 3, 5, 4, 2, 0, 6, 4, 5, 0, 2, 7, 3, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hu1S5IowfSDG"
   },
   "source": [
    "Now, the confusion matrix: it will show us the misclassified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "fdy09SCEd7Cl",
    "outputId": "4fd020f5-74c5-40e7-8b54-076c944901be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 38   6   0   8   1   1   1   2]\n",
      " [  4 107   3   7   0   2   5   2]\n",
      " [  3   7  84   3   7  13   2   7]\n",
      " [  3  14   1  83   0  10   4   8]\n",
      " [  3   2   4   3  88  10   6   6]\n",
      " [  0   0   9  17   4  87   0   7]\n",
      " [  1   4   2   2   5   4  38   7]\n",
      " [  0   0   4   3   1   0   9  48]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(new_Ytest, predictions)\n",
    "print (matrix)\n",
    "\n",
    "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        57\n",
      "           1       0.76      0.82      0.79       130\n",
      "           2       0.79      0.67      0.72       126\n",
      "           3       0.66      0.67      0.67       123\n",
      "           4       0.83      0.72      0.77       122\n",
      "           5       0.69      0.70      0.69       124\n",
      "           6       0.58      0.60      0.59        63\n",
      "           7       0.55      0.74      0.63        65\n",
      "\n",
      "    accuracy                           0.71       810\n",
      "   macro avg       0.70      0.70      0.70       810\n",
      "weighted avg       0.72      0.71      0.71       810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_ySPOyHxkZ3"
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f5kRmoD-sdHj",
    "outputId": "99ad6a5b-a4a6-42bc-ed78-229864c33d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at ../model/Emotion_Voice_Detection_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = '../model/'\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EmotionsRecognition.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "43a24ecce625020f2d6631fb4cfb730bba30d877e9fe9ec2e0d85cb5a52a2b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
