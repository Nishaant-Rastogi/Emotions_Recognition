{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjWvnaQUrZmD"
   },
   "source": [
    "# Emotion classification using the RAVDESS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDNbxj45rkvB"
   },
   "source": [
    "# Analysis\n",
    "\n",
    "We are will first install LibROSA, a python package for music and audio analysis.\n",
    "\n",
    "After the import, we will plot the signal of the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "EgFwaDhMbJVm",
    "outputId": "d1f5d32b-177d-4858-c366-d43a6b8783ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.9.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (21.3)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.19.5)\n",
      "Requirement already satisfied: numba>=0.45.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (0.55.2)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.7.3)\n",
      "Requirement already satisfied: resampy>=0.2.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (0.3.1)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from numba>=0.45.1->librosa) (0.38.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from numba>=0.45.1->librosa) (62.3.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from packaging>=20.0->librosa) (3.0.9)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pooch>=1.0->librosa) (2.27.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2021.10.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxI4xzngdS-e"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "from librosa import display\n",
    "\n",
    "data, sampling_rate = librosa.load('../features/Actor_01/03-01-01-01-01-01-01.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCtNuVWlr5jL"
   },
   "source": [
    "# Load all files\n",
    "\n",
    "We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AKvuF--gd6F-",
    "outputId": "4fbbbdc4-3bce-47b3-812c-1dd9e3938159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data loaded. Loading time: 1022.1491544246674 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "path = '../features/'\n",
    "lst = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "  for file in files:\n",
    "      try:\n",
    "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
    "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
    "        file = int(file[7:8]) - 1 \n",
    "        arr = mfccs, file\n",
    "        lst.append(arr)\n",
    "      # If the file is not valid, skip it\n",
    "      except ValueError:\n",
    "        continue\n",
    "\n",
    "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLSggnF7kKY1"
   },
   "outputs": [],
   "source": [
    "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
    "X, y = zip(*lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VzvBRTJIlIE9",
    "outputId": "6eb806ec-f065-4420-d526-c1fa8d00c26c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2452, 40), (2452,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOutQiAlCjOY"
   },
   "outputs": [],
   "source": [
    "# Saving joblib files to not load them again with the loop above\n",
    "\n",
    "import joblib\n",
    "\n",
    "X_name = 'X.joblib'\n",
    "y_name = 'y.joblib'\n",
    "save_dir = './model/'\n",
    "\n",
    "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
    "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIoFdycUXMxA"
   },
   "outputs": [],
   "source": [
    "# Loading saved models\n",
    "\n",
    "X = joblib.load('./model/X.joblib')\n",
    "y = joblib.load('./model/y.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Agw-3KN1sDhh"
   },
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "To make a first attempt in accomplishing this classification task I chose a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-Xgb5NslTBO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UshLOC1ClWL3"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BnCR52nlXw0"
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "qWyTownblZM0",
    "outputId": "61708923-f907-442b-86d7-b3e5b2840c29"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEuw6TUQlr7C"
   },
   "outputs": [],
   "source": [
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1v0i0V7sMw7"
   },
   "source": [
    "Let's go with our classification report.\n",
    "\n",
    "Before we start, a quick reminder of the classes we are trying to predict:\n",
    "\n",
    "emotions = {\n",
    "    \"neutral\": \"0\",\n",
    "    \"calm\": \"1\",\n",
    "    \"happy\": \"2\",\n",
    "    \"sad\": \"3\",\n",
    "    \"angry\": \"4\", \n",
    "    \"fearful\": \"5\", \n",
    "    \"disgust\": \"6\", \n",
    "    \"surprised\": \"7\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "c4kNSYkAleIv",
    "outputId": "fb407f71-b7de-4a15-cee9-527a69d45c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.51      0.43        57\n",
      "           1       0.64      0.58      0.60       130\n",
      "           2       0.45      0.47      0.46       126\n",
      "           3       0.44      0.40      0.42       123\n",
      "           4       0.59      0.60      0.60       122\n",
      "           5       0.46      0.43      0.44       124\n",
      "           6       0.31      0.30      0.30        63\n",
      "           7       0.44      0.48      0.46        65\n",
      "\n",
      "    accuracy                           0.48       810\n",
      "   macro avg       0.46      0.47      0.46       810\n",
      "weighted avg       0.48      0.48      0.48       810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCVgjLj-gwE2"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jfaTxzZ1w__y"
   },
   "source": [
    "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
    "\n",
    "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcov_DCXgs7v"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eo0ljqzg-KM"
   },
   "outputs": [],
   "source": [
    "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
    "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
    "                                 n_estimators= 22000, random_state= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Tg45qSOfg-26",
    "outputId": "c31df1c1-9342-4485-b7f8-211cc4077e7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10, max_features=&#x27;log2&#x27;, max_leaf_nodes=100,\n",
       "                       min_samples_leaf=3, min_samples_split=20,\n",
       "                       n_estimators=22000, random_state=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, max_features=&#x27;log2&#x27;, max_leaf_nodes=100,\n",
       "                       min_samples_leaf=3, min_samples_split=20,\n",
       "                       n_estimators=22000, random_state=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
       "                       min_samples_leaf=3, min_samples_split=20,\n",
       "                       n_estimators=22000, random_state=5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aM8KU3qxhGBM"
   },
   "outputs": [],
   "source": [
    "predictions = rforest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "296FW5sBdanI",
    "outputId": "a9fbfcc2-f9c5-4a3d-9bc0-2a6f513ba609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.44      0.60        57\n",
      "           1       0.64      0.88      0.74       130\n",
      "           2       0.72      0.51      0.60       126\n",
      "           3       0.55      0.63      0.59       123\n",
      "           4       0.68      0.77      0.72       122\n",
      "           5       0.58      0.54      0.56       124\n",
      "           6       0.47      0.30      0.37        63\n",
      "           7       0.46      0.57      0.51        65\n",
      "\n",
      "    accuracy                           0.62       810\n",
      "   macro avg       0.63      0.58      0.59       810\n",
      "weighted avg       0.63      0.62      0.61       810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9eqMHV3S8i6"
   },
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-QscoyMxQtn"
   },
   "source": [
    "Let's build our neural network!\n",
    "\n",
    "To do so, we need to expand the dimensions of our array, adding a third one using the numpy \"expand_dims\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4i187-Pe-w5"
   },
   "outputs": [],
   "source": [
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vnvoCRX1gQCh",
    "outputId": "cc9d5f67-0c48-443c-e6b1-6798d200529e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1642, 40, 1), (810, 40, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_traincnn.shape, x_testcnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "HZOGIpuefCd3",
    "outputId": "4fe2802a-3147-4725-d79b-3c9cbe993e16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(40,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LphftMIZzUvz"
   },
   "source": [
    "With *model.summary* we can see a recap of what we have build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "pIWPB4Zgfic7",
    "outputId": "49b5d344-637a-452e-c730-76f5471ea889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 40, 128)           768       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 5128      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 87,944\n",
      "Trainable params: 87,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qQSBeBhzcLu"
   },
   "source": [
    "Now we can compile and fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "iNI1znbsfpTx",
    "outputId": "872a1dbe-5206-4d6c-a7ce-943b585f4a9e"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ktdF-nJKfq6F",
    "outputId": "a05f0852-1564-4425-8885-bcea35dd0e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "103/103 [==============================] - 2s 8ms/step - loss: 7.2354 - accuracy: 0.1352 - val_loss: 2.6281 - val_accuracy: 0.1975\n",
      "Epoch 2/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 5.6404 - accuracy: 0.1468 - val_loss: 2.1162 - val_accuracy: 0.1802\n",
      "Epoch 3/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 5.0534 - accuracy: 0.1498 - val_loss: 2.7041 - val_accuracy: 0.1556\n",
      "Epoch 4/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 4.5286 - accuracy: 0.1553 - val_loss: 2.3140 - val_accuracy: 0.2074\n",
      "Epoch 5/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 3.8277 - accuracy: 0.1754 - val_loss: 2.0492 - val_accuracy: 0.1914\n",
      "Epoch 6/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 3.4764 - accuracy: 0.1784 - val_loss: 1.8134 - val_accuracy: 0.2975\n",
      "Epoch 7/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 2.9858 - accuracy: 0.1918 - val_loss: 1.9426 - val_accuracy: 0.2284\n",
      "Epoch 8/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 2.7732 - accuracy: 0.2052 - val_loss: 1.9728 - val_accuracy: 0.2642\n",
      "Epoch 9/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 2.4350 - accuracy: 0.2199 - val_loss: 1.8581 - val_accuracy: 0.2864\n",
      "Epoch 10/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 2.2495 - accuracy: 0.2235 - val_loss: 1.7954 - val_accuracy: 0.2914\n",
      "Epoch 11/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 2.1537 - accuracy: 0.2558 - val_loss: 1.8028 - val_accuracy: 0.3037\n",
      "Epoch 12/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 2.0559 - accuracy: 0.2546 - val_loss: 1.7705 - val_accuracy: 0.3321\n",
      "Epoch 13/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.9880 - accuracy: 0.2503 - val_loss: 1.7743 - val_accuracy: 0.3148\n",
      "Epoch 14/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.9503 - accuracy: 0.2881 - val_loss: 1.7757 - val_accuracy: 0.2988\n",
      "Epoch 15/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.9221 - accuracy: 0.2747 - val_loss: 1.7241 - val_accuracy: 0.3025\n",
      "Epoch 16/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.8609 - accuracy: 0.2917 - val_loss: 1.7342 - val_accuracy: 0.3259\n",
      "Epoch 17/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.8453 - accuracy: 0.2972 - val_loss: 1.7340 - val_accuracy: 0.3605\n",
      "Epoch 18/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.8386 - accuracy: 0.2850 - val_loss: 1.6973 - val_accuracy: 0.3815\n",
      "Epoch 19/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.7969 - accuracy: 0.3191 - val_loss: 1.6920 - val_accuracy: 0.3667\n",
      "Epoch 20/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.7704 - accuracy: 0.3118 - val_loss: 1.6538 - val_accuracy: 0.3889\n",
      "Epoch 21/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.7515 - accuracy: 0.3258 - val_loss: 1.6795 - val_accuracy: 0.3914\n",
      "Epoch 22/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.7586 - accuracy: 0.3465 - val_loss: 1.6494 - val_accuracy: 0.3802\n",
      "Epoch 23/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.7245 - accuracy: 0.3417 - val_loss: 1.6449 - val_accuracy: 0.3877\n",
      "Epoch 24/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.7210 - accuracy: 0.3410 - val_loss: 1.6200 - val_accuracy: 0.4049\n",
      "Epoch 25/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.7020 - accuracy: 0.3404 - val_loss: 1.6125 - val_accuracy: 0.4259\n",
      "Epoch 26/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.7141 - accuracy: 0.3465 - val_loss: 1.6201 - val_accuracy: 0.3815\n",
      "Epoch 27/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.7020 - accuracy: 0.3563 - val_loss: 1.5981 - val_accuracy: 0.4185\n",
      "Epoch 28/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.6836 - accuracy: 0.3544 - val_loss: 1.5855 - val_accuracy: 0.4222\n",
      "Epoch 29/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.6580 - accuracy: 0.3654 - val_loss: 1.5958 - val_accuracy: 0.3951\n",
      "Epoch 30/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.6550 - accuracy: 0.3867 - val_loss: 1.5811 - val_accuracy: 0.4049\n",
      "Epoch 31/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.6203 - accuracy: 0.3837 - val_loss: 1.5526 - val_accuracy: 0.4383\n",
      "Epoch 32/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.6252 - accuracy: 0.3721 - val_loss: 1.5459 - val_accuracy: 0.4531\n",
      "Epoch 33/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.6219 - accuracy: 0.3867 - val_loss: 1.5519 - val_accuracy: 0.4284\n",
      "Epoch 34/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.6123 - accuracy: 0.3983 - val_loss: 1.5386 - val_accuracy: 0.4407\n",
      "Epoch 35/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.5957 - accuracy: 0.3946 - val_loss: 1.5240 - val_accuracy: 0.4519\n",
      "Epoch 36/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.5811 - accuracy: 0.4080 - val_loss: 1.5166 - val_accuracy: 0.4556\n",
      "Epoch 37/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.5825 - accuracy: 0.4019 - val_loss: 1.5083 - val_accuracy: 0.4444\n",
      "Epoch 38/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.5487 - accuracy: 0.4172 - val_loss: 1.5030 - val_accuracy: 0.4284\n",
      "Epoch 39/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.5582 - accuracy: 0.4306 - val_loss: 1.5004 - val_accuracy: 0.4358\n",
      "Epoch 40/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.5463 - accuracy: 0.4013 - val_loss: 1.5009 - val_accuracy: 0.4716\n",
      "Epoch 41/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.5366 - accuracy: 0.4056 - val_loss: 1.4804 - val_accuracy: 0.4370\n",
      "Epoch 42/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.5159 - accuracy: 0.4342 - val_loss: 1.4933 - val_accuracy: 0.4185\n",
      "Epoch 43/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.5255 - accuracy: 0.4263 - val_loss: 1.4653 - val_accuracy: 0.4630\n",
      "Epoch 44/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.5210 - accuracy: 0.4220 - val_loss: 1.4719 - val_accuracy: 0.4605\n",
      "Epoch 45/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.5042 - accuracy: 0.4263 - val_loss: 1.4661 - val_accuracy: 0.4543\n",
      "Epoch 46/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.5113 - accuracy: 0.4294 - val_loss: 1.4447 - val_accuracy: 0.4679\n",
      "Epoch 47/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.4953 - accuracy: 0.4409 - val_loss: 1.4487 - val_accuracy: 0.4667\n",
      "Epoch 48/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.5062 - accuracy: 0.4409 - val_loss: 1.4303 - val_accuracy: 0.4778\n",
      "Epoch 49/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.4768 - accuracy: 0.4464 - val_loss: 1.4293 - val_accuracy: 0.4741\n",
      "Epoch 50/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.4532 - accuracy: 0.4501 - val_loss: 1.4179 - val_accuracy: 0.4815\n",
      "Epoch 51/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.4660 - accuracy: 0.4428 - val_loss: 1.4173 - val_accuracy: 0.5123\n",
      "Epoch 52/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.4737 - accuracy: 0.4495 - val_loss: 1.4143 - val_accuracy: 0.4988\n",
      "Epoch 53/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.4441 - accuracy: 0.4732 - val_loss: 1.4153 - val_accuracy: 0.4877\n",
      "Epoch 54/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.4370 - accuracy: 0.4580 - val_loss: 1.4010 - val_accuracy: 0.4815\n",
      "Epoch 55/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.4173 - accuracy: 0.4683 - val_loss: 1.3868 - val_accuracy: 0.5086\n",
      "Epoch 56/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.4320 - accuracy: 0.4543 - val_loss: 1.3783 - val_accuracy: 0.5012\n",
      "Epoch 57/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.4303 - accuracy: 0.4665 - val_loss: 1.3823 - val_accuracy: 0.5000\n",
      "Epoch 58/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.4058 - accuracy: 0.4854 - val_loss: 1.3689 - val_accuracy: 0.5111\n",
      "Epoch 59/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.4203 - accuracy: 0.4586 - val_loss: 1.3746 - val_accuracy: 0.5012\n",
      "Epoch 60/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.4067 - accuracy: 0.4799 - val_loss: 1.3844 - val_accuracy: 0.4840\n",
      "Epoch 61/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.3949 - accuracy: 0.4756 - val_loss: 1.3595 - val_accuracy: 0.5025\n",
      "Epoch 62/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.4129 - accuracy: 0.4872 - val_loss: 1.3620 - val_accuracy: 0.5074\n",
      "Epoch 63/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.4117 - accuracy: 0.4836 - val_loss: 1.3613 - val_accuracy: 0.5049\n",
      "Epoch 64/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.3834 - accuracy: 0.4915 - val_loss: 1.3513 - val_accuracy: 0.5185\n",
      "Epoch 65/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.3829 - accuracy: 0.4805 - val_loss: 1.3338 - val_accuracy: 0.5309\n",
      "Epoch 66/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.3770 - accuracy: 0.4872 - val_loss: 1.3408 - val_accuracy: 0.5173\n",
      "Epoch 67/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.3777 - accuracy: 0.4823 - val_loss: 1.3682 - val_accuracy: 0.4790\n",
      "Epoch 68/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.3599 - accuracy: 0.5018 - val_loss: 1.3249 - val_accuracy: 0.5210\n",
      "Epoch 69/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.3483 - accuracy: 0.4982 - val_loss: 1.3331 - val_accuracy: 0.5222\n",
      "Epoch 70/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.3497 - accuracy: 0.4957 - val_loss: 1.3451 - val_accuracy: 0.5074\n",
      "Epoch 71/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.3501 - accuracy: 0.4890 - val_loss: 1.3334 - val_accuracy: 0.5086\n",
      "Epoch 72/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.3425 - accuracy: 0.4982 - val_loss: 1.3071 - val_accuracy: 0.5346\n",
      "Epoch 73/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.3312 - accuracy: 0.5067 - val_loss: 1.3051 - val_accuracy: 0.5185\n",
      "Epoch 74/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.3266 - accuracy: 0.5104 - val_loss: 1.3120 - val_accuracy: 0.5259\n",
      "Epoch 75/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.3291 - accuracy: 0.5104 - val_loss: 1.3135 - val_accuracy: 0.5284\n",
      "Epoch 76/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.3179 - accuracy: 0.5189 - val_loss: 1.3101 - val_accuracy: 0.5420\n",
      "Epoch 77/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.3306 - accuracy: 0.5079 - val_loss: 1.2972 - val_accuracy: 0.5247\n",
      "Epoch 78/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.3132 - accuracy: 0.5152 - val_loss: 1.2847 - val_accuracy: 0.5383\n",
      "Epoch 79/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.3031 - accuracy: 0.5207 - val_loss: 1.2943 - val_accuracy: 0.5543\n",
      "Epoch 80/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.2976 - accuracy: 0.5164 - val_loss: 1.2788 - val_accuracy: 0.5605\n",
      "Epoch 81/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.3073 - accuracy: 0.5219 - val_loss: 1.2713 - val_accuracy: 0.5543\n",
      "Epoch 82/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.2919 - accuracy: 0.5378 - val_loss: 1.3006 - val_accuracy: 0.5259\n",
      "Epoch 83/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.2823 - accuracy: 0.5219 - val_loss: 1.2775 - val_accuracy: 0.5370\n",
      "Epoch 84/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.2884 - accuracy: 0.5091 - val_loss: 1.2703 - val_accuracy: 0.5568\n",
      "Epoch 85/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.2726 - accuracy: 0.5231 - val_loss: 1.2791 - val_accuracy: 0.5284\n",
      "Epoch 86/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.2632 - accuracy: 0.5280 - val_loss: 1.2565 - val_accuracy: 0.5605\n",
      "Epoch 87/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.2726 - accuracy: 0.5402 - val_loss: 1.2543 - val_accuracy: 0.5778\n",
      "Epoch 88/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.2646 - accuracy: 0.5286 - val_loss: 1.2580 - val_accuracy: 0.5469\n",
      "Epoch 89/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.2554 - accuracy: 0.5420 - val_loss: 1.2452 - val_accuracy: 0.5642\n",
      "Epoch 90/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.2740 - accuracy: 0.5274 - val_loss: 1.2487 - val_accuracy: 0.5556\n",
      "Epoch 91/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.2507 - accuracy: 0.5359 - val_loss: 1.2486 - val_accuracy: 0.5642\n",
      "Epoch 92/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.2373 - accuracy: 0.5591 - val_loss: 1.2766 - val_accuracy: 0.5272\n",
      "Epoch 93/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.2399 - accuracy: 0.5560 - val_loss: 1.2303 - val_accuracy: 0.5778\n",
      "Epoch 94/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.2340 - accuracy: 0.5463 - val_loss: 1.2220 - val_accuracy: 0.5679\n",
      "Epoch 95/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2232 - accuracy: 0.5560 - val_loss: 1.2626 - val_accuracy: 0.5358\n",
      "Epoch 96/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2330 - accuracy: 0.5371 - val_loss: 1.2311 - val_accuracy: 0.5593\n",
      "Epoch 97/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2167 - accuracy: 0.5597 - val_loss: 1.2558 - val_accuracy: 0.5247\n",
      "Epoch 98/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.2025 - accuracy: 0.5646 - val_loss: 1.2304 - val_accuracy: 0.5481\n",
      "Epoch 99/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2167 - accuracy: 0.5579 - val_loss: 1.2069 - val_accuracy: 0.5617\n",
      "Epoch 100/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2120 - accuracy: 0.5542 - val_loss: 1.2207 - val_accuracy: 0.5753\n",
      "Epoch 101/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.1812 - accuracy: 0.5585 - val_loss: 1.2043 - val_accuracy: 0.5827\n",
      "Epoch 102/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.1928 - accuracy: 0.5603 - val_loss: 1.2320 - val_accuracy: 0.5481\n",
      "Epoch 103/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.2070 - accuracy: 0.5633 - val_loss: 1.1987 - val_accuracy: 0.5580\n",
      "Epoch 104/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.1938 - accuracy: 0.5609 - val_loss: 1.2029 - val_accuracy: 0.5753\n",
      "Epoch 105/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.2105 - accuracy: 0.5597 - val_loss: 1.2031 - val_accuracy: 0.5778\n",
      "Epoch 106/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.1947 - accuracy: 0.5579 - val_loss: 1.1966 - val_accuracy: 0.5951\n",
      "Epoch 107/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1755 - accuracy: 0.5834 - val_loss: 1.1906 - val_accuracy: 0.5679\n",
      "Epoch 108/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.1919 - accuracy: 0.5579 - val_loss: 1.1784 - val_accuracy: 0.5901\n",
      "Epoch 109/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1950 - accuracy: 0.5481 - val_loss: 1.1800 - val_accuracy: 0.5790\n",
      "Epoch 110/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.1829 - accuracy: 0.5761 - val_loss: 1.1647 - val_accuracy: 0.5852\n",
      "Epoch 111/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.1811 - accuracy: 0.5627 - val_loss: 1.2081 - val_accuracy: 0.5679\n",
      "Epoch 112/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.1612 - accuracy: 0.5792 - val_loss: 1.1779 - val_accuracy: 0.5815\n",
      "Epoch 113/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.1576 - accuracy: 0.5731 - val_loss: 1.2031 - val_accuracy: 0.5543\n",
      "Epoch 114/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.1568 - accuracy: 0.5804 - val_loss: 1.1814 - val_accuracy: 0.5840\n",
      "Epoch 115/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1671 - accuracy: 0.5816 - val_loss: 1.1686 - val_accuracy: 0.5852\n",
      "Epoch 116/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1512 - accuracy: 0.5713 - val_loss: 1.1732 - val_accuracy: 0.6037\n",
      "Epoch 117/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.1464 - accuracy: 0.5859 - val_loss: 1.1668 - val_accuracy: 0.5852\n",
      "Epoch 118/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.1543 - accuracy: 0.5847 - val_loss: 1.1555 - val_accuracy: 0.6037\n",
      "Epoch 119/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.1296 - accuracy: 0.5914 - val_loss: 1.1568 - val_accuracy: 0.5815\n",
      "Epoch 120/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.1262 - accuracy: 0.5932 - val_loss: 1.1555 - val_accuracy: 0.5926\n",
      "Epoch 121/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.1371 - accuracy: 0.5706 - val_loss: 1.1977 - val_accuracy: 0.5630\n",
      "Epoch 122/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.1266 - accuracy: 0.5968 - val_loss: 1.1547 - val_accuracy: 0.5963\n",
      "Epoch 123/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.1257 - accuracy: 0.5920 - val_loss: 1.1471 - val_accuracy: 0.6074\n",
      "Epoch 124/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.1178 - accuracy: 0.5871 - val_loss: 1.1462 - val_accuracy: 0.6025\n",
      "Epoch 125/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.1090 - accuracy: 0.5895 - val_loss: 1.1615 - val_accuracy: 0.5704\n",
      "Epoch 126/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1150 - accuracy: 0.5804 - val_loss: 1.1414 - val_accuracy: 0.6025\n",
      "Epoch 127/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.1197 - accuracy: 0.5871 - val_loss: 1.1397 - val_accuracy: 0.5988\n",
      "Epoch 128/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1107 - accuracy: 0.5859 - val_loss: 1.1540 - val_accuracy: 0.5926\n",
      "Epoch 129/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.1104 - accuracy: 0.6072 - val_loss: 1.1614 - val_accuracy: 0.5741\n",
      "Epoch 130/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.1118 - accuracy: 0.5889 - val_loss: 1.1342 - val_accuracy: 0.6074\n",
      "Epoch 131/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1002 - accuracy: 0.5968 - val_loss: 1.1485 - val_accuracy: 0.5938\n",
      "Epoch 132/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0994 - accuracy: 0.5999 - val_loss: 1.1466 - val_accuracy: 0.6012\n",
      "Epoch 133/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.0867 - accuracy: 0.6072 - val_loss: 1.1456 - val_accuracy: 0.5679\n",
      "Epoch 134/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.0897 - accuracy: 0.6151 - val_loss: 1.1398 - val_accuracy: 0.5827\n",
      "Epoch 135/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0787 - accuracy: 0.5999 - val_loss: 1.1172 - val_accuracy: 0.6210\n",
      "Epoch 136/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.0855 - accuracy: 0.5968 - val_loss: 1.1285 - val_accuracy: 0.5877\n",
      "Epoch 137/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0884 - accuracy: 0.5962 - val_loss: 1.1223 - val_accuracy: 0.5926\n",
      "Epoch 138/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.0809 - accuracy: 0.6078 - val_loss: 1.1060 - val_accuracy: 0.5963\n",
      "Epoch 139/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0762 - accuracy: 0.6023 - val_loss: 1.1139 - val_accuracy: 0.6062\n",
      "Epoch 140/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0885 - accuracy: 0.6048 - val_loss: 1.1255 - val_accuracy: 0.5951\n",
      "Epoch 141/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0588 - accuracy: 0.6206 - val_loss: 1.1402 - val_accuracy: 0.5827\n",
      "Epoch 142/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0690 - accuracy: 0.5974 - val_loss: 1.1158 - val_accuracy: 0.5840\n",
      "Epoch 143/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.0574 - accuracy: 0.6066 - val_loss: 1.1136 - val_accuracy: 0.6000\n",
      "Epoch 144/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.0549 - accuracy: 0.6096 - val_loss: 1.1141 - val_accuracy: 0.6012\n",
      "Epoch 145/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0745 - accuracy: 0.6127 - val_loss: 1.1199 - val_accuracy: 0.6049\n",
      "Epoch 146/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.0602 - accuracy: 0.6151 - val_loss: 1.1125 - val_accuracy: 0.6062\n",
      "Epoch 147/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.0594 - accuracy: 0.6255 - val_loss: 1.1097 - val_accuracy: 0.6049\n",
      "Epoch 148/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0461 - accuracy: 0.6242 - val_loss: 1.1004 - val_accuracy: 0.6111\n",
      "Epoch 149/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.0429 - accuracy: 0.6054 - val_loss: 1.1398 - val_accuracy: 0.5852\n",
      "Epoch 150/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.0412 - accuracy: 0.6303 - val_loss: 1.0932 - val_accuracy: 0.6136\n",
      "Epoch 151/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0446 - accuracy: 0.6230 - val_loss: 1.1003 - val_accuracy: 0.6099\n",
      "Epoch 152/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0440 - accuracy: 0.6169 - val_loss: 1.1002 - val_accuracy: 0.6037\n",
      "Epoch 153/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.0448 - accuracy: 0.6248 - val_loss: 1.1206 - val_accuracy: 0.5716\n",
      "Epoch 154/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.0377 - accuracy: 0.6315 - val_loss: 1.0930 - val_accuracy: 0.6037\n",
      "Epoch 155/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0292 - accuracy: 0.6169 - val_loss: 1.1144 - val_accuracy: 0.5877\n",
      "Epoch 156/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0253 - accuracy: 0.6273 - val_loss: 1.1409 - val_accuracy: 0.5753\n",
      "Epoch 157/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0234 - accuracy: 0.6346 - val_loss: 1.0996 - val_accuracy: 0.6086\n",
      "Epoch 158/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0194 - accuracy: 0.6242 - val_loss: 1.0917 - val_accuracy: 0.5901\n",
      "Epoch 159/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0349 - accuracy: 0.6267 - val_loss: 1.0903 - val_accuracy: 0.6025\n",
      "Epoch 160/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.0004 - accuracy: 0.6425 - val_loss: 1.0855 - val_accuracy: 0.6000\n",
      "Epoch 161/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.0137 - accuracy: 0.6297 - val_loss: 1.0860 - val_accuracy: 0.6160\n",
      "Epoch 162/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.0166 - accuracy: 0.6364 - val_loss: 1.0885 - val_accuracy: 0.6025\n",
      "Epoch 163/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.0130 - accuracy: 0.6200 - val_loss: 1.0732 - val_accuracy: 0.6160\n",
      "Epoch 164/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.9961 - accuracy: 0.6285 - val_loss: 1.0927 - val_accuracy: 0.6025\n",
      "Epoch 165/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.9923 - accuracy: 0.6443 - val_loss: 1.0803 - val_accuracy: 0.6123\n",
      "Epoch 166/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.0196 - accuracy: 0.6175 - val_loss: 1.1316 - val_accuracy: 0.5802\n",
      "Epoch 167/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0003 - accuracy: 0.6529 - val_loss: 1.0650 - val_accuracy: 0.6185\n",
      "Epoch 168/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9907 - accuracy: 0.6382 - val_loss: 1.0729 - val_accuracy: 0.6321\n",
      "Epoch 169/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9884 - accuracy: 0.6468 - val_loss: 1.0648 - val_accuracy: 0.6037\n",
      "Epoch 170/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.0095 - accuracy: 0.6297 - val_loss: 1.0929 - val_accuracy: 0.6000\n",
      "Epoch 171/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9935 - accuracy: 0.6449 - val_loss: 1.0896 - val_accuracy: 0.5914\n",
      "Epoch 172/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9749 - accuracy: 0.6535 - val_loss: 1.0897 - val_accuracy: 0.6062\n",
      "Epoch 173/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.9910 - accuracy: 0.6468 - val_loss: 1.0711 - val_accuracy: 0.6086\n",
      "Epoch 174/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.9953 - accuracy: 0.6358 - val_loss: 1.0615 - val_accuracy: 0.6247\n",
      "Epoch 175/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.9635 - accuracy: 0.6657 - val_loss: 1.0725 - val_accuracy: 0.5926\n",
      "Epoch 176/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9834 - accuracy: 0.6510 - val_loss: 1.0678 - val_accuracy: 0.5926\n",
      "Epoch 177/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.9736 - accuracy: 0.6431 - val_loss: 1.0536 - val_accuracy: 0.6173\n",
      "Epoch 178/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.9618 - accuracy: 0.6565 - val_loss: 1.0799 - val_accuracy: 0.5963\n",
      "Epoch 179/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9725 - accuracy: 0.6443 - val_loss: 1.0540 - val_accuracy: 0.6123\n",
      "Epoch 180/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9757 - accuracy: 0.6382 - val_loss: 1.0636 - val_accuracy: 0.6136\n",
      "Epoch 181/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9800 - accuracy: 0.6303 - val_loss: 1.0652 - val_accuracy: 0.6099\n",
      "Epoch 182/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.9605 - accuracy: 0.6523 - val_loss: 1.0670 - val_accuracy: 0.6025\n",
      "Epoch 183/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.9554 - accuracy: 0.6614 - val_loss: 1.0715 - val_accuracy: 0.6099\n",
      "Epoch 184/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.9565 - accuracy: 0.6590 - val_loss: 1.0537 - val_accuracy: 0.6296\n",
      "Epoch 185/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.9604 - accuracy: 0.6474 - val_loss: 1.0715 - val_accuracy: 0.5840\n",
      "Epoch 186/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.9424 - accuracy: 0.6657 - val_loss: 1.0718 - val_accuracy: 0.5889\n",
      "Epoch 187/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.9518 - accuracy: 0.6516 - val_loss: 1.0515 - val_accuracy: 0.6173\n",
      "Epoch 188/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.9575 - accuracy: 0.6516 - val_loss: 1.0505 - val_accuracy: 0.6210\n",
      "Epoch 189/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.9429 - accuracy: 0.6559 - val_loss: 1.0797 - val_accuracy: 0.5901\n",
      "Epoch 190/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9557 - accuracy: 0.6583 - val_loss: 1.0677 - val_accuracy: 0.6049\n",
      "Epoch 191/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9373 - accuracy: 0.6687 - val_loss: 1.0375 - val_accuracy: 0.6296\n",
      "Epoch 192/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.9499 - accuracy: 0.6413 - val_loss: 1.0341 - val_accuracy: 0.6235\n",
      "Epoch 193/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.9490 - accuracy: 0.6395 - val_loss: 1.0299 - val_accuracy: 0.6272\n",
      "Epoch 194/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.9596 - accuracy: 0.6547 - val_loss: 1.0654 - val_accuracy: 0.6062\n",
      "Epoch 195/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.9408 - accuracy: 0.6498 - val_loss: 1.0439 - val_accuracy: 0.6111\n",
      "Epoch 196/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.9330 - accuracy: 0.6663 - val_loss: 1.0326 - val_accuracy: 0.6333\n",
      "Epoch 197/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9311 - accuracy: 0.6687 - val_loss: 1.0404 - val_accuracy: 0.6148\n",
      "Epoch 198/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.9315 - accuracy: 0.6681 - val_loss: 1.0528 - val_accuracy: 0.6210\n",
      "Epoch 199/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9279 - accuracy: 0.6583 - val_loss: 1.0449 - val_accuracy: 0.6074\n",
      "Epoch 200/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9342 - accuracy: 0.6559 - val_loss: 1.0382 - val_accuracy: 0.6136\n",
      "Epoch 201/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.9120 - accuracy: 0.6681 - val_loss: 1.0474 - val_accuracy: 0.6074\n",
      "Epoch 202/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.9186 - accuracy: 0.6657 - val_loss: 1.0443 - val_accuracy: 0.6111\n",
      "Epoch 203/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.9189 - accuracy: 0.6657 - val_loss: 1.0385 - val_accuracy: 0.6173\n",
      "Epoch 204/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.9013 - accuracy: 0.6876 - val_loss: 1.0314 - val_accuracy: 0.6321\n",
      "Epoch 205/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9189 - accuracy: 0.6724 - val_loss: 1.0187 - val_accuracy: 0.6333\n",
      "Epoch 206/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9159 - accuracy: 0.6675 - val_loss: 1.0189 - val_accuracy: 0.6247\n",
      "Epoch 207/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.9009 - accuracy: 0.6717 - val_loss: 1.0390 - val_accuracy: 0.5963\n",
      "Epoch 208/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9179 - accuracy: 0.6736 - val_loss: 1.0190 - val_accuracy: 0.6284\n",
      "Epoch 209/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.8977 - accuracy: 0.6742 - val_loss: 1.0404 - val_accuracy: 0.6136\n",
      "Epoch 210/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.9047 - accuracy: 0.6644 - val_loss: 1.0252 - val_accuracy: 0.6333\n",
      "Epoch 211/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9058 - accuracy: 0.6821 - val_loss: 1.0163 - val_accuracy: 0.6321\n",
      "Epoch 212/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8986 - accuracy: 0.6809 - val_loss: 1.0201 - val_accuracy: 0.6259\n",
      "Epoch 213/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.9000 - accuracy: 0.6882 - val_loss: 1.0183 - val_accuracy: 0.6198\n",
      "Epoch 214/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.9049 - accuracy: 0.6821 - val_loss: 1.0104 - val_accuracy: 0.6296\n",
      "Epoch 215/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8966 - accuracy: 0.6827 - val_loss: 1.0315 - val_accuracy: 0.6160\n",
      "Epoch 216/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8871 - accuracy: 0.6894 - val_loss: 1.0142 - val_accuracy: 0.6235\n",
      "Epoch 217/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8994 - accuracy: 0.6663 - val_loss: 1.0157 - val_accuracy: 0.6123\n",
      "Epoch 218/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.8936 - accuracy: 0.6724 - val_loss: 1.0116 - val_accuracy: 0.6160\n",
      "Epoch 219/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8772 - accuracy: 0.6870 - val_loss: 1.0104 - val_accuracy: 0.6383\n",
      "Epoch 220/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8864 - accuracy: 0.6845 - val_loss: 1.0228 - val_accuracy: 0.6321\n",
      "Epoch 221/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.8819 - accuracy: 0.6809 - val_loss: 1.0012 - val_accuracy: 0.6321\n",
      "Epoch 222/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8720 - accuracy: 0.6766 - val_loss: 1.0157 - val_accuracy: 0.6321\n",
      "Epoch 223/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.8724 - accuracy: 0.6888 - val_loss: 1.0413 - val_accuracy: 0.6210\n",
      "Epoch 224/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.8663 - accuracy: 0.6833 - val_loss: 1.0501 - val_accuracy: 0.6111\n",
      "Epoch 225/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.8846 - accuracy: 0.6809 - val_loss: 0.9987 - val_accuracy: 0.6358\n",
      "Epoch 226/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.8762 - accuracy: 0.6772 - val_loss: 1.0003 - val_accuracy: 0.6346\n",
      "Epoch 227/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8741 - accuracy: 0.6809 - val_loss: 1.0168 - val_accuracy: 0.6210\n",
      "Epoch 228/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8495 - accuracy: 0.6815 - val_loss: 1.0026 - val_accuracy: 0.6136\n",
      "Epoch 229/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8760 - accuracy: 0.6888 - val_loss: 1.0036 - val_accuracy: 0.6222\n",
      "Epoch 230/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8877 - accuracy: 0.6845 - val_loss: 1.0081 - val_accuracy: 0.6457\n",
      "Epoch 231/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.8497 - accuracy: 0.7119 - val_loss: 1.0151 - val_accuracy: 0.6259\n",
      "Epoch 232/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.8603 - accuracy: 0.6784 - val_loss: 1.0196 - val_accuracy: 0.6321\n",
      "Epoch 233/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.8600 - accuracy: 0.6784 - val_loss: 1.0071 - val_accuracy: 0.6247\n",
      "Epoch 234/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.8508 - accuracy: 0.6876 - val_loss: 1.0084 - val_accuracy: 0.6284\n",
      "Epoch 235/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8510 - accuracy: 0.6973 - val_loss: 1.0119 - val_accuracy: 0.6111\n",
      "Epoch 236/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8620 - accuracy: 0.6900 - val_loss: 0.9935 - val_accuracy: 0.6383\n",
      "Epoch 237/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.8546 - accuracy: 0.6998 - val_loss: 1.0061 - val_accuracy: 0.6321\n",
      "Epoch 238/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8484 - accuracy: 0.6967 - val_loss: 0.9998 - val_accuracy: 0.6346\n",
      "Epoch 239/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.8477 - accuracy: 0.6900 - val_loss: 0.9927 - val_accuracy: 0.6321\n",
      "Epoch 240/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.8376 - accuracy: 0.6991 - val_loss: 1.0248 - val_accuracy: 0.6272\n",
      "Epoch 241/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8149 - accuracy: 0.7107 - val_loss: 1.0123 - val_accuracy: 0.6123\n",
      "Epoch 242/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8397 - accuracy: 0.6931 - val_loss: 0.9983 - val_accuracy: 0.6272\n",
      "Epoch 243/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.8484 - accuracy: 0.6973 - val_loss: 0.9941 - val_accuracy: 0.6247\n",
      "Epoch 244/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.8401 - accuracy: 0.6979 - val_loss: 1.0066 - val_accuracy: 0.6173\n",
      "Epoch 245/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.8436 - accuracy: 0.6839 - val_loss: 0.9969 - val_accuracy: 0.6296\n",
      "Epoch 246/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8284 - accuracy: 0.7119 - val_loss: 0.9847 - val_accuracy: 0.6333\n",
      "Epoch 247/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8270 - accuracy: 0.7058 - val_loss: 0.9942 - val_accuracy: 0.6383\n",
      "Epoch 248/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8219 - accuracy: 0.6906 - val_loss: 0.9971 - val_accuracy: 0.6333\n",
      "Epoch 249/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8154 - accuracy: 0.7150 - val_loss: 0.9916 - val_accuracy: 0.6333\n",
      "Epoch 250/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8319 - accuracy: 0.7022 - val_loss: 0.9999 - val_accuracy: 0.6235\n",
      "Epoch 251/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8075 - accuracy: 0.7229 - val_loss: 0.9752 - val_accuracy: 0.6531\n",
      "Epoch 252/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8355 - accuracy: 0.6894 - val_loss: 1.0106 - val_accuracy: 0.6173\n",
      "Epoch 253/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.8213 - accuracy: 0.7028 - val_loss: 1.0087 - val_accuracy: 0.6272\n",
      "Epoch 254/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.8299 - accuracy: 0.6998 - val_loss: 0.9963 - val_accuracy: 0.6370\n",
      "Epoch 255/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8097 - accuracy: 0.7107 - val_loss: 0.9886 - val_accuracy: 0.6420\n",
      "Epoch 256/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8106 - accuracy: 0.7016 - val_loss: 1.0051 - val_accuracy: 0.6086\n",
      "Epoch 257/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8153 - accuracy: 0.7107 - val_loss: 0.9926 - val_accuracy: 0.6346\n",
      "Epoch 258/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8124 - accuracy: 0.7071 - val_loss: 0.9934 - val_accuracy: 0.6284\n",
      "Epoch 259/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8140 - accuracy: 0.7028 - val_loss: 0.9860 - val_accuracy: 0.6407\n",
      "Epoch 260/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8065 - accuracy: 0.7162 - val_loss: 0.9863 - val_accuracy: 0.6235\n",
      "Epoch 261/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.8103 - accuracy: 0.7113 - val_loss: 1.0207 - val_accuracy: 0.6173\n",
      "Epoch 262/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.8090 - accuracy: 0.6991 - val_loss: 0.9999 - val_accuracy: 0.6272\n",
      "Epoch 263/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.7976 - accuracy: 0.7144 - val_loss: 0.9897 - val_accuracy: 0.6333\n",
      "Epoch 264/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8078 - accuracy: 0.7052 - val_loss: 0.9987 - val_accuracy: 0.6210\n",
      "Epoch 265/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8057 - accuracy: 0.7101 - val_loss: 0.9860 - val_accuracy: 0.6333\n",
      "Epoch 266/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7824 - accuracy: 0.7046 - val_loss: 1.0496 - val_accuracy: 0.6062\n",
      "Epoch 267/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.7911 - accuracy: 0.7058 - val_loss: 0.9850 - val_accuracy: 0.6321\n",
      "Epoch 268/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.8031 - accuracy: 0.6955 - val_loss: 1.0017 - val_accuracy: 0.6222\n",
      "Epoch 269/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7948 - accuracy: 0.7052 - val_loss: 0.9745 - val_accuracy: 0.6321\n",
      "Epoch 270/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7974 - accuracy: 0.7034 - val_loss: 0.9834 - val_accuracy: 0.6309\n",
      "Epoch 271/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.7977 - accuracy: 0.7162 - val_loss: 0.9894 - val_accuracy: 0.6395\n",
      "Epoch 272/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.8014 - accuracy: 0.7095 - val_loss: 0.9647 - val_accuracy: 0.6358\n",
      "Epoch 273/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7918 - accuracy: 0.7016 - val_loss: 0.9619 - val_accuracy: 0.6494\n",
      "Epoch 274/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8073 - accuracy: 0.7065 - val_loss: 0.9662 - val_accuracy: 0.6444\n",
      "Epoch 275/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.7676 - accuracy: 0.7308 - val_loss: 0.9955 - val_accuracy: 0.6321\n",
      "Epoch 276/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.7829 - accuracy: 0.7138 - val_loss: 0.9777 - val_accuracy: 0.6358\n",
      "Epoch 277/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7747 - accuracy: 0.7229 - val_loss: 0.9690 - val_accuracy: 0.6457\n",
      "Epoch 278/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7857 - accuracy: 0.7089 - val_loss: 0.9758 - val_accuracy: 0.6407\n",
      "Epoch 279/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7634 - accuracy: 0.7174 - val_loss: 0.9503 - val_accuracy: 0.6506\n",
      "Epoch 280/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.7693 - accuracy: 0.7174 - val_loss: 0.9638 - val_accuracy: 0.6469\n",
      "Epoch 281/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.7845 - accuracy: 0.7253 - val_loss: 0.9745 - val_accuracy: 0.6284\n",
      "Epoch 282/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.7669 - accuracy: 0.7168 - val_loss: 0.9660 - val_accuracy: 0.6420\n",
      "Epoch 283/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7614 - accuracy: 0.7229 - val_loss: 1.0088 - val_accuracy: 0.6210\n",
      "Epoch 284/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7736 - accuracy: 0.7132 - val_loss: 0.9795 - val_accuracy: 0.6321\n",
      "Epoch 285/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7689 - accuracy: 0.7205 - val_loss: 0.9512 - val_accuracy: 0.6457\n",
      "Epoch 286/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7665 - accuracy: 0.7199 - val_loss: 0.9523 - val_accuracy: 0.6370\n",
      "Epoch 287/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7590 - accuracy: 0.7290 - val_loss: 0.9633 - val_accuracy: 0.6407\n",
      "Epoch 288/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.7486 - accuracy: 0.7369 - val_loss: 0.9797 - val_accuracy: 0.6296\n",
      "Epoch 289/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.7703 - accuracy: 0.7235 - val_loss: 0.9723 - val_accuracy: 0.6333\n",
      "Epoch 290/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.7701 - accuracy: 0.7266 - val_loss: 0.9723 - val_accuracy: 0.6235\n",
      "Epoch 291/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7548 - accuracy: 0.7229 - val_loss: 0.9739 - val_accuracy: 0.6383\n",
      "Epoch 292/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.7649 - accuracy: 0.7138 - val_loss: 0.9429 - val_accuracy: 0.6543\n",
      "Epoch 293/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.7456 - accuracy: 0.7357 - val_loss: 0.9611 - val_accuracy: 0.6395\n",
      "Epoch 294/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7493 - accuracy: 0.7333 - val_loss: 0.9737 - val_accuracy: 0.6198\n",
      "Epoch 295/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7348 - accuracy: 0.7339 - val_loss: 0.9540 - val_accuracy: 0.6346\n",
      "Epoch 296/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7382 - accuracy: 0.7418 - val_loss: 0.9832 - val_accuracy: 0.6284\n",
      "Epoch 297/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7526 - accuracy: 0.7326 - val_loss: 0.9782 - val_accuracy: 0.6383\n",
      "Epoch 298/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.7355 - accuracy: 0.7314 - val_loss: 0.9445 - val_accuracy: 0.6395\n",
      "Epoch 299/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.7584 - accuracy: 0.7278 - val_loss: 0.9474 - val_accuracy: 0.6469\n",
      "Epoch 300/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.7482 - accuracy: 0.7211 - val_loss: 0.9476 - val_accuracy: 0.6531\n",
      "Epoch 301/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7456 - accuracy: 0.7308 - val_loss: 0.9548 - val_accuracy: 0.6519\n",
      "Epoch 302/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7390 - accuracy: 0.7290 - val_loss: 0.9903 - val_accuracy: 0.6383\n",
      "Epoch 303/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.7327 - accuracy: 0.7400 - val_loss: 0.9560 - val_accuracy: 0.6383\n",
      "Epoch 304/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7377 - accuracy: 0.7229 - val_loss: 0.9337 - val_accuracy: 0.6519\n",
      "Epoch 305/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7349 - accuracy: 0.7387 - val_loss: 0.9610 - val_accuracy: 0.6333\n",
      "Epoch 306/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7200 - accuracy: 0.7351 - val_loss: 0.9657 - val_accuracy: 0.6506\n",
      "Epoch 307/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.7374 - accuracy: 0.7266 - val_loss: 0.9379 - val_accuracy: 0.6605\n",
      "Epoch 308/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.7406 - accuracy: 0.7333 - val_loss: 0.9381 - val_accuracy: 0.6543\n",
      "Epoch 309/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7277 - accuracy: 0.7351 - val_loss: 0.9443 - val_accuracy: 0.6481\n",
      "Epoch 310/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7199 - accuracy: 0.7393 - val_loss: 0.9347 - val_accuracy: 0.6444\n",
      "Epoch 311/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7341 - accuracy: 0.7314 - val_loss: 0.9393 - val_accuracy: 0.6457\n",
      "Epoch 312/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7181 - accuracy: 0.7387 - val_loss: 0.9544 - val_accuracy: 0.6309\n",
      "Epoch 313/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.7262 - accuracy: 0.7308 - val_loss: 0.9443 - val_accuracy: 0.6358\n",
      "Epoch 314/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7041 - accuracy: 0.7485 - val_loss: 0.9324 - val_accuracy: 0.6630\n",
      "Epoch 315/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7179 - accuracy: 0.7333 - val_loss: 0.9800 - val_accuracy: 0.6136\n",
      "Epoch 316/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.7391 - accuracy: 0.7284 - val_loss: 0.9562 - val_accuracy: 0.6494\n",
      "Epoch 317/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7203 - accuracy: 0.7393 - val_loss: 0.9563 - val_accuracy: 0.6358\n",
      "Epoch 318/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.7197 - accuracy: 0.7393 - val_loss: 0.9321 - val_accuracy: 0.6469\n",
      "Epoch 319/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6981 - accuracy: 0.7564 - val_loss: 0.9482 - val_accuracy: 0.6383\n",
      "Epoch 320/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7056 - accuracy: 0.7436 - val_loss: 0.9444 - val_accuracy: 0.6481\n",
      "Epoch 321/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7198 - accuracy: 0.7351 - val_loss: 0.9304 - val_accuracy: 0.6519\n",
      "Epoch 322/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7182 - accuracy: 0.7491 - val_loss: 0.9471 - val_accuracy: 0.6370\n",
      "Epoch 323/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.7058 - accuracy: 0.7448 - val_loss: 0.9543 - val_accuracy: 0.6481\n",
      "Epoch 324/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7147 - accuracy: 0.7442 - val_loss: 0.9433 - val_accuracy: 0.6395\n",
      "Epoch 325/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6985 - accuracy: 0.7479 - val_loss: 0.9686 - val_accuracy: 0.6358\n",
      "Epoch 326/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6966 - accuracy: 0.7588 - val_loss: 0.9321 - val_accuracy: 0.6580\n",
      "Epoch 327/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6992 - accuracy: 0.7460 - val_loss: 0.9454 - val_accuracy: 0.6494\n",
      "Epoch 328/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7155 - accuracy: 0.7393 - val_loss: 0.9801 - val_accuracy: 0.6346\n",
      "Epoch 329/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7063 - accuracy: 0.7479 - val_loss: 0.9478 - val_accuracy: 0.6407\n",
      "Epoch 330/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6952 - accuracy: 0.7503 - val_loss: 0.9461 - val_accuracy: 0.6617\n",
      "Epoch 331/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7053 - accuracy: 0.7564 - val_loss: 0.9311 - val_accuracy: 0.6432\n",
      "Epoch 332/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6994 - accuracy: 0.7418 - val_loss: 0.9359 - val_accuracy: 0.6444\n",
      "Epoch 333/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6889 - accuracy: 0.7497 - val_loss: 0.9319 - val_accuracy: 0.6593\n",
      "Epoch 334/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.7050 - accuracy: 0.7473 - val_loss: 0.9520 - val_accuracy: 0.6481\n",
      "Epoch 335/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.6930 - accuracy: 0.7503 - val_loss: 0.9418 - val_accuracy: 0.6420\n",
      "Epoch 336/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.6924 - accuracy: 0.7552 - val_loss: 0.9278 - val_accuracy: 0.6407\n",
      "Epoch 337/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6714 - accuracy: 0.7674 - val_loss: 0.9487 - val_accuracy: 0.6210\n",
      "Epoch 338/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6777 - accuracy: 0.7607 - val_loss: 0.9637 - val_accuracy: 0.6370\n",
      "Epoch 339/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6621 - accuracy: 0.7674 - val_loss: 0.9497 - val_accuracy: 0.6259\n",
      "Epoch 340/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6850 - accuracy: 0.7625 - val_loss: 0.9276 - val_accuracy: 0.6580\n",
      "Epoch 341/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6793 - accuracy: 0.7625 - val_loss: 0.9534 - val_accuracy: 0.6346\n",
      "Epoch 342/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.7108 - accuracy: 0.7533 - val_loss: 0.9469 - val_accuracy: 0.6420\n",
      "Epoch 343/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6881 - accuracy: 0.7503 - val_loss: 0.9322 - val_accuracy: 0.6494\n",
      "Epoch 344/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6733 - accuracy: 0.7588 - val_loss: 0.9455 - val_accuracy: 0.6370\n",
      "Epoch 345/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6914 - accuracy: 0.7582 - val_loss: 0.9296 - val_accuracy: 0.6543\n",
      "Epoch 346/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6672 - accuracy: 0.7661 - val_loss: 0.9237 - val_accuracy: 0.6605\n",
      "Epoch 347/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6790 - accuracy: 0.7546 - val_loss: 0.9524 - val_accuracy: 0.6568\n",
      "Epoch 348/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6742 - accuracy: 0.7515 - val_loss: 0.9569 - val_accuracy: 0.6531\n",
      "Epoch 349/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.6839 - accuracy: 0.7485 - val_loss: 0.9474 - val_accuracy: 0.6259\n",
      "Epoch 350/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6704 - accuracy: 0.7722 - val_loss: 0.9194 - val_accuracy: 0.6642\n",
      "Epoch 351/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6661 - accuracy: 0.7637 - val_loss: 0.9268 - val_accuracy: 0.6444\n",
      "Epoch 352/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6679 - accuracy: 0.7594 - val_loss: 0.9355 - val_accuracy: 0.6333\n",
      "Epoch 353/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6598 - accuracy: 0.7631 - val_loss: 0.9391 - val_accuracy: 0.6457\n",
      "Epoch 354/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.6609 - accuracy: 0.7607 - val_loss: 0.9303 - val_accuracy: 0.6593\n",
      "Epoch 355/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.6603 - accuracy: 0.7576 - val_loss: 0.9370 - val_accuracy: 0.6469\n",
      "Epoch 356/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6680 - accuracy: 0.7600 - val_loss: 0.9351 - val_accuracy: 0.6506\n",
      "Epoch 357/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6555 - accuracy: 0.7637 - val_loss: 0.9399 - val_accuracy: 0.6383\n",
      "Epoch 358/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6636 - accuracy: 0.7667 - val_loss: 0.9646 - val_accuracy: 0.6383\n",
      "Epoch 359/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6424 - accuracy: 0.7741 - val_loss: 0.9321 - val_accuracy: 0.6617\n",
      "Epoch 360/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6535 - accuracy: 0.7680 - val_loss: 0.9228 - val_accuracy: 0.6605\n",
      "Epoch 361/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6491 - accuracy: 0.7613 - val_loss: 0.9224 - val_accuracy: 0.6531\n",
      "Epoch 362/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6495 - accuracy: 0.7680 - val_loss: 0.9314 - val_accuracy: 0.6556\n",
      "Epoch 363/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6699 - accuracy: 0.7558 - val_loss: 0.9329 - val_accuracy: 0.6444\n",
      "Epoch 364/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6581 - accuracy: 0.7509 - val_loss: 0.9185 - val_accuracy: 0.6506\n",
      "Epoch 365/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6436 - accuracy: 0.7826 - val_loss: 0.9501 - val_accuracy: 0.6407\n",
      "Epoch 366/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6474 - accuracy: 0.7692 - val_loss: 0.9283 - val_accuracy: 0.6568\n",
      "Epoch 367/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6385 - accuracy: 0.7655 - val_loss: 0.9399 - val_accuracy: 0.6605\n",
      "Epoch 368/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6503 - accuracy: 0.7625 - val_loss: 0.9004 - val_accuracy: 0.6728\n",
      "Epoch 369/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6441 - accuracy: 0.7728 - val_loss: 0.9345 - val_accuracy: 0.6506\n",
      "Epoch 370/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6344 - accuracy: 0.7753 - val_loss: 0.9191 - val_accuracy: 0.6506\n",
      "Epoch 371/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6353 - accuracy: 0.7722 - val_loss: 0.9145 - val_accuracy: 0.6420\n",
      "Epoch 372/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6293 - accuracy: 0.7868 - val_loss: 0.9219 - val_accuracy: 0.6667\n",
      "Epoch 373/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6666 - accuracy: 0.7613 - val_loss: 0.9422 - val_accuracy: 0.6284\n",
      "Epoch 374/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.6376 - accuracy: 0.7728 - val_loss: 0.9089 - val_accuracy: 0.6667\n",
      "Epoch 375/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6292 - accuracy: 0.7759 - val_loss: 0.9275 - val_accuracy: 0.6654\n",
      "Epoch 376/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.6366 - accuracy: 0.7655 - val_loss: 0.9189 - val_accuracy: 0.6642\n",
      "Epoch 377/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.6252 - accuracy: 0.7893 - val_loss: 0.9193 - val_accuracy: 0.6580\n",
      "Epoch 378/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.6387 - accuracy: 0.7686 - val_loss: 0.9288 - val_accuracy: 0.6432\n",
      "Epoch 379/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6374 - accuracy: 0.7753 - val_loss: 0.9210 - val_accuracy: 0.6605\n",
      "Epoch 380/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6228 - accuracy: 0.7765 - val_loss: 0.8884 - val_accuracy: 0.6704\n",
      "Epoch 381/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6391 - accuracy: 0.7698 - val_loss: 0.9418 - val_accuracy: 0.6358\n",
      "Epoch 382/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6183 - accuracy: 0.7826 - val_loss: 0.9316 - val_accuracy: 0.6481\n",
      "Epoch 383/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6286 - accuracy: 0.7698 - val_loss: 0.9166 - val_accuracy: 0.6617\n",
      "Epoch 384/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6026 - accuracy: 0.7826 - val_loss: 0.9269 - val_accuracy: 0.6494\n",
      "Epoch 385/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6278 - accuracy: 0.7765 - val_loss: 0.9273 - val_accuracy: 0.6506\n",
      "Epoch 386/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6313 - accuracy: 0.7722 - val_loss: 0.8969 - val_accuracy: 0.6642\n",
      "Epoch 387/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6201 - accuracy: 0.7826 - val_loss: 0.9368 - val_accuracy: 0.6494\n",
      "Epoch 388/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6251 - accuracy: 0.7771 - val_loss: 0.9565 - val_accuracy: 0.6407\n",
      "Epoch 389/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6109 - accuracy: 0.7881 - val_loss: 0.9149 - val_accuracy: 0.6469\n",
      "Epoch 390/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6195 - accuracy: 0.7850 - val_loss: 0.9258 - val_accuracy: 0.6444\n",
      "Epoch 391/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.6143 - accuracy: 0.7795 - val_loss: 0.8966 - val_accuracy: 0.6741\n",
      "Epoch 392/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6003 - accuracy: 0.7923 - val_loss: 0.9192 - val_accuracy: 0.6568\n",
      "Epoch 393/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6020 - accuracy: 0.7881 - val_loss: 0.9022 - val_accuracy: 0.6667\n",
      "Epoch 394/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6282 - accuracy: 0.7765 - val_loss: 0.9193 - val_accuracy: 0.6531\n",
      "Epoch 395/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6279 - accuracy: 0.7771 - val_loss: 0.9045 - val_accuracy: 0.6556\n",
      "Epoch 396/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6345 - accuracy: 0.7814 - val_loss: 0.9102 - val_accuracy: 0.6457\n",
      "Epoch 397/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6090 - accuracy: 0.7923 - val_loss: 0.9059 - val_accuracy: 0.6556\n",
      "Epoch 398/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6047 - accuracy: 0.7801 - val_loss: 0.9103 - val_accuracy: 0.6679\n",
      "Epoch 399/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6144 - accuracy: 0.7728 - val_loss: 0.9154 - val_accuracy: 0.6519\n",
      "Epoch 400/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6036 - accuracy: 0.7826 - val_loss: 0.9140 - val_accuracy: 0.6580\n",
      "Epoch 401/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5936 - accuracy: 0.7844 - val_loss: 0.8980 - val_accuracy: 0.6642\n",
      "Epoch 402/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6026 - accuracy: 0.7753 - val_loss: 0.9532 - val_accuracy: 0.6395\n",
      "Epoch 403/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6084 - accuracy: 0.7856 - val_loss: 0.9183 - val_accuracy: 0.6580\n",
      "Epoch 404/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6062 - accuracy: 0.7893 - val_loss: 0.9303 - val_accuracy: 0.6630\n",
      "Epoch 405/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6064 - accuracy: 0.7844 - val_loss: 0.9008 - val_accuracy: 0.6679\n",
      "Epoch 406/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6029 - accuracy: 0.7777 - val_loss: 0.8931 - val_accuracy: 0.6753\n",
      "Epoch 407/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5787 - accuracy: 0.7911 - val_loss: 0.9112 - val_accuracy: 0.6531\n",
      "Epoch 408/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6005 - accuracy: 0.7868 - val_loss: 0.8926 - val_accuracy: 0.6580\n",
      "Epoch 409/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5947 - accuracy: 0.7783 - val_loss: 0.9095 - val_accuracy: 0.6593\n",
      "Epoch 410/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5906 - accuracy: 0.7899 - val_loss: 0.9161 - val_accuracy: 0.6580\n",
      "Epoch 411/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5982 - accuracy: 0.7850 - val_loss: 0.8996 - val_accuracy: 0.6642\n",
      "Epoch 412/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5982 - accuracy: 0.7814 - val_loss: 0.8788 - val_accuracy: 0.6691\n",
      "Epoch 413/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6047 - accuracy: 0.7862 - val_loss: 0.9018 - val_accuracy: 0.6593\n",
      "Epoch 414/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5912 - accuracy: 0.7935 - val_loss: 0.8897 - val_accuracy: 0.6716\n",
      "Epoch 415/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6020 - accuracy: 0.7808 - val_loss: 0.8945 - val_accuracy: 0.6630\n",
      "Epoch 416/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5769 - accuracy: 0.7935 - val_loss: 0.8996 - val_accuracy: 0.6543\n",
      "Epoch 417/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5834 - accuracy: 0.7868 - val_loss: 0.9167 - val_accuracy: 0.6481\n",
      "Epoch 418/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5842 - accuracy: 0.8027 - val_loss: 0.8969 - val_accuracy: 0.6630\n",
      "Epoch 419/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5965 - accuracy: 0.7795 - val_loss: 0.9192 - val_accuracy: 0.6543\n",
      "Epoch 420/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5809 - accuracy: 0.7814 - val_loss: 0.8936 - val_accuracy: 0.6728\n",
      "Epoch 421/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5767 - accuracy: 0.7808 - val_loss: 0.8899 - val_accuracy: 0.6617\n",
      "Epoch 422/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5819 - accuracy: 0.7868 - val_loss: 0.8946 - val_accuracy: 0.6642\n",
      "Epoch 423/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5753 - accuracy: 0.7966 - val_loss: 0.9050 - val_accuracy: 0.6667\n",
      "Epoch 424/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5725 - accuracy: 0.7935 - val_loss: 0.8945 - val_accuracy: 0.6568\n",
      "Epoch 425/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5801 - accuracy: 0.7905 - val_loss: 0.9041 - val_accuracy: 0.6543\n",
      "Epoch 426/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.5674 - accuracy: 0.8130 - val_loss: 0.8953 - val_accuracy: 0.6691\n",
      "Epoch 427/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5743 - accuracy: 0.7893 - val_loss: 0.9082 - val_accuracy: 0.6642\n",
      "Epoch 428/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5795 - accuracy: 0.7923 - val_loss: 0.9318 - val_accuracy: 0.6556\n",
      "Epoch 429/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5761 - accuracy: 0.7935 - val_loss: 0.9121 - val_accuracy: 0.6642\n",
      "Epoch 430/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5732 - accuracy: 0.7990 - val_loss: 0.8902 - val_accuracy: 0.6679\n",
      "Epoch 431/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5596 - accuracy: 0.7978 - val_loss: 0.8895 - val_accuracy: 0.6716\n",
      "Epoch 432/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5716 - accuracy: 0.8002 - val_loss: 0.9052 - val_accuracy: 0.6543\n",
      "Epoch 433/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5664 - accuracy: 0.7875 - val_loss: 0.9210 - val_accuracy: 0.6481\n",
      "Epoch 434/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5796 - accuracy: 0.7917 - val_loss: 0.8933 - val_accuracy: 0.6617\n",
      "Epoch 435/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5759 - accuracy: 0.7942 - val_loss: 0.8923 - val_accuracy: 0.6753\n",
      "Epoch 436/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5679 - accuracy: 0.8021 - val_loss: 0.9115 - val_accuracy: 0.6593\n",
      "Epoch 437/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5813 - accuracy: 0.7868 - val_loss: 0.8954 - val_accuracy: 0.6593\n",
      "Epoch 438/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5769 - accuracy: 0.7966 - val_loss: 0.9284 - val_accuracy: 0.6346\n",
      "Epoch 439/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5577 - accuracy: 0.7935 - val_loss: 0.8986 - val_accuracy: 0.6556\n",
      "Epoch 440/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5490 - accuracy: 0.8076 - val_loss: 0.8991 - val_accuracy: 0.6642\n",
      "Epoch 441/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5702 - accuracy: 0.7954 - val_loss: 0.8821 - val_accuracy: 0.6679\n",
      "Epoch 442/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5552 - accuracy: 0.8094 - val_loss: 0.9148 - val_accuracy: 0.6580\n",
      "Epoch 443/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5545 - accuracy: 0.8051 - val_loss: 0.8880 - val_accuracy: 0.6704\n",
      "Epoch 444/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5703 - accuracy: 0.7954 - val_loss: 0.8761 - val_accuracy: 0.6704\n",
      "Epoch 445/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5529 - accuracy: 0.8039 - val_loss: 0.8940 - val_accuracy: 0.6815\n",
      "Epoch 446/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5479 - accuracy: 0.8094 - val_loss: 0.9108 - val_accuracy: 0.6531\n",
      "Epoch 447/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5570 - accuracy: 0.8009 - val_loss: 0.9192 - val_accuracy: 0.6519\n",
      "Epoch 448/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5508 - accuracy: 0.8106 - val_loss: 0.9011 - val_accuracy: 0.6593\n",
      "Epoch 449/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5368 - accuracy: 0.8039 - val_loss: 0.8790 - val_accuracy: 0.6790\n",
      "Epoch 450/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5555 - accuracy: 0.8136 - val_loss: 0.9069 - val_accuracy: 0.6568\n",
      "Epoch 451/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5581 - accuracy: 0.7984 - val_loss: 0.8737 - val_accuracy: 0.6704\n",
      "Epoch 452/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5491 - accuracy: 0.8094 - val_loss: 0.8748 - val_accuracy: 0.6753\n",
      "Epoch 453/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5537 - accuracy: 0.8100 - val_loss: 0.8787 - val_accuracy: 0.6753\n",
      "Epoch 454/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5410 - accuracy: 0.8094 - val_loss: 0.8962 - val_accuracy: 0.6642\n",
      "Epoch 455/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5231 - accuracy: 0.8167 - val_loss: 0.8842 - val_accuracy: 0.6815\n",
      "Epoch 456/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5456 - accuracy: 0.8015 - val_loss: 0.8782 - val_accuracy: 0.6716\n",
      "Epoch 457/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5396 - accuracy: 0.8143 - val_loss: 0.8763 - val_accuracy: 0.6852\n",
      "Epoch 458/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5438 - accuracy: 0.8027 - val_loss: 0.9042 - val_accuracy: 0.6741\n",
      "Epoch 459/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5362 - accuracy: 0.8076 - val_loss: 0.9006 - val_accuracy: 0.6543\n",
      "Epoch 460/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5791 - accuracy: 0.7966 - val_loss: 0.9102 - val_accuracy: 0.6605\n",
      "Epoch 461/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5278 - accuracy: 0.8106 - val_loss: 0.9280 - val_accuracy: 0.6395\n",
      "Epoch 462/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5398 - accuracy: 0.8051 - val_loss: 0.8984 - val_accuracy: 0.6605\n",
      "Epoch 463/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5318 - accuracy: 0.8082 - val_loss: 0.8867 - val_accuracy: 0.6815\n",
      "Epoch 464/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5270 - accuracy: 0.8118 - val_loss: 0.9155 - val_accuracy: 0.6531\n",
      "Epoch 465/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5302 - accuracy: 0.8045 - val_loss: 0.8812 - val_accuracy: 0.6753\n",
      "Epoch 466/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5306 - accuracy: 0.8063 - val_loss: 0.8883 - val_accuracy: 0.6716\n",
      "Epoch 467/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.5381 - accuracy: 0.8124 - val_loss: 0.8996 - val_accuracy: 0.6642\n",
      "Epoch 468/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5351 - accuracy: 0.8118 - val_loss: 0.9034 - val_accuracy: 0.6617\n",
      "Epoch 469/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5189 - accuracy: 0.8228 - val_loss: 0.8932 - val_accuracy: 0.6617\n",
      "Epoch 470/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5223 - accuracy: 0.8216 - val_loss: 0.8808 - val_accuracy: 0.6741\n",
      "Epoch 471/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5466 - accuracy: 0.7978 - val_loss: 0.8855 - val_accuracy: 0.6753\n",
      "Epoch 472/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5345 - accuracy: 0.8106 - val_loss: 0.8888 - val_accuracy: 0.6815\n",
      "Epoch 473/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5367 - accuracy: 0.8063 - val_loss: 0.8819 - val_accuracy: 0.6679\n",
      "Epoch 474/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4972 - accuracy: 0.8313 - val_loss: 0.8799 - val_accuracy: 0.6815\n",
      "Epoch 475/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5239 - accuracy: 0.8136 - val_loss: 0.8870 - val_accuracy: 0.6728\n",
      "Epoch 476/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5060 - accuracy: 0.8301 - val_loss: 0.9111 - val_accuracy: 0.6630\n",
      "Epoch 477/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5342 - accuracy: 0.8069 - val_loss: 0.9181 - val_accuracy: 0.6654\n",
      "Epoch 478/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.5301 - accuracy: 0.8112 - val_loss: 0.8954 - val_accuracy: 0.6716\n",
      "Epoch 479/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5314 - accuracy: 0.8088 - val_loss: 0.8992 - val_accuracy: 0.6778\n",
      "Epoch 480/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5082 - accuracy: 0.8197 - val_loss: 0.8880 - val_accuracy: 0.6753\n",
      "Epoch 481/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4992 - accuracy: 0.8313 - val_loss: 0.8792 - val_accuracy: 0.6728\n",
      "Epoch 482/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5227 - accuracy: 0.8179 - val_loss: 0.8674 - val_accuracy: 0.6790\n",
      "Epoch 483/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5162 - accuracy: 0.8203 - val_loss: 0.9021 - val_accuracy: 0.6728\n",
      "Epoch 484/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5066 - accuracy: 0.8252 - val_loss: 0.9081 - val_accuracy: 0.6605\n",
      "Epoch 485/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5121 - accuracy: 0.8197 - val_loss: 0.8827 - val_accuracy: 0.6753\n",
      "Epoch 486/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5116 - accuracy: 0.8149 - val_loss: 0.8777 - val_accuracy: 0.6753\n",
      "Epoch 487/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.5154 - accuracy: 0.8094 - val_loss: 0.8986 - val_accuracy: 0.6679\n",
      "Epoch 488/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4948 - accuracy: 0.8283 - val_loss: 0.8843 - val_accuracy: 0.6642\n",
      "Epoch 489/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5198 - accuracy: 0.8203 - val_loss: 0.8823 - val_accuracy: 0.6802\n",
      "Epoch 490/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5036 - accuracy: 0.8283 - val_loss: 0.8970 - val_accuracy: 0.6679\n",
      "Epoch 491/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4957 - accuracy: 0.8228 - val_loss: 0.8848 - val_accuracy: 0.6802\n",
      "Epoch 492/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5016 - accuracy: 0.8136 - val_loss: 0.8905 - val_accuracy: 0.6840\n",
      "Epoch 493/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5041 - accuracy: 0.8264 - val_loss: 0.8907 - val_accuracy: 0.6728\n",
      "Epoch 494/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5029 - accuracy: 0.8106 - val_loss: 0.9116 - val_accuracy: 0.6543\n",
      "Epoch 495/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5037 - accuracy: 0.8185 - val_loss: 0.8825 - val_accuracy: 0.6704\n",
      "Epoch 496/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4970 - accuracy: 0.8173 - val_loss: 0.8964 - val_accuracy: 0.6654\n",
      "Epoch 497/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5140 - accuracy: 0.8173 - val_loss: 0.8848 - val_accuracy: 0.6778\n",
      "Epoch 498/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5031 - accuracy: 0.8313 - val_loss: 0.8884 - val_accuracy: 0.6716\n",
      "Epoch 499/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4956 - accuracy: 0.8179 - val_loss: 0.8900 - val_accuracy: 0.6543\n",
      "Epoch 500/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4994 - accuracy: 0.8179 - val_loss: 0.8894 - val_accuracy: 0.6765\n",
      "Epoch 501/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5047 - accuracy: 0.8197 - val_loss: 0.8840 - val_accuracy: 0.6741\n",
      "Epoch 502/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4889 - accuracy: 0.8289 - val_loss: 0.9300 - val_accuracy: 0.6568\n",
      "Epoch 503/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5044 - accuracy: 0.8210 - val_loss: 0.8825 - val_accuracy: 0.6741\n",
      "Epoch 504/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4859 - accuracy: 0.8301 - val_loss: 0.8960 - val_accuracy: 0.6704\n",
      "Epoch 505/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.5036 - accuracy: 0.8191 - val_loss: 0.8811 - val_accuracy: 0.6716\n",
      "Epoch 506/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4880 - accuracy: 0.8203 - val_loss: 0.8830 - val_accuracy: 0.6802\n",
      "Epoch 507/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4901 - accuracy: 0.8252 - val_loss: 0.8900 - val_accuracy: 0.6642\n",
      "Epoch 508/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4800 - accuracy: 0.8362 - val_loss: 0.8965 - val_accuracy: 0.6765\n",
      "Epoch 509/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.4714 - accuracy: 0.8295 - val_loss: 0.9056 - val_accuracy: 0.6679\n",
      "Epoch 510/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4749 - accuracy: 0.8301 - val_loss: 0.8664 - val_accuracy: 0.6728\n",
      "Epoch 511/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4582 - accuracy: 0.8484 - val_loss: 0.8903 - val_accuracy: 0.6765\n",
      "Epoch 512/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4934 - accuracy: 0.8270 - val_loss: 0.8883 - val_accuracy: 0.6716\n",
      "Epoch 513/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4633 - accuracy: 0.8289 - val_loss: 0.9058 - val_accuracy: 0.6667\n",
      "Epoch 514/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5025 - accuracy: 0.8234 - val_loss: 0.8925 - val_accuracy: 0.6642\n",
      "Epoch 515/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4622 - accuracy: 0.8325 - val_loss: 0.8823 - val_accuracy: 0.6617\n",
      "Epoch 516/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4852 - accuracy: 0.8276 - val_loss: 0.8870 - val_accuracy: 0.6667\n",
      "Epoch 517/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4701 - accuracy: 0.8356 - val_loss: 0.8870 - val_accuracy: 0.6741\n",
      "Epoch 518/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4795 - accuracy: 0.8368 - val_loss: 0.8934 - val_accuracy: 0.6667\n",
      "Epoch 519/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4813 - accuracy: 0.8368 - val_loss: 0.8777 - val_accuracy: 0.6778\n",
      "Epoch 520/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4620 - accuracy: 0.8429 - val_loss: 0.8871 - val_accuracy: 0.6568\n",
      "Epoch 521/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4753 - accuracy: 0.8307 - val_loss: 0.8666 - val_accuracy: 0.6765\n",
      "Epoch 522/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4734 - accuracy: 0.8350 - val_loss: 0.8792 - val_accuracy: 0.6790\n",
      "Epoch 523/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4704 - accuracy: 0.8325 - val_loss: 0.8867 - val_accuracy: 0.6778\n",
      "Epoch 524/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4562 - accuracy: 0.8417 - val_loss: 0.9017 - val_accuracy: 0.6790\n",
      "Epoch 525/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4650 - accuracy: 0.8325 - val_loss: 0.9033 - val_accuracy: 0.6630\n",
      "Epoch 526/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4645 - accuracy: 0.8417 - val_loss: 0.8805 - val_accuracy: 0.6815\n",
      "Epoch 527/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4697 - accuracy: 0.8283 - val_loss: 0.8957 - val_accuracy: 0.6765\n",
      "Epoch 528/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4693 - accuracy: 0.8331 - val_loss: 0.8884 - val_accuracy: 0.6802\n",
      "Epoch 529/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4611 - accuracy: 0.8435 - val_loss: 0.8781 - val_accuracy: 0.6852\n",
      "Epoch 530/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4727 - accuracy: 0.8398 - val_loss: 0.8750 - val_accuracy: 0.6728\n",
      "Epoch 531/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4772 - accuracy: 0.8343 - val_loss: 0.8842 - val_accuracy: 0.6704\n",
      "Epoch 532/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4539 - accuracy: 0.8477 - val_loss: 0.8838 - val_accuracy: 0.6679\n",
      "Epoch 533/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4664 - accuracy: 0.8374 - val_loss: 0.8641 - val_accuracy: 0.6753\n",
      "Epoch 534/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4508 - accuracy: 0.8417 - val_loss: 0.8816 - val_accuracy: 0.6753\n",
      "Epoch 535/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4479 - accuracy: 0.8398 - val_loss: 0.8836 - val_accuracy: 0.6741\n",
      "Epoch 536/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4628 - accuracy: 0.8350 - val_loss: 0.8830 - val_accuracy: 0.6753\n",
      "Epoch 537/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4608 - accuracy: 0.8343 - val_loss: 0.8856 - val_accuracy: 0.6728\n",
      "Epoch 538/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4505 - accuracy: 0.8465 - val_loss: 0.8964 - val_accuracy: 0.6617\n",
      "Epoch 539/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4548 - accuracy: 0.8374 - val_loss: 0.8921 - val_accuracy: 0.6741\n",
      "Epoch 540/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4734 - accuracy: 0.8331 - val_loss: 0.8895 - val_accuracy: 0.6630\n",
      "Epoch 541/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4823 - accuracy: 0.8264 - val_loss: 0.9039 - val_accuracy: 0.6704\n",
      "Epoch 542/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4800 - accuracy: 0.8350 - val_loss: 0.9012 - val_accuracy: 0.6642\n",
      "Epoch 543/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4717 - accuracy: 0.8429 - val_loss: 0.8636 - val_accuracy: 0.6802\n",
      "Epoch 544/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4413 - accuracy: 0.8435 - val_loss: 0.8847 - val_accuracy: 0.6654\n",
      "Epoch 545/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4712 - accuracy: 0.8380 - val_loss: 0.8806 - val_accuracy: 0.6852\n",
      "Epoch 546/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4413 - accuracy: 0.8490 - val_loss: 0.8698 - val_accuracy: 0.6765\n",
      "Epoch 547/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4494 - accuracy: 0.8526 - val_loss: 0.8784 - val_accuracy: 0.6728\n",
      "Epoch 548/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4614 - accuracy: 0.8362 - val_loss: 0.9048 - val_accuracy: 0.6617\n",
      "Epoch 549/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4477 - accuracy: 0.8502 - val_loss: 0.8782 - val_accuracy: 0.6741\n",
      "Epoch 550/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4482 - accuracy: 0.8368 - val_loss: 0.8501 - val_accuracy: 0.6877\n",
      "Epoch 551/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4415 - accuracy: 0.8508 - val_loss: 0.8720 - val_accuracy: 0.6704\n",
      "Epoch 552/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4582 - accuracy: 0.8417 - val_loss: 0.8868 - val_accuracy: 0.6790\n",
      "Epoch 553/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4499 - accuracy: 0.8386 - val_loss: 0.8603 - val_accuracy: 0.6802\n",
      "Epoch 554/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4539 - accuracy: 0.8441 - val_loss: 0.8719 - val_accuracy: 0.6864\n",
      "Epoch 555/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4409 - accuracy: 0.8471 - val_loss: 0.8966 - val_accuracy: 0.6704\n",
      "Epoch 556/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4587 - accuracy: 0.8307 - val_loss: 0.8830 - val_accuracy: 0.6827\n",
      "Epoch 557/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4305 - accuracy: 0.8575 - val_loss: 0.8794 - val_accuracy: 0.6753\n",
      "Epoch 558/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4322 - accuracy: 0.8526 - val_loss: 0.8809 - val_accuracy: 0.6741\n",
      "Epoch 559/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4443 - accuracy: 0.8417 - val_loss: 0.9090 - val_accuracy: 0.6580\n",
      "Epoch 560/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4320 - accuracy: 0.8502 - val_loss: 0.8659 - val_accuracy: 0.6790\n",
      "Epoch 561/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4406 - accuracy: 0.8544 - val_loss: 0.8766 - val_accuracy: 0.6753\n",
      "Epoch 562/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4458 - accuracy: 0.8404 - val_loss: 0.8916 - val_accuracy: 0.6679\n",
      "Epoch 563/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4436 - accuracy: 0.8386 - val_loss: 0.8723 - val_accuracy: 0.6691\n",
      "Epoch 564/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4344 - accuracy: 0.8453 - val_loss: 0.8921 - val_accuracy: 0.6765\n",
      "Epoch 565/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4270 - accuracy: 0.8514 - val_loss: 0.8690 - val_accuracy: 0.6827\n",
      "Epoch 566/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4330 - accuracy: 0.8477 - val_loss: 0.8821 - val_accuracy: 0.6778\n",
      "Epoch 567/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4284 - accuracy: 0.8526 - val_loss: 0.8971 - val_accuracy: 0.6691\n",
      "Epoch 568/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4312 - accuracy: 0.8404 - val_loss: 0.8875 - val_accuracy: 0.6778\n",
      "Epoch 569/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4375 - accuracy: 0.8429 - val_loss: 0.8549 - val_accuracy: 0.6889\n",
      "Epoch 570/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4227 - accuracy: 0.8569 - val_loss: 0.8753 - val_accuracy: 0.6716\n",
      "Epoch 571/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4419 - accuracy: 0.8453 - val_loss: 0.8747 - val_accuracy: 0.6840\n",
      "Epoch 572/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4512 - accuracy: 0.8392 - val_loss: 0.8747 - val_accuracy: 0.6802\n",
      "Epoch 573/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4418 - accuracy: 0.8392 - val_loss: 0.8729 - val_accuracy: 0.6704\n",
      "Epoch 574/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4419 - accuracy: 0.8423 - val_loss: 0.8798 - val_accuracy: 0.6877\n",
      "Epoch 575/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4303 - accuracy: 0.8429 - val_loss: 0.8671 - val_accuracy: 0.6716\n",
      "Epoch 576/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4184 - accuracy: 0.8490 - val_loss: 0.8966 - val_accuracy: 0.6765\n",
      "Epoch 577/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4275 - accuracy: 0.8496 - val_loss: 0.9382 - val_accuracy: 0.6531\n",
      "Epoch 578/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4277 - accuracy: 0.8508 - val_loss: 0.8649 - val_accuracy: 0.6901\n",
      "Epoch 579/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4243 - accuracy: 0.8471 - val_loss: 0.8688 - val_accuracy: 0.6914\n",
      "Epoch 580/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4124 - accuracy: 0.8484 - val_loss: 0.8986 - val_accuracy: 0.6728\n",
      "Epoch 581/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4182 - accuracy: 0.8575 - val_loss: 0.8588 - val_accuracy: 0.6827\n",
      "Epoch 582/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4465 - accuracy: 0.8508 - val_loss: 0.8808 - val_accuracy: 0.6716\n",
      "Epoch 583/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4323 - accuracy: 0.8538 - val_loss: 0.8699 - val_accuracy: 0.6815\n",
      "Epoch 584/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4123 - accuracy: 0.8496 - val_loss: 0.8818 - val_accuracy: 0.6765\n",
      "Epoch 585/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4149 - accuracy: 0.8490 - val_loss: 0.8857 - val_accuracy: 0.6593\n",
      "Epoch 586/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4423 - accuracy: 0.8392 - val_loss: 0.8811 - val_accuracy: 0.6926\n",
      "Epoch 587/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4108 - accuracy: 0.8636 - val_loss: 0.8955 - val_accuracy: 0.6716\n",
      "Epoch 588/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4146 - accuracy: 0.8538 - val_loss: 0.8846 - val_accuracy: 0.6716\n",
      "Epoch 589/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4282 - accuracy: 0.8447 - val_loss: 0.9026 - val_accuracy: 0.6654\n",
      "Epoch 590/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4171 - accuracy: 0.8618 - val_loss: 0.8892 - val_accuracy: 0.6790\n",
      "Epoch 591/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4241 - accuracy: 0.8551 - val_loss: 0.8964 - val_accuracy: 0.6704\n",
      "Epoch 592/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4307 - accuracy: 0.8410 - val_loss: 0.8824 - val_accuracy: 0.6938\n",
      "Epoch 593/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4103 - accuracy: 0.8447 - val_loss: 0.9013 - val_accuracy: 0.6864\n",
      "Epoch 594/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4103 - accuracy: 0.8496 - val_loss: 0.8890 - val_accuracy: 0.6753\n",
      "Epoch 595/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4035 - accuracy: 0.8611 - val_loss: 0.8964 - val_accuracy: 0.6765\n",
      "Epoch 596/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3875 - accuracy: 0.8685 - val_loss: 0.8839 - val_accuracy: 0.6802\n",
      "Epoch 597/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4315 - accuracy: 0.8356 - val_loss: 0.8860 - val_accuracy: 0.6716\n",
      "Epoch 598/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4061 - accuracy: 0.8611 - val_loss: 0.8837 - val_accuracy: 0.6642\n",
      "Epoch 599/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4033 - accuracy: 0.8496 - val_loss: 0.8821 - val_accuracy: 0.6716\n",
      "Epoch 600/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4031 - accuracy: 0.8593 - val_loss: 0.8895 - val_accuracy: 0.6815\n",
      "Epoch 601/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3942 - accuracy: 0.8678 - val_loss: 0.8881 - val_accuracy: 0.6679\n",
      "Epoch 602/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3960 - accuracy: 0.8654 - val_loss: 0.8589 - val_accuracy: 0.6753\n",
      "Epoch 603/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4147 - accuracy: 0.8551 - val_loss: 0.8823 - val_accuracy: 0.6765\n",
      "Epoch 604/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4049 - accuracy: 0.8624 - val_loss: 0.8884 - val_accuracy: 0.6679\n",
      "Epoch 605/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3962 - accuracy: 0.8672 - val_loss: 0.8546 - val_accuracy: 0.6926\n",
      "Epoch 606/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4113 - accuracy: 0.8484 - val_loss: 0.8774 - val_accuracy: 0.6926\n",
      "Epoch 607/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4064 - accuracy: 0.8618 - val_loss: 0.8734 - val_accuracy: 0.6877\n",
      "Epoch 608/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4070 - accuracy: 0.8599 - val_loss: 0.8717 - val_accuracy: 0.6864\n",
      "Epoch 609/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3858 - accuracy: 0.8575 - val_loss: 0.9196 - val_accuracy: 0.6728\n",
      "Epoch 610/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3940 - accuracy: 0.8618 - val_loss: 0.8769 - val_accuracy: 0.6802\n",
      "Epoch 611/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4067 - accuracy: 0.8569 - val_loss: 0.9275 - val_accuracy: 0.6691\n",
      "Epoch 612/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4017 - accuracy: 0.8575 - val_loss: 0.8781 - val_accuracy: 0.6864\n",
      "Epoch 613/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4014 - accuracy: 0.8654 - val_loss: 0.8783 - val_accuracy: 0.6840\n",
      "Epoch 614/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4327 - accuracy: 0.8465 - val_loss: 0.8620 - val_accuracy: 0.6877\n",
      "Epoch 615/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3972 - accuracy: 0.8648 - val_loss: 0.8712 - val_accuracy: 0.6889\n",
      "Epoch 616/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3994 - accuracy: 0.8642 - val_loss: 0.8736 - val_accuracy: 0.6765\n",
      "Epoch 617/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3893 - accuracy: 0.8672 - val_loss: 0.8880 - val_accuracy: 0.6802\n",
      "Epoch 618/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3866 - accuracy: 0.8672 - val_loss: 0.8981 - val_accuracy: 0.6827\n",
      "Epoch 619/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3953 - accuracy: 0.8587 - val_loss: 0.8707 - val_accuracy: 0.6951\n",
      "Epoch 620/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3949 - accuracy: 0.8624 - val_loss: 0.8791 - val_accuracy: 0.6889\n",
      "Epoch 621/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3947 - accuracy: 0.8618 - val_loss: 0.8825 - val_accuracy: 0.6877\n",
      "Epoch 622/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3814 - accuracy: 0.8611 - val_loss: 0.8543 - val_accuracy: 0.6889\n",
      "Epoch 623/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4001 - accuracy: 0.8599 - val_loss: 0.9036 - val_accuracy: 0.6840\n",
      "Epoch 624/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3877 - accuracy: 0.8697 - val_loss: 0.8658 - val_accuracy: 0.6926\n",
      "Epoch 625/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3891 - accuracy: 0.8605 - val_loss: 0.8905 - val_accuracy: 0.6790\n",
      "Epoch 626/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3866 - accuracy: 0.8624 - val_loss: 0.8803 - val_accuracy: 0.6988\n",
      "Epoch 627/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3975 - accuracy: 0.8599 - val_loss: 0.8742 - val_accuracy: 0.6728\n",
      "Epoch 628/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3815 - accuracy: 0.8593 - val_loss: 0.8662 - val_accuracy: 0.6914\n",
      "Epoch 629/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3914 - accuracy: 0.8618 - val_loss: 0.8738 - val_accuracy: 0.6840\n",
      "Epoch 630/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3828 - accuracy: 0.8581 - val_loss: 0.8893 - val_accuracy: 0.6753\n",
      "Epoch 631/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3789 - accuracy: 0.8764 - val_loss: 0.8870 - val_accuracy: 0.6716\n",
      "Epoch 632/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3815 - accuracy: 0.8648 - val_loss: 0.8859 - val_accuracy: 0.6840\n",
      "Epoch 633/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4026 - accuracy: 0.8520 - val_loss: 0.8676 - val_accuracy: 0.6815\n",
      "Epoch 634/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.3962 - accuracy: 0.8630 - val_loss: 0.8952 - val_accuracy: 0.6864\n",
      "Epoch 635/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3879 - accuracy: 0.8703 - val_loss: 0.8808 - val_accuracy: 0.6938\n",
      "Epoch 636/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3942 - accuracy: 0.8624 - val_loss: 0.8644 - val_accuracy: 0.6926\n",
      "Epoch 637/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3797 - accuracy: 0.8611 - val_loss: 0.8965 - val_accuracy: 0.6790\n",
      "Epoch 638/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3773 - accuracy: 0.8721 - val_loss: 0.8847 - val_accuracy: 0.6827\n",
      "Epoch 639/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3810 - accuracy: 0.8685 - val_loss: 0.8678 - val_accuracy: 0.6864\n",
      "Epoch 640/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3793 - accuracy: 0.8739 - val_loss: 0.8616 - val_accuracy: 0.6753\n",
      "Epoch 641/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3584 - accuracy: 0.8715 - val_loss: 0.8848 - val_accuracy: 0.6889\n",
      "Epoch 642/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3748 - accuracy: 0.8697 - val_loss: 0.8940 - val_accuracy: 0.6926\n",
      "Epoch 643/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3840 - accuracy: 0.8605 - val_loss: 0.8623 - val_accuracy: 0.6938\n",
      "Epoch 644/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3756 - accuracy: 0.8654 - val_loss: 0.8892 - val_accuracy: 0.6852\n",
      "Epoch 645/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3721 - accuracy: 0.8666 - val_loss: 0.8935 - val_accuracy: 0.6741\n",
      "Epoch 646/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3893 - accuracy: 0.8624 - val_loss: 0.8898 - val_accuracy: 0.6840\n",
      "Epoch 647/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3757 - accuracy: 0.8709 - val_loss: 0.9217 - val_accuracy: 0.6840\n",
      "Epoch 648/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3574 - accuracy: 0.8739 - val_loss: 0.8900 - val_accuracy: 0.6951\n",
      "Epoch 649/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3670 - accuracy: 0.8794 - val_loss: 0.8740 - val_accuracy: 0.6840\n",
      "Epoch 650/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3789 - accuracy: 0.8721 - val_loss: 0.8825 - val_accuracy: 0.6877\n",
      "Epoch 651/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3757 - accuracy: 0.8703 - val_loss: 0.9123 - val_accuracy: 0.6815\n",
      "Epoch 652/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3734 - accuracy: 0.8648 - val_loss: 0.8768 - val_accuracy: 0.6914\n",
      "Epoch 653/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3650 - accuracy: 0.8654 - val_loss: 0.8785 - val_accuracy: 0.6802\n",
      "Epoch 654/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3607 - accuracy: 0.8867 - val_loss: 0.8680 - val_accuracy: 0.6963\n",
      "Epoch 655/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3809 - accuracy: 0.8721 - val_loss: 0.8794 - val_accuracy: 0.6728\n",
      "Epoch 656/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3607 - accuracy: 0.8776 - val_loss: 0.8987 - val_accuracy: 0.6840\n",
      "Epoch 657/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3619 - accuracy: 0.8758 - val_loss: 0.8850 - val_accuracy: 0.6951\n",
      "Epoch 658/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3694 - accuracy: 0.8782 - val_loss: 0.9004 - val_accuracy: 0.6938\n",
      "Epoch 659/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3734 - accuracy: 0.8678 - val_loss: 0.8567 - val_accuracy: 0.6963\n",
      "Epoch 660/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3596 - accuracy: 0.8764 - val_loss: 0.9048 - val_accuracy: 0.6827\n",
      "Epoch 661/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.3556 - accuracy: 0.8727 - val_loss: 0.8933 - val_accuracy: 0.6901\n",
      "Epoch 662/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3510 - accuracy: 0.8764 - val_loss: 0.8743 - val_accuracy: 0.6951\n",
      "Epoch 663/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3528 - accuracy: 0.8745 - val_loss: 0.8781 - val_accuracy: 0.6901\n",
      "Epoch 664/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3464 - accuracy: 0.8739 - val_loss: 0.8732 - val_accuracy: 0.6951\n",
      "Epoch 665/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3495 - accuracy: 0.8739 - val_loss: 0.8737 - val_accuracy: 0.6889\n",
      "Epoch 666/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3609 - accuracy: 0.8721 - val_loss: 0.8776 - val_accuracy: 0.6864\n",
      "Epoch 667/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3852 - accuracy: 0.8636 - val_loss: 0.8676 - val_accuracy: 0.6938\n",
      "Epoch 668/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3420 - accuracy: 0.8825 - val_loss: 0.8887 - val_accuracy: 0.6864\n",
      "Epoch 669/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3653 - accuracy: 0.8691 - val_loss: 0.8726 - val_accuracy: 0.6802\n",
      "Epoch 670/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3514 - accuracy: 0.8837 - val_loss: 0.8740 - val_accuracy: 0.6827\n",
      "Epoch 671/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3413 - accuracy: 0.8812 - val_loss: 0.8672 - val_accuracy: 0.7099\n",
      "Epoch 672/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3510 - accuracy: 0.8721 - val_loss: 0.8606 - val_accuracy: 0.7025\n",
      "Epoch 673/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3400 - accuracy: 0.8916 - val_loss: 0.8906 - val_accuracy: 0.6877\n",
      "Epoch 674/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3550 - accuracy: 0.8788 - val_loss: 0.9091 - val_accuracy: 0.6852\n",
      "Epoch 675/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3647 - accuracy: 0.8691 - val_loss: 0.8967 - val_accuracy: 0.6827\n",
      "Epoch 676/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3562 - accuracy: 0.8721 - val_loss: 0.8696 - val_accuracy: 0.6938\n",
      "Epoch 677/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3497 - accuracy: 0.8770 - val_loss: 0.8899 - val_accuracy: 0.6864\n",
      "Epoch 678/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3645 - accuracy: 0.8678 - val_loss: 0.8791 - val_accuracy: 0.6864\n",
      "Epoch 679/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3545 - accuracy: 0.8745 - val_loss: 0.8638 - val_accuracy: 0.7099\n",
      "Epoch 680/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3439 - accuracy: 0.8819 - val_loss: 0.8723 - val_accuracy: 0.6914\n",
      "Epoch 681/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3481 - accuracy: 0.8831 - val_loss: 0.8792 - val_accuracy: 0.6938\n",
      "Epoch 682/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3415 - accuracy: 0.8758 - val_loss: 0.8522 - val_accuracy: 0.7037\n",
      "Epoch 683/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3246 - accuracy: 0.8892 - val_loss: 0.9219 - val_accuracy: 0.6802\n",
      "Epoch 684/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3516 - accuracy: 0.8739 - val_loss: 0.8908 - val_accuracy: 0.6938\n",
      "Epoch 685/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3554 - accuracy: 0.8764 - val_loss: 0.8938 - val_accuracy: 0.6691\n",
      "Epoch 686/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3448 - accuracy: 0.8819 - val_loss: 0.8880 - val_accuracy: 0.6938\n",
      "Epoch 687/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3484 - accuracy: 0.8806 - val_loss: 0.8832 - val_accuracy: 0.6840\n",
      "Epoch 688/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3549 - accuracy: 0.8703 - val_loss: 0.8795 - val_accuracy: 0.6938\n",
      "Epoch 689/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3501 - accuracy: 0.8715 - val_loss: 0.8763 - val_accuracy: 0.6938\n",
      "Epoch 690/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3481 - accuracy: 0.8800 - val_loss: 0.9225 - val_accuracy: 0.6667\n",
      "Epoch 691/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3343 - accuracy: 0.8837 - val_loss: 0.8952 - val_accuracy: 0.6889\n",
      "Epoch 692/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3440 - accuracy: 0.8867 - val_loss: 0.8884 - val_accuracy: 0.6778\n",
      "Epoch 693/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3366 - accuracy: 0.8849 - val_loss: 0.8900 - val_accuracy: 0.6778\n",
      "Epoch 694/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3630 - accuracy: 0.8752 - val_loss: 0.8955 - val_accuracy: 0.6864\n",
      "Epoch 695/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3368 - accuracy: 0.8910 - val_loss: 0.8812 - val_accuracy: 0.7037\n",
      "Epoch 696/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3460 - accuracy: 0.8800 - val_loss: 0.8781 - val_accuracy: 0.6914\n",
      "Epoch 697/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3111 - accuracy: 0.8946 - val_loss: 0.9485 - val_accuracy: 0.6642\n",
      "Epoch 698/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3380 - accuracy: 0.8703 - val_loss: 0.8876 - val_accuracy: 0.6778\n",
      "Epoch 699/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3289 - accuracy: 0.8922 - val_loss: 0.8917 - val_accuracy: 0.7000\n",
      "Epoch 700/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3437 - accuracy: 0.8770 - val_loss: 0.8825 - val_accuracy: 0.6988\n",
      "Epoch 701/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3462 - accuracy: 0.8758 - val_loss: 0.8779 - val_accuracy: 0.6975\n",
      "Epoch 702/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3401 - accuracy: 0.8861 - val_loss: 0.8775 - val_accuracy: 0.6963\n",
      "Epoch 703/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3359 - accuracy: 0.8825 - val_loss: 0.8819 - val_accuracy: 0.6901\n",
      "Epoch 704/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3089 - accuracy: 0.8904 - val_loss: 0.8988 - val_accuracy: 0.6963\n",
      "Epoch 705/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3422 - accuracy: 0.8764 - val_loss: 0.8645 - val_accuracy: 0.7025\n",
      "Epoch 706/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3465 - accuracy: 0.8819 - val_loss: 0.8598 - val_accuracy: 0.7000\n",
      "Epoch 707/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3423 - accuracy: 0.8831 - val_loss: 0.8630 - val_accuracy: 0.6914\n",
      "Epoch 708/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3146 - accuracy: 0.8922 - val_loss: 0.8653 - val_accuracy: 0.6914\n",
      "Epoch 709/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3256 - accuracy: 0.8879 - val_loss: 0.8689 - val_accuracy: 0.6926\n",
      "Epoch 710/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3403 - accuracy: 0.8879 - val_loss: 0.9025 - val_accuracy: 0.6963\n",
      "Epoch 711/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3378 - accuracy: 0.8782 - val_loss: 0.8912 - val_accuracy: 0.6914\n",
      "Epoch 712/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3348 - accuracy: 0.8782 - val_loss: 0.8653 - val_accuracy: 0.7062\n",
      "Epoch 713/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3243 - accuracy: 0.8843 - val_loss: 0.9003 - val_accuracy: 0.6938\n",
      "Epoch 714/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3280 - accuracy: 0.8843 - val_loss: 0.8657 - val_accuracy: 0.6975\n",
      "Epoch 715/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3286 - accuracy: 0.8861 - val_loss: 0.8831 - val_accuracy: 0.6951\n",
      "Epoch 716/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3271 - accuracy: 0.8873 - val_loss: 0.8760 - val_accuracy: 0.7074\n",
      "Epoch 717/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3382 - accuracy: 0.8861 - val_loss: 0.9176 - val_accuracy: 0.6765\n",
      "Epoch 718/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3410 - accuracy: 0.8745 - val_loss: 0.8961 - val_accuracy: 0.6901\n",
      "Epoch 719/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3163 - accuracy: 0.8861 - val_loss: 0.8867 - val_accuracy: 0.6988\n",
      "Epoch 720/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3170 - accuracy: 0.8959 - val_loss: 0.8926 - val_accuracy: 0.6889\n",
      "Epoch 721/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3183 - accuracy: 0.8922 - val_loss: 0.8836 - val_accuracy: 0.6914\n",
      "Epoch 722/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3286 - accuracy: 0.8825 - val_loss: 0.8768 - val_accuracy: 0.6914\n",
      "Epoch 723/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3168 - accuracy: 0.8934 - val_loss: 0.8891 - val_accuracy: 0.6975\n",
      "Epoch 724/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3091 - accuracy: 0.8922 - val_loss: 0.8906 - val_accuracy: 0.6914\n",
      "Epoch 725/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3110 - accuracy: 0.8855 - val_loss: 0.8881 - val_accuracy: 0.6975\n",
      "Epoch 726/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3283 - accuracy: 0.8825 - val_loss: 0.8738 - val_accuracy: 0.6975\n",
      "Epoch 727/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3305 - accuracy: 0.8855 - val_loss: 0.8707 - val_accuracy: 0.6975\n",
      "Epoch 728/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3319 - accuracy: 0.8873 - val_loss: 0.8711 - val_accuracy: 0.6901\n",
      "Epoch 729/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3223 - accuracy: 0.8983 - val_loss: 0.9012 - val_accuracy: 0.6914\n",
      "Epoch 730/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3145 - accuracy: 0.8922 - val_loss: 0.8792 - val_accuracy: 0.7000\n",
      "Epoch 731/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3127 - accuracy: 0.8879 - val_loss: 0.8720 - val_accuracy: 0.7074\n",
      "Epoch 732/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3191 - accuracy: 0.8898 - val_loss: 0.8927 - val_accuracy: 0.6889\n",
      "Epoch 733/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3169 - accuracy: 0.8867 - val_loss: 0.9103 - val_accuracy: 0.6741\n",
      "Epoch 734/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2983 - accuracy: 0.9044 - val_loss: 0.9236 - val_accuracy: 0.6852\n",
      "Epoch 735/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3206 - accuracy: 0.8837 - val_loss: 0.9114 - val_accuracy: 0.6852\n",
      "Epoch 736/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3207 - accuracy: 0.8825 - val_loss: 0.8926 - val_accuracy: 0.6951\n",
      "Epoch 737/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3152 - accuracy: 0.8934 - val_loss: 0.8877 - val_accuracy: 0.7000\n",
      "Epoch 738/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3069 - accuracy: 0.9001 - val_loss: 0.8922 - val_accuracy: 0.6926\n",
      "Epoch 739/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3179 - accuracy: 0.8867 - val_loss: 0.8632 - val_accuracy: 0.7037\n",
      "Epoch 740/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3120 - accuracy: 0.8886 - val_loss: 0.8777 - val_accuracy: 0.7000\n",
      "Epoch 741/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3216 - accuracy: 0.8928 - val_loss: 0.8800 - val_accuracy: 0.6877\n",
      "Epoch 742/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3183 - accuracy: 0.8977 - val_loss: 0.9041 - val_accuracy: 0.6938\n",
      "Epoch 743/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3059 - accuracy: 0.8965 - val_loss: 0.8814 - val_accuracy: 0.7062\n",
      "Epoch 744/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3221 - accuracy: 0.8928 - val_loss: 0.8866 - val_accuracy: 0.7160\n",
      "Epoch 745/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3052 - accuracy: 0.8983 - val_loss: 0.9122 - val_accuracy: 0.6852\n",
      "Epoch 746/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3223 - accuracy: 0.8867 - val_loss: 0.8940 - val_accuracy: 0.6914\n",
      "Epoch 747/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2981 - accuracy: 0.8965 - val_loss: 0.9558 - val_accuracy: 0.6827\n",
      "Epoch 748/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3019 - accuracy: 0.8983 - val_loss: 0.8907 - val_accuracy: 0.6963\n",
      "Epoch 749/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3107 - accuracy: 0.8886 - val_loss: 0.9130 - val_accuracy: 0.6889\n",
      "Epoch 750/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3058 - accuracy: 0.8940 - val_loss: 0.9079 - val_accuracy: 0.6938\n",
      "Epoch 751/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2998 - accuracy: 0.8910 - val_loss: 0.9202 - val_accuracy: 0.6877\n",
      "Epoch 752/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3014 - accuracy: 0.8971 - val_loss: 0.9130 - val_accuracy: 0.6938\n",
      "Epoch 753/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3051 - accuracy: 0.8940 - val_loss: 0.8944 - val_accuracy: 0.6840\n",
      "Epoch 754/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3236 - accuracy: 0.8788 - val_loss: 0.9255 - val_accuracy: 0.6877\n",
      "Epoch 755/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3042 - accuracy: 0.8922 - val_loss: 0.8839 - val_accuracy: 0.6951\n",
      "Epoch 756/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2949 - accuracy: 0.9026 - val_loss: 0.9081 - val_accuracy: 0.6889\n",
      "Epoch 757/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2973 - accuracy: 0.9013 - val_loss: 0.9013 - val_accuracy: 0.6951\n",
      "Epoch 758/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3030 - accuracy: 0.8898 - val_loss: 0.8872 - val_accuracy: 0.6975\n",
      "Epoch 759/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3000 - accuracy: 0.9013 - val_loss: 0.8876 - val_accuracy: 0.6864\n",
      "Epoch 760/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2881 - accuracy: 0.9086 - val_loss: 0.9549 - val_accuracy: 0.6889\n",
      "Epoch 761/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2999 - accuracy: 0.8934 - val_loss: 0.8751 - val_accuracy: 0.7025\n",
      "Epoch 762/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2861 - accuracy: 0.9044 - val_loss: 0.8974 - val_accuracy: 0.7062\n",
      "Epoch 763/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3027 - accuracy: 0.8946 - val_loss: 0.8916 - val_accuracy: 0.7012\n",
      "Epoch 764/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3041 - accuracy: 0.8934 - val_loss: 0.8895 - val_accuracy: 0.6877\n",
      "Epoch 765/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2865 - accuracy: 0.9050 - val_loss: 0.8866 - val_accuracy: 0.6988\n",
      "Epoch 766/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2964 - accuracy: 0.9001 - val_loss: 0.8887 - val_accuracy: 0.7062\n",
      "Epoch 767/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2963 - accuracy: 0.8904 - val_loss: 0.9111 - val_accuracy: 0.6963\n",
      "Epoch 768/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2959 - accuracy: 0.8959 - val_loss: 0.8866 - val_accuracy: 0.7012\n",
      "Epoch 769/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2902 - accuracy: 0.8995 - val_loss: 0.8770 - val_accuracy: 0.6963\n",
      "Epoch 770/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2981 - accuracy: 0.8959 - val_loss: 0.9048 - val_accuracy: 0.6988\n",
      "Epoch 771/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2949 - accuracy: 0.9032 - val_loss: 0.8986 - val_accuracy: 0.6975\n",
      "Epoch 772/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3101 - accuracy: 0.8855 - val_loss: 0.8939 - val_accuracy: 0.7025\n",
      "Epoch 773/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2883 - accuracy: 0.8995 - val_loss: 0.8844 - val_accuracy: 0.6963\n",
      "Epoch 774/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3005 - accuracy: 0.9026 - val_loss: 0.9022 - val_accuracy: 0.6926\n",
      "Epoch 775/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2892 - accuracy: 0.8952 - val_loss: 0.8801 - val_accuracy: 0.6889\n",
      "Epoch 776/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2887 - accuracy: 0.9105 - val_loss: 0.9234 - val_accuracy: 0.6914\n",
      "Epoch 777/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2956 - accuracy: 0.8965 - val_loss: 0.8828 - val_accuracy: 0.7000\n",
      "Epoch 778/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2821 - accuracy: 0.9019 - val_loss: 0.9018 - val_accuracy: 0.7025\n",
      "Epoch 779/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2662 - accuracy: 0.9178 - val_loss: 0.8969 - val_accuracy: 0.7025\n",
      "Epoch 780/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2833 - accuracy: 0.8995 - val_loss: 0.9083 - val_accuracy: 0.6901\n",
      "Epoch 781/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2969 - accuracy: 0.8989 - val_loss: 0.8849 - val_accuracy: 0.7062\n",
      "Epoch 782/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2866 - accuracy: 0.8940 - val_loss: 0.9276 - val_accuracy: 0.6827\n",
      "Epoch 783/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2827 - accuracy: 0.9080 - val_loss: 0.9107 - val_accuracy: 0.7086\n",
      "Epoch 784/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3010 - accuracy: 0.8946 - val_loss: 0.8844 - val_accuracy: 0.7111\n",
      "Epoch 785/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2882 - accuracy: 0.8965 - val_loss: 0.8805 - val_accuracy: 0.6951\n",
      "Epoch 786/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2756 - accuracy: 0.9105 - val_loss: 0.9200 - val_accuracy: 0.6864\n",
      "Epoch 787/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2969 - accuracy: 0.8892 - val_loss: 0.8791 - val_accuracy: 0.7099\n",
      "Epoch 788/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2730 - accuracy: 0.9001 - val_loss: 0.9153 - val_accuracy: 0.6914\n",
      "Epoch 789/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2592 - accuracy: 0.9141 - val_loss: 0.9392 - val_accuracy: 0.6790\n",
      "Epoch 790/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2873 - accuracy: 0.8959 - val_loss: 0.8923 - val_accuracy: 0.6840\n",
      "Epoch 791/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2803 - accuracy: 0.9026 - val_loss: 0.9135 - val_accuracy: 0.6901\n",
      "Epoch 792/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2823 - accuracy: 0.9044 - val_loss: 0.8982 - val_accuracy: 0.7000\n",
      "Epoch 793/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2725 - accuracy: 0.9068 - val_loss: 0.9034 - val_accuracy: 0.7012\n",
      "Epoch 794/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2711 - accuracy: 0.9074 - val_loss: 0.9040 - val_accuracy: 0.6963\n",
      "Epoch 795/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2743 - accuracy: 0.9032 - val_loss: 0.9047 - val_accuracy: 0.6951\n",
      "Epoch 796/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2661 - accuracy: 0.9062 - val_loss: 0.9248 - val_accuracy: 0.6975\n",
      "Epoch 797/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2807 - accuracy: 0.9111 - val_loss: 0.9327 - val_accuracy: 0.6840\n",
      "Epoch 798/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2747 - accuracy: 0.9123 - val_loss: 0.8976 - val_accuracy: 0.7111\n",
      "Epoch 799/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2632 - accuracy: 0.9038 - val_loss: 0.9086 - val_accuracy: 0.6790\n",
      "Epoch 800/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2822 - accuracy: 0.9001 - val_loss: 0.9153 - val_accuracy: 0.6827\n",
      "Epoch 801/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2864 - accuracy: 0.9099 - val_loss: 0.9069 - val_accuracy: 0.6963\n",
      "Epoch 802/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2769 - accuracy: 0.9080 - val_loss: 0.8797 - val_accuracy: 0.7049\n",
      "Epoch 803/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2789 - accuracy: 0.8995 - val_loss: 0.9122 - val_accuracy: 0.6901\n",
      "Epoch 804/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2707 - accuracy: 0.9068 - val_loss: 0.8915 - val_accuracy: 0.6963\n",
      "Epoch 805/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2823 - accuracy: 0.9105 - val_loss: 0.9104 - val_accuracy: 0.6889\n",
      "Epoch 806/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2802 - accuracy: 0.8977 - val_loss: 0.8928 - val_accuracy: 0.7012\n",
      "Epoch 807/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2786 - accuracy: 0.9086 - val_loss: 0.9133 - val_accuracy: 0.6938\n",
      "Epoch 808/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2758 - accuracy: 0.9001 - val_loss: 0.9311 - val_accuracy: 0.6840\n",
      "Epoch 809/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2814 - accuracy: 0.9056 - val_loss: 0.8993 - val_accuracy: 0.6988\n",
      "Epoch 810/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2602 - accuracy: 0.9026 - val_loss: 0.8798 - val_accuracy: 0.7185\n",
      "Epoch 811/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2780 - accuracy: 0.9093 - val_loss: 0.9067 - val_accuracy: 0.6938\n",
      "Epoch 812/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2705 - accuracy: 0.9093 - val_loss: 0.9046 - val_accuracy: 0.7148\n",
      "Epoch 813/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2781 - accuracy: 0.9056 - val_loss: 0.9006 - val_accuracy: 0.6852\n",
      "Epoch 814/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2714 - accuracy: 0.9074 - val_loss: 0.8995 - val_accuracy: 0.6827\n",
      "Epoch 815/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2742 - accuracy: 0.9038 - val_loss: 0.9132 - val_accuracy: 0.6877\n",
      "Epoch 816/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2718 - accuracy: 0.9001 - val_loss: 0.9106 - val_accuracy: 0.7062\n",
      "Epoch 817/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2615 - accuracy: 0.9153 - val_loss: 0.8933 - val_accuracy: 0.6938\n",
      "Epoch 818/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2684 - accuracy: 0.9086 - val_loss: 0.8861 - val_accuracy: 0.6951\n",
      "Epoch 819/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2568 - accuracy: 0.9141 - val_loss: 0.9017 - val_accuracy: 0.7012\n",
      "Epoch 820/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2939 - accuracy: 0.9044 - val_loss: 0.8978 - val_accuracy: 0.7062\n",
      "Epoch 821/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2588 - accuracy: 0.9117 - val_loss: 0.9393 - val_accuracy: 0.6864\n",
      "Epoch 822/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2662 - accuracy: 0.9105 - val_loss: 0.9026 - val_accuracy: 0.7086\n",
      "Epoch 823/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2535 - accuracy: 0.9123 - val_loss: 0.9407 - val_accuracy: 0.6877\n",
      "Epoch 824/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2798 - accuracy: 0.9111 - val_loss: 0.9225 - val_accuracy: 0.6988\n",
      "Epoch 825/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2782 - accuracy: 0.8983 - val_loss: 0.9181 - val_accuracy: 0.7025\n",
      "Epoch 826/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2607 - accuracy: 0.9160 - val_loss: 0.9158 - val_accuracy: 0.6877\n",
      "Epoch 827/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2711 - accuracy: 0.9117 - val_loss: 0.9064 - val_accuracy: 0.6840\n",
      "Epoch 828/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2772 - accuracy: 0.9013 - val_loss: 0.8987 - val_accuracy: 0.6988\n",
      "Epoch 829/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2576 - accuracy: 0.9086 - val_loss: 0.9044 - val_accuracy: 0.6988\n",
      "Epoch 830/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2880 - accuracy: 0.8952 - val_loss: 0.9003 - val_accuracy: 0.7037\n",
      "Epoch 831/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2657 - accuracy: 0.9153 - val_loss: 0.9127 - val_accuracy: 0.6926\n",
      "Epoch 832/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2740 - accuracy: 0.9050 - val_loss: 0.8981 - val_accuracy: 0.7037\n",
      "Epoch 833/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2723 - accuracy: 0.9056 - val_loss: 0.8999 - val_accuracy: 0.6877\n",
      "Epoch 834/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2653 - accuracy: 0.9068 - val_loss: 0.9018 - val_accuracy: 0.6988\n",
      "Epoch 835/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2488 - accuracy: 0.9184 - val_loss: 0.9113 - val_accuracy: 0.7037\n",
      "Epoch 836/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2698 - accuracy: 0.9050 - val_loss: 0.8930 - val_accuracy: 0.7025\n",
      "Epoch 837/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2479 - accuracy: 0.9196 - val_loss: 0.9229 - val_accuracy: 0.6938\n",
      "Epoch 838/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2785 - accuracy: 0.9062 - val_loss: 0.8986 - val_accuracy: 0.6988\n",
      "Epoch 839/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2609 - accuracy: 0.9056 - val_loss: 0.9255 - val_accuracy: 0.6963\n",
      "Epoch 840/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2508 - accuracy: 0.9196 - val_loss: 0.8899 - val_accuracy: 0.7012\n",
      "Epoch 841/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2596 - accuracy: 0.9117 - val_loss: 0.8970 - val_accuracy: 0.6988\n",
      "Epoch 842/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2510 - accuracy: 0.9135 - val_loss: 0.9278 - val_accuracy: 0.6901\n",
      "Epoch 843/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2592 - accuracy: 0.9013 - val_loss: 0.9144 - val_accuracy: 0.6988\n",
      "Epoch 844/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2548 - accuracy: 0.9141 - val_loss: 0.9138 - val_accuracy: 0.6901\n",
      "Epoch 845/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2547 - accuracy: 0.9227 - val_loss: 0.9016 - val_accuracy: 0.6963\n",
      "Epoch 846/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2501 - accuracy: 0.9141 - val_loss: 0.8900 - val_accuracy: 0.7037\n",
      "Epoch 847/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2690 - accuracy: 0.9074 - val_loss: 0.9095 - val_accuracy: 0.7025\n",
      "Epoch 848/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2339 - accuracy: 0.9208 - val_loss: 0.9289 - val_accuracy: 0.7025\n",
      "Epoch 849/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2478 - accuracy: 0.9160 - val_loss: 0.9435 - val_accuracy: 0.7049\n",
      "Epoch 850/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2598 - accuracy: 0.9080 - val_loss: 0.9029 - val_accuracy: 0.7062\n",
      "Epoch 851/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2582 - accuracy: 0.9220 - val_loss: 0.9245 - val_accuracy: 0.6889\n",
      "Epoch 852/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2574 - accuracy: 0.9068 - val_loss: 0.9174 - val_accuracy: 0.6926\n",
      "Epoch 853/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2528 - accuracy: 0.9111 - val_loss: 0.9144 - val_accuracy: 0.7000\n",
      "Epoch 854/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2579 - accuracy: 0.9123 - val_loss: 0.9040 - val_accuracy: 0.6988\n",
      "Epoch 855/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2317 - accuracy: 0.9208 - val_loss: 0.8999 - val_accuracy: 0.6988\n",
      "Epoch 856/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2697 - accuracy: 0.9099 - val_loss: 0.9134 - val_accuracy: 0.6951\n",
      "Epoch 857/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2556 - accuracy: 0.9117 - val_loss: 0.9104 - val_accuracy: 0.7000\n",
      "Epoch 858/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2599 - accuracy: 0.9026 - val_loss: 0.9207 - val_accuracy: 0.6877\n",
      "Epoch 859/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2622 - accuracy: 0.9147 - val_loss: 0.8945 - val_accuracy: 0.7074\n",
      "Epoch 860/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2555 - accuracy: 0.9117 - val_loss: 0.9023 - val_accuracy: 0.7037\n",
      "Epoch 861/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2572 - accuracy: 0.9160 - val_loss: 0.9326 - val_accuracy: 0.6988\n",
      "Epoch 862/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2438 - accuracy: 0.9196 - val_loss: 0.9155 - val_accuracy: 0.7025\n",
      "Epoch 863/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2707 - accuracy: 0.9044 - val_loss: 0.9162 - val_accuracy: 0.7025\n",
      "Epoch 864/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2441 - accuracy: 0.9184 - val_loss: 0.9156 - val_accuracy: 0.7012\n",
      "Epoch 865/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2495 - accuracy: 0.9123 - val_loss: 0.9017 - val_accuracy: 0.7062\n",
      "Epoch 866/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2468 - accuracy: 0.9147 - val_loss: 0.9010 - val_accuracy: 0.7049\n",
      "Epoch 867/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2452 - accuracy: 0.9196 - val_loss: 0.9220 - val_accuracy: 0.7074\n",
      "Epoch 868/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2508 - accuracy: 0.9202 - val_loss: 0.9083 - val_accuracy: 0.7012\n",
      "Epoch 869/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2324 - accuracy: 0.9318 - val_loss: 0.9266 - val_accuracy: 0.7037\n",
      "Epoch 870/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2327 - accuracy: 0.9208 - val_loss: 0.9782 - val_accuracy: 0.6963\n",
      "Epoch 871/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2679 - accuracy: 0.9062 - val_loss: 0.9377 - val_accuracy: 0.6914\n",
      "Epoch 872/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2427 - accuracy: 0.9147 - val_loss: 0.9047 - val_accuracy: 0.6988\n",
      "Epoch 873/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2468 - accuracy: 0.9105 - val_loss: 0.9205 - val_accuracy: 0.7025\n",
      "Epoch 874/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2331 - accuracy: 0.9257 - val_loss: 0.8980 - val_accuracy: 0.6963\n",
      "Epoch 875/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2418 - accuracy: 0.9184 - val_loss: 0.8892 - val_accuracy: 0.7062\n",
      "Epoch 876/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2379 - accuracy: 0.9172 - val_loss: 0.8883 - val_accuracy: 0.7136\n",
      "Epoch 877/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2629 - accuracy: 0.9111 - val_loss: 0.8988 - val_accuracy: 0.7025\n",
      "Epoch 878/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2591 - accuracy: 0.9111 - val_loss: 0.8901 - val_accuracy: 0.7049\n",
      "Epoch 879/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2414 - accuracy: 0.9208 - val_loss: 0.9123 - val_accuracy: 0.6901\n",
      "Epoch 880/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2321 - accuracy: 0.9166 - val_loss: 0.9027 - val_accuracy: 0.6963\n",
      "Epoch 881/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2389 - accuracy: 0.9117 - val_loss: 0.9077 - val_accuracy: 0.6938\n",
      "Epoch 882/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2346 - accuracy: 0.9227 - val_loss: 0.8748 - val_accuracy: 0.7111\n",
      "Epoch 883/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2418 - accuracy: 0.9287 - val_loss: 0.9276 - val_accuracy: 0.6938\n",
      "Epoch 884/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2331 - accuracy: 0.9178 - val_loss: 0.9499 - val_accuracy: 0.6938\n",
      "Epoch 885/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2450 - accuracy: 0.9129 - val_loss: 0.9129 - val_accuracy: 0.7012\n",
      "Epoch 886/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2385 - accuracy: 0.9233 - val_loss: 0.9084 - val_accuracy: 0.6975\n",
      "Epoch 887/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2392 - accuracy: 0.9208 - val_loss: 0.9172 - val_accuracy: 0.6988\n",
      "Epoch 888/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2422 - accuracy: 0.9147 - val_loss: 0.9243 - val_accuracy: 0.7037\n",
      "Epoch 889/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2275 - accuracy: 0.9281 - val_loss: 0.9249 - val_accuracy: 0.7099\n",
      "Epoch 890/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2330 - accuracy: 0.9190 - val_loss: 0.9059 - val_accuracy: 0.7086\n",
      "Epoch 891/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2323 - accuracy: 0.9220 - val_loss: 0.8936 - val_accuracy: 0.7074\n",
      "Epoch 892/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2248 - accuracy: 0.9257 - val_loss: 0.9334 - val_accuracy: 0.6988\n",
      "Epoch 893/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2423 - accuracy: 0.9099 - val_loss: 0.9490 - val_accuracy: 0.6901\n",
      "Epoch 894/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2330 - accuracy: 0.9257 - val_loss: 0.9088 - val_accuracy: 0.7074\n",
      "Epoch 895/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2294 - accuracy: 0.9166 - val_loss: 0.9212 - val_accuracy: 0.7000\n",
      "Epoch 896/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2277 - accuracy: 0.9233 - val_loss: 0.9381 - val_accuracy: 0.6914\n",
      "Epoch 897/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2294 - accuracy: 0.9178 - val_loss: 0.9457 - val_accuracy: 0.6889\n",
      "Epoch 898/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2411 - accuracy: 0.9196 - val_loss: 0.9380 - val_accuracy: 0.6938\n",
      "Epoch 899/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2379 - accuracy: 0.9227 - val_loss: 0.9365 - val_accuracy: 0.6938\n",
      "Epoch 900/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2175 - accuracy: 0.9306 - val_loss: 0.9176 - val_accuracy: 0.6951\n",
      "Epoch 901/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2304 - accuracy: 0.9251 - val_loss: 0.9276 - val_accuracy: 0.6963\n",
      "Epoch 902/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2208 - accuracy: 0.9202 - val_loss: 0.9403 - val_accuracy: 0.7012\n",
      "Epoch 903/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2311 - accuracy: 0.9269 - val_loss: 0.9201 - val_accuracy: 0.7210\n",
      "Epoch 904/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2260 - accuracy: 0.9287 - val_loss: 0.9469 - val_accuracy: 0.6988\n",
      "Epoch 905/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2211 - accuracy: 0.9263 - val_loss: 0.9109 - val_accuracy: 0.7099\n",
      "Epoch 906/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2440 - accuracy: 0.9135 - val_loss: 0.8932 - val_accuracy: 0.7099\n",
      "Epoch 907/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2376 - accuracy: 0.9214 - val_loss: 0.9308 - val_accuracy: 0.6963\n",
      "Epoch 908/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2377 - accuracy: 0.9233 - val_loss: 0.9166 - val_accuracy: 0.7037\n",
      "Epoch 909/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2166 - accuracy: 0.9257 - val_loss: 0.9158 - val_accuracy: 0.7136\n",
      "Epoch 910/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2216 - accuracy: 0.9239 - val_loss: 0.9483 - val_accuracy: 0.7000\n",
      "Epoch 911/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2329 - accuracy: 0.9245 - val_loss: 0.9374 - val_accuracy: 0.6975\n",
      "Epoch 912/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2255 - accuracy: 0.9202 - val_loss: 0.9440 - val_accuracy: 0.6926\n",
      "Epoch 913/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2207 - accuracy: 0.9281 - val_loss: 0.9685 - val_accuracy: 0.6951\n",
      "Epoch 914/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2223 - accuracy: 0.9263 - val_loss: 1.0085 - val_accuracy: 0.6938\n",
      "Epoch 915/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2390 - accuracy: 0.9172 - val_loss: 0.9134 - val_accuracy: 0.7000\n",
      "Epoch 916/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2244 - accuracy: 0.9257 - val_loss: 0.9021 - val_accuracy: 0.7025\n",
      "Epoch 917/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2130 - accuracy: 0.9348 - val_loss: 0.9274 - val_accuracy: 0.7037\n",
      "Epoch 918/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2322 - accuracy: 0.9281 - val_loss: 0.9305 - val_accuracy: 0.7037\n",
      "Epoch 919/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2130 - accuracy: 0.9239 - val_loss: 0.9351 - val_accuracy: 0.6889\n",
      "Epoch 920/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2283 - accuracy: 0.9196 - val_loss: 0.9530 - val_accuracy: 0.6926\n",
      "Epoch 921/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2417 - accuracy: 0.9178 - val_loss: 0.9049 - val_accuracy: 0.7148\n",
      "Epoch 922/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2337 - accuracy: 0.9214 - val_loss: 0.9392 - val_accuracy: 0.7062\n",
      "Epoch 923/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2243 - accuracy: 0.9263 - val_loss: 0.9414 - val_accuracy: 0.7099\n",
      "Epoch 924/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2317 - accuracy: 0.9196 - val_loss: 0.9174 - val_accuracy: 0.7062\n",
      "Epoch 925/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2109 - accuracy: 0.9318 - val_loss: 0.9291 - val_accuracy: 0.7025\n",
      "Epoch 926/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2239 - accuracy: 0.9269 - val_loss: 0.9231 - val_accuracy: 0.7173\n",
      "Epoch 927/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2295 - accuracy: 0.9172 - val_loss: 0.9280 - val_accuracy: 0.7037\n",
      "Epoch 928/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2175 - accuracy: 0.9239 - val_loss: 0.9269 - val_accuracy: 0.7086\n",
      "Epoch 929/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2172 - accuracy: 0.9312 - val_loss: 0.9215 - val_accuracy: 0.7136\n",
      "Epoch 930/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.1967 - accuracy: 0.9409 - val_loss: 0.9573 - val_accuracy: 0.7000\n",
      "Epoch 931/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2306 - accuracy: 0.9220 - val_loss: 0.9312 - val_accuracy: 0.7000\n",
      "Epoch 932/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2448 - accuracy: 0.9135 - val_loss: 0.9385 - val_accuracy: 0.7037\n",
      "Epoch 933/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2248 - accuracy: 0.9227 - val_loss: 0.9304 - val_accuracy: 0.7099\n",
      "Epoch 934/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2097 - accuracy: 0.9300 - val_loss: 0.9131 - val_accuracy: 0.7062\n",
      "Epoch 935/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2123 - accuracy: 0.9245 - val_loss: 0.9009 - val_accuracy: 0.7136\n",
      "Epoch 936/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2383 - accuracy: 0.9214 - val_loss: 0.9235 - val_accuracy: 0.7037\n",
      "Epoch 937/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2221 - accuracy: 0.9294 - val_loss: 0.9144 - val_accuracy: 0.7037\n",
      "Epoch 938/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2294 - accuracy: 0.9190 - val_loss: 0.9279 - val_accuracy: 0.6951\n",
      "Epoch 939/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.1997 - accuracy: 0.9354 - val_loss: 0.9103 - val_accuracy: 0.7062\n",
      "Epoch 940/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2255 - accuracy: 0.9227 - val_loss: 0.9266 - val_accuracy: 0.7074\n",
      "Epoch 941/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2188 - accuracy: 0.9257 - val_loss: 0.9019 - val_accuracy: 0.7074\n",
      "Epoch 942/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2129 - accuracy: 0.9330 - val_loss: 0.9080 - val_accuracy: 0.7062\n",
      "Epoch 943/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2214 - accuracy: 0.9287 - val_loss: 0.9344 - val_accuracy: 0.6901\n",
      "Epoch 944/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2133 - accuracy: 0.9281 - val_loss: 0.9342 - val_accuracy: 0.6938\n",
      "Epoch 945/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2095 - accuracy: 0.9306 - val_loss: 0.9142 - val_accuracy: 0.7000\n",
      "Epoch 946/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2248 - accuracy: 0.9269 - val_loss: 0.9461 - val_accuracy: 0.7062\n",
      "Epoch 947/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2315 - accuracy: 0.9166 - val_loss: 0.9317 - val_accuracy: 0.7012\n",
      "Epoch 948/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2087 - accuracy: 0.9287 - val_loss: 0.9633 - val_accuracy: 0.6951\n",
      "Epoch 949/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2192 - accuracy: 0.9348 - val_loss: 0.9300 - val_accuracy: 0.7086\n",
      "Epoch 950/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2159 - accuracy: 0.9361 - val_loss: 0.8941 - val_accuracy: 0.7148\n",
      "Epoch 951/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2076 - accuracy: 0.9269 - val_loss: 0.9521 - val_accuracy: 0.6889\n",
      "Epoch 952/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2050 - accuracy: 0.9324 - val_loss: 0.9293 - val_accuracy: 0.7086\n",
      "Epoch 953/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2249 - accuracy: 0.9202 - val_loss: 0.9629 - val_accuracy: 0.6926\n",
      "Epoch 954/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2111 - accuracy: 0.9324 - val_loss: 0.9325 - val_accuracy: 0.7136\n",
      "Epoch 955/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2024 - accuracy: 0.9336 - val_loss: 0.9088 - val_accuracy: 0.7086\n",
      "Epoch 956/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2294 - accuracy: 0.9160 - val_loss: 0.9415 - val_accuracy: 0.7012\n",
      "Epoch 957/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2203 - accuracy: 0.9287 - val_loss: 0.9454 - val_accuracy: 0.7025\n",
      "Epoch 958/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2222 - accuracy: 0.9275 - val_loss: 0.9399 - val_accuracy: 0.6938\n",
      "Epoch 959/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2158 - accuracy: 0.9263 - val_loss: 0.9156 - val_accuracy: 0.6877\n",
      "Epoch 960/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2173 - accuracy: 0.9245 - val_loss: 0.9305 - val_accuracy: 0.7012\n",
      "Epoch 961/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.1988 - accuracy: 0.9361 - val_loss: 0.9272 - val_accuracy: 0.7074\n",
      "Epoch 962/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.1969 - accuracy: 0.9385 - val_loss: 0.9032 - val_accuracy: 0.7111\n",
      "Epoch 963/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.1971 - accuracy: 0.9415 - val_loss: 0.9549 - val_accuracy: 0.7086\n",
      "Epoch 964/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2303 - accuracy: 0.9263 - val_loss: 0.9528 - val_accuracy: 0.6963\n",
      "Epoch 965/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.1973 - accuracy: 0.9336 - val_loss: 0.9142 - val_accuracy: 0.7074\n",
      "Epoch 966/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.1904 - accuracy: 0.9440 - val_loss: 0.9422 - val_accuracy: 0.7049\n",
      "Epoch 967/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.1984 - accuracy: 0.9324 - val_loss: 0.9429 - val_accuracy: 0.7037\n",
      "Epoch 968/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.1999 - accuracy: 0.9385 - val_loss: 0.9326 - val_accuracy: 0.6951\n",
      "Epoch 969/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.1976 - accuracy: 0.9330 - val_loss: 0.9270 - val_accuracy: 0.7185\n",
      "Epoch 970/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2021 - accuracy: 0.9300 - val_loss: 0.9131 - val_accuracy: 0.7074\n",
      "Epoch 971/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2105 - accuracy: 0.9257 - val_loss: 0.9714 - val_accuracy: 0.6938\n",
      "Epoch 972/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2165 - accuracy: 0.9220 - val_loss: 0.9839 - val_accuracy: 0.7000\n",
      "Epoch 973/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2030 - accuracy: 0.9373 - val_loss: 0.9771 - val_accuracy: 0.6914\n",
      "Epoch 974/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2157 - accuracy: 0.9233 - val_loss: 0.9284 - val_accuracy: 0.7062\n",
      "Epoch 975/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2108 - accuracy: 0.9324 - val_loss: 0.9286 - val_accuracy: 0.7012\n",
      "Epoch 976/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2187 - accuracy: 0.9239 - val_loss: 0.9426 - val_accuracy: 0.7049\n",
      "Epoch 977/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2024 - accuracy: 0.9306 - val_loss: 0.9396 - val_accuracy: 0.7049\n",
      "Epoch 978/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2068 - accuracy: 0.9318 - val_loss: 0.9071 - val_accuracy: 0.7160\n",
      "Epoch 979/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2206 - accuracy: 0.9220 - val_loss: 0.9134 - val_accuracy: 0.7222\n",
      "Epoch 980/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2102 - accuracy: 0.9318 - val_loss: 0.9342 - val_accuracy: 0.7185\n",
      "Epoch 981/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.1958 - accuracy: 0.9354 - val_loss: 0.9719 - val_accuracy: 0.7012\n",
      "Epoch 982/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2053 - accuracy: 0.9330 - val_loss: 0.9206 - val_accuracy: 0.6938\n",
      "Epoch 983/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.1948 - accuracy: 0.9330 - val_loss: 0.9489 - val_accuracy: 0.7012\n",
      "Epoch 984/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.1881 - accuracy: 0.9391 - val_loss: 0.9182 - val_accuracy: 0.7037\n",
      "Epoch 985/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.1981 - accuracy: 0.9379 - val_loss: 0.9502 - val_accuracy: 0.7037\n",
      "Epoch 986/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2089 - accuracy: 0.9342 - val_loss: 1.0011 - val_accuracy: 0.6840\n",
      "Epoch 987/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.1977 - accuracy: 0.9312 - val_loss: 0.9441 - val_accuracy: 0.7037\n",
      "Epoch 988/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.1869 - accuracy: 0.9348 - val_loss: 0.9280 - val_accuracy: 0.7173\n",
      "Epoch 989/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2036 - accuracy: 0.9318 - val_loss: 0.9340 - val_accuracy: 0.6975\n",
      "Epoch 990/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2221 - accuracy: 0.9220 - val_loss: 0.9444 - val_accuracy: 0.6988\n",
      "Epoch 991/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2010 - accuracy: 0.9348 - val_loss: 0.9144 - val_accuracy: 0.7037\n",
      "Epoch 992/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.1840 - accuracy: 0.9409 - val_loss: 0.9567 - val_accuracy: 0.6914\n",
      "Epoch 993/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.1948 - accuracy: 0.9361 - val_loss: 0.9277 - val_accuracy: 0.6975\n",
      "Epoch 994/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.1937 - accuracy: 0.9415 - val_loss: 0.9529 - val_accuracy: 0.7025\n",
      "Epoch 995/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.1937 - accuracy: 0.9397 - val_loss: 0.9230 - val_accuracy: 0.7136\n",
      "Epoch 996/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2044 - accuracy: 0.9324 - val_loss: 0.9288 - val_accuracy: 0.7037\n",
      "Epoch 997/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.1995 - accuracy: 0.9354 - val_loss: 0.9180 - val_accuracy: 0.7074\n",
      "Epoch 998/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.1766 - accuracy: 0.9440 - val_loss: 0.9530 - val_accuracy: 0.7123\n",
      "Epoch 999/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.1884 - accuracy: 0.9306 - val_loss: 0.9344 - val_accuracy: 0.6975\n",
      "Epoch 1000/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.1832 - accuracy: 0.9403 - val_loss: 0.9508 - val_accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFytY6LDzgJ0"
   },
   "source": [
    "Let's plot the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "TFz4ClZov9gZ",
    "outputId": "e3fdf6e2-f249-4b36-a063-683c88ab705f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vf1W7LgP2DA5"
   },
   "source": [
    "\n",
    "\n",
    "And now let's plot the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "8yyFBt7ASPUe",
    "outputId": "d149ff38-7f2f-4eb4-d62e-08d8683ede2d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['accuracy'])\n",
    "plt.plot(cnnhistory.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gaZONl1mD8XD"
   },
   "source": [
    "Let's now create a classification report to review the f1-score of the model per class.\n",
    "To do so, we have to:\n",
    "- Create a variable predictions that will contain the model.predict_classes outcome\n",
    "- Convert our y_test (array of strings with our classes) to an array of int called new_Ytest, otherwise it will not be comparable to the predictions by the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EO25uIL-9vqx"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_testcnn)\n",
    "predictions=np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1i06grlBBSrn",
    "outputId": "af34893b-827c-4355-a92a-17b53b80ad3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 1, 1, 0, 0, 4, 4, 4, 3, 6, 4, 2, 1, 2, 4, 4, 5, 1, 6, 4,\n",
       "       5, 1, 6, 2, 5, 6, 7, 7, 4, 5, 3, 4, 0, 4, 1, 3, 4, 5, 3, 3, 2, 2,\n",
       "       2, 4, 5, 3, 6, 3, 0, 4, 6, 5, 6, 4, 5, 0, 5, 2, 1, 3, 5, 3, 3, 3,\n",
       "       2, 4, 0, 3, 4, 2, 4, 3, 5, 2, 2, 0, 5, 3, 6, 3, 3, 1, 4, 4, 0, 4,\n",
       "       5, 5, 1, 3, 4, 2, 2, 0, 1, 3, 4, 7, 6, 1, 1, 7, 3, 2, 7, 2, 5, 4,\n",
       "       7, 4, 4, 5, 1, 1, 0, 1, 7, 1, 2, 1, 6, 0, 2, 2, 5, 7, 2, 3, 5, 0,\n",
       "       6, 7, 0, 5, 6, 7, 7, 6, 3, 1, 5, 6, 4, 4, 5, 2, 2, 4, 6, 7, 6, 0,\n",
       "       6, 5, 3, 4, 5, 4, 4, 1, 4, 5, 1, 1, 6, 5, 5, 1, 5, 2, 1, 3, 1, 5,\n",
       "       1, 7, 7, 2, 3, 1, 1, 2, 3, 4, 1, 4, 7, 1, 3, 5, 5, 1, 0, 5, 1, 0,\n",
       "       4, 3, 1, 4, 3, 0, 7, 4, 0, 1, 5, 4, 7, 3, 3, 3, 5, 0, 2, 1, 1, 6,\n",
       "       6, 3, 5, 5, 4, 2, 1, 1, 2, 4, 3, 5, 5, 4, 2, 5, 4, 5, 6, 3, 3, 7,\n",
       "       1, 2, 3, 0, 4, 3, 3, 3, 5, 5, 2, 4, 7, 4, 6, 2, 3, 0, 3, 0, 2, 3,\n",
       "       2, 7, 2, 1, 7, 7, 1, 6, 4, 2, 1, 2, 0, 0, 2, 1, 4, 7, 3, 2, 6, 5,\n",
       "       2, 3, 3, 0, 5, 0, 2, 1, 5, 6, 6, 3, 2, 4, 7, 0, 0, 1, 6, 3, 7, 5,\n",
       "       1, 6, 2, 0, 0, 1, 4, 2, 3, 2, 6, 3, 5, 5, 3, 1, 4, 7, 6, 1, 4, 4,\n",
       "       2, 5, 2, 2, 5, 1, 1, 4, 5, 3, 0, 1, 4, 2, 1, 7, 5, 2, 1, 5, 2, 7,\n",
       "       1, 2, 3, 5, 1, 7, 2, 6, 0, 4, 2, 6, 4, 6, 6, 1, 3, 2, 5, 1, 2, 2,\n",
       "       3, 1, 3, 3, 3, 1, 4, 4, 1, 3, 2, 0, 7, 3, 7, 4, 2, 4, 4, 4, 7, 2,\n",
       "       0, 2, 7, 1, 2, 4, 3, 7, 3, 7, 5, 5, 1, 5, 2, 5, 3, 5, 0, 5, 4, 4,\n",
       "       1, 6, 5, 7, 3, 0, 1, 1, 3, 0, 1, 4, 4, 7, 0, 3, 4, 4, 3, 6, 2, 4,\n",
       "       5, 0, 4, 2, 7, 2, 3, 5, 5, 1, 1, 6, 7, 3, 1, 1, 4, 1, 5, 3, 4, 4,\n",
       "       5, 6, 4, 6, 4, 2, 2, 0, 4, 3, 3, 7, 1, 7, 6, 7, 3, 6, 7, 3, 5, 0,\n",
       "       5, 0, 2, 4, 5, 1, 2, 3, 6, 2, 0, 0, 3, 7, 3, 5, 6, 5, 7, 2, 7, 5,\n",
       "       7, 3, 1, 3, 5, 5, 5, 2, 1, 3, 1, 4, 5, 3, 1, 1, 5, 5, 4, 7, 5, 7,\n",
       "       4, 4, 1, 4, 1, 4, 5, 2, 7, 2, 6, 0, 3, 2, 0, 7, 3, 5, 1, 2, 5, 2,\n",
       "       2, 5, 6, 5, 1, 1, 3, 5, 2, 2, 3, 4, 4, 1, 4, 3, 7, 0, 0, 5, 2, 0,\n",
       "       5, 3, 5, 7, 5, 2, 2, 0, 2, 5, 0, 0, 2, 5, 3, 5, 2, 5, 4, 4, 7, 0,\n",
       "       2, 4, 0, 3, 2, 0, 2, 5, 4, 4, 5, 5, 0, 4, 0, 6, 1, 5, 4, 7, 4, 3,\n",
       "       7, 4, 5, 5, 2, 0, 0, 1, 5, 5, 0, 2, 3, 2, 1, 1, 3, 4, 0, 6, 2, 6,\n",
       "       7, 3, 6, 4, 2, 1, 5, 3, 5, 0, 1, 2, 3, 3, 4, 1, 1, 4, 7, 2, 0, 7,\n",
       "       2, 1, 5, 1, 6, 7, 1, 3, 6, 5, 1, 0, 1, 2, 3, 5, 3, 3, 6, 1, 3, 4,\n",
       "       4, 3, 4, 5, 2, 6, 5, 1, 4, 3, 1, 3, 3, 4, 3, 1, 1, 2, 5, 7, 1, 3,\n",
       "       3, 6, 4, 4, 5, 2, 4, 1, 1, 0, 7, 3, 2, 0, 2, 3, 6, 3, 2, 1, 5, 5,\n",
       "       5, 5, 2, 1, 2, 5, 3, 5, 4, 5, 0, 0, 1, 5, 5, 0, 1, 0, 2, 6, 0, 5,\n",
       "       3, 2, 5, 7, 2, 1, 7, 6, 4, 5, 5, 2, 4, 4, 3, 4, 1, 3, 1, 5, 2, 0,\n",
       "       3, 5, 2, 5, 5, 6, 0, 5, 6, 6, 5, 1, 2, 4, 5, 5, 6, 4, 5, 2, 6, 4,\n",
       "       5, 2, 0, 1, 5, 3, 5, 4, 2, 0, 1, 4, 5, 6, 2, 7, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HUHshx93CM_6",
    "outputId": "5b33758e-9a1a-403d-9679-19c0e6a2c0e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 1, 1, 1, 1, 2, 4, 4, 0, 6, 5, 2, 1, 3, 2, 4, 5, 3, 6, 4,\n",
       "       0, 1, 6, 4, 2, 6, 7, 7, 4, 5, 3, 7, 4, 4, 1, 3, 4, 3, 3, 5, 2, 2,\n",
       "       7, 4, 5, 3, 6, 3, 1, 4, 6, 3, 1, 4, 5, 0, 5, 2, 1, 3, 5, 3, 1, 3,\n",
       "       2, 4, 6, 3, 4, 2, 6, 5, 5, 2, 2, 0, 5, 3, 7, 3, 3, 1, 4, 4, 0, 4,\n",
       "       3, 5, 1, 5, 4, 2, 2, 1, 3, 5, 4, 7, 6, 2, 1, 7, 3, 2, 7, 2, 3, 4,\n",
       "       7, 4, 4, 5, 1, 1, 0, 1, 7, 1, 2, 1, 6, 0, 2, 2, 3, 2, 2, 3, 5, 0,\n",
       "       4, 6, 0, 7, 3, 7, 7, 1, 5, 1, 2, 7, 4, 4, 5, 2, 2, 4, 6, 7, 7, 2,\n",
       "       6, 5, 3, 6, 2, 4, 4, 1, 4, 4, 3, 1, 6, 0, 5, 1, 5, 2, 1, 5, 1, 2,\n",
       "       1, 7, 2, 2, 6, 1, 2, 2, 1, 1, 1, 4, 7, 1, 3, 3, 5, 3, 0, 5, 1, 0,\n",
       "       4, 1, 1, 4, 3, 3, 7, 4, 3, 1, 3, 4, 7, 0, 5, 3, 5, 0, 2, 1, 1, 2,\n",
       "       6, 1, 5, 3, 4, 2, 3, 0, 2, 4, 3, 5, 5, 4, 2, 7, 4, 5, 3, 3, 0, 7,\n",
       "       1, 5, 3, 0, 4, 3, 3, 3, 2, 5, 2, 4, 2, 4, 3, 2, 3, 0, 3, 4, 5, 3,\n",
       "       1, 3, 1, 5, 2, 7, 1, 7, 4, 1, 1, 2, 0, 0, 2, 1, 4, 5, 5, 2, 6, 5,\n",
       "       5, 3, 3, 0, 4, 0, 2, 1, 5, 6, 6, 5, 1, 4, 2, 0, 0, 1, 6, 3, 7, 0,\n",
       "       1, 7, 2, 1, 2, 1, 6, 1, 3, 2, 6, 3, 5, 5, 3, 3, 4, 7, 7, 1, 4, 5,\n",
       "       2, 5, 6, 2, 4, 1, 1, 4, 5, 3, 0, 1, 4, 2, 1, 7, 5, 2, 1, 5, 2, 7,\n",
       "       1, 2, 3, 5, 1, 7, 2, 4, 0, 4, 3, 6, 4, 6, 6, 1, 3, 1, 5, 1, 2, 2,\n",
       "       3, 2, 3, 3, 3, 1, 4, 4, 1, 2, 2, 0, 7, 5, 4, 4, 2, 4, 4, 4, 7, 4,\n",
       "       1, 2, 7, 2, 2, 3, 7, 7, 3, 4, 5, 5, 1, 5, 3, 5, 3, 4, 1, 5, 4, 4,\n",
       "       3, 6, 5, 7, 3, 0, 1, 0, 3, 0, 1, 4, 4, 7, 0, 1, 3, 2, 3, 6, 2, 4,\n",
       "       2, 6, 4, 2, 7, 2, 3, 5, 1, 1, 1, 7, 5, 0, 1, 1, 4, 1, 5, 3, 4, 4,\n",
       "       5, 6, 4, 6, 4, 2, 2, 0, 5, 3, 5, 7, 3, 7, 4, 3, 5, 6, 6, 5, 5, 0,\n",
       "       5, 6, 4, 4, 3, 1, 2, 3, 6, 2, 0, 1, 3, 4, 3, 5, 7, 5, 3, 2, 2, 5,\n",
       "       7, 0, 1, 1, 2, 3, 6, 2, 1, 3, 1, 4, 5, 3, 1, 1, 5, 7, 5, 7, 4, 7,\n",
       "       5, 4, 1, 4, 1, 6, 5, 2, 3, 2, 6, 0, 3, 2, 4, 7, 5, 2, 1, 2, 5, 2,\n",
       "       2, 2, 0, 5, 1, 1, 3, 6, 5, 2, 3, 2, 4, 2, 4, 1, 7, 3, 7, 5, 1, 1,\n",
       "       2, 3, 3, 3, 5, 2, 2, 1, 2, 5, 6, 0, 6, 5, 3, 4, 7, 5, 4, 2, 4, 1,\n",
       "       4, 4, 2, 3, 2, 2, 2, 3, 4, 4, 4, 5, 0, 3, 6, 6, 1, 5, 2, 7, 4, 3,\n",
       "       7, 4, 5, 5, 2, 0, 0, 1, 5, 5, 4, 3, 5, 2, 1, 1, 3, 4, 7, 6, 2, 6,\n",
       "       7, 3, 6, 4, 2, 1, 5, 5, 6, 0, 1, 5, 3, 4, 4, 1, 1, 4, 4, 4, 3, 7,\n",
       "       2, 1, 5, 1, 6, 7, 1, 5, 1, 5, 1, 0, 1, 2, 3, 3, 1, 3, 4, 1, 3, 4,\n",
       "       5, 3, 6, 6, 6, 0, 5, 1, 4, 3, 1, 1, 3, 4, 3, 1, 6, 2, 3, 7, 1, 3,\n",
       "       3, 2, 4, 2, 5, 2, 6, 1, 1, 0, 7, 6, 2, 0, 2, 3, 7, 3, 5, 1, 5, 5,\n",
       "       5, 6, 5, 1, 2, 5, 3, 4, 4, 5, 0, 0, 1, 5, 5, 0, 2, 0, 7, 6, 2, 5,\n",
       "       5, 4, 5, 7, 5, 1, 7, 6, 4, 5, 5, 5, 4, 4, 3, 4, 1, 5, 1, 5, 2, 0,\n",
       "       3, 3, 2, 5, 4, 6, 0, 2, 6, 6, 5, 1, 2, 4, 2, 3, 1, 5, 5, 2, 1, 4,\n",
       "       5, 7, 0, 1, 2, 3, 5, 4, 2, 0, 6, 4, 5, 0, 2, 7, 3, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMxojpvWCxOs"
   },
   "outputs": [],
   "source": [
    "new_Ytest = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W07EQaC8DE6i",
    "outputId": "9e7d7f0f-8cd3-4068-e42e-4e23c22f8a71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 1, 1, 1, 1, 2, 4, 4, 0, 6, 5, 2, 1, 3, 2, 4, 5, 3, 6, 4,\n",
       "       0, 1, 6, 4, 2, 6, 7, 7, 4, 5, 3, 7, 4, 4, 1, 3, 4, 3, 3, 5, 2, 2,\n",
       "       7, 4, 5, 3, 6, 3, 1, 4, 6, 3, 1, 4, 5, 0, 5, 2, 1, 3, 5, 3, 1, 3,\n",
       "       2, 4, 6, 3, 4, 2, 6, 5, 5, 2, 2, 0, 5, 3, 7, 3, 3, 1, 4, 4, 0, 4,\n",
       "       3, 5, 1, 5, 4, 2, 2, 1, 3, 5, 4, 7, 6, 2, 1, 7, 3, 2, 7, 2, 3, 4,\n",
       "       7, 4, 4, 5, 1, 1, 0, 1, 7, 1, 2, 1, 6, 0, 2, 2, 3, 2, 2, 3, 5, 0,\n",
       "       4, 6, 0, 7, 3, 7, 7, 1, 5, 1, 2, 7, 4, 4, 5, 2, 2, 4, 6, 7, 7, 2,\n",
       "       6, 5, 3, 6, 2, 4, 4, 1, 4, 4, 3, 1, 6, 0, 5, 1, 5, 2, 1, 5, 1, 2,\n",
       "       1, 7, 2, 2, 6, 1, 2, 2, 1, 1, 1, 4, 7, 1, 3, 3, 5, 3, 0, 5, 1, 0,\n",
       "       4, 1, 1, 4, 3, 3, 7, 4, 3, 1, 3, 4, 7, 0, 5, 3, 5, 0, 2, 1, 1, 2,\n",
       "       6, 1, 5, 3, 4, 2, 3, 0, 2, 4, 3, 5, 5, 4, 2, 7, 4, 5, 3, 3, 0, 7,\n",
       "       1, 5, 3, 0, 4, 3, 3, 3, 2, 5, 2, 4, 2, 4, 3, 2, 3, 0, 3, 4, 5, 3,\n",
       "       1, 3, 1, 5, 2, 7, 1, 7, 4, 1, 1, 2, 0, 0, 2, 1, 4, 5, 5, 2, 6, 5,\n",
       "       5, 3, 3, 0, 4, 0, 2, 1, 5, 6, 6, 5, 1, 4, 2, 0, 0, 1, 6, 3, 7, 0,\n",
       "       1, 7, 2, 1, 2, 1, 6, 1, 3, 2, 6, 3, 5, 5, 3, 3, 4, 7, 7, 1, 4, 5,\n",
       "       2, 5, 6, 2, 4, 1, 1, 4, 5, 3, 0, 1, 4, 2, 1, 7, 5, 2, 1, 5, 2, 7,\n",
       "       1, 2, 3, 5, 1, 7, 2, 4, 0, 4, 3, 6, 4, 6, 6, 1, 3, 1, 5, 1, 2, 2,\n",
       "       3, 2, 3, 3, 3, 1, 4, 4, 1, 2, 2, 0, 7, 5, 4, 4, 2, 4, 4, 4, 7, 4,\n",
       "       1, 2, 7, 2, 2, 3, 7, 7, 3, 4, 5, 5, 1, 5, 3, 5, 3, 4, 1, 5, 4, 4,\n",
       "       3, 6, 5, 7, 3, 0, 1, 0, 3, 0, 1, 4, 4, 7, 0, 1, 3, 2, 3, 6, 2, 4,\n",
       "       2, 6, 4, 2, 7, 2, 3, 5, 1, 1, 1, 7, 5, 0, 1, 1, 4, 1, 5, 3, 4, 4,\n",
       "       5, 6, 4, 6, 4, 2, 2, 0, 5, 3, 5, 7, 3, 7, 4, 3, 5, 6, 6, 5, 5, 0,\n",
       "       5, 6, 4, 4, 3, 1, 2, 3, 6, 2, 0, 1, 3, 4, 3, 5, 7, 5, 3, 2, 2, 5,\n",
       "       7, 0, 1, 1, 2, 3, 6, 2, 1, 3, 1, 4, 5, 3, 1, 1, 5, 7, 5, 7, 4, 7,\n",
       "       5, 4, 1, 4, 1, 6, 5, 2, 3, 2, 6, 0, 3, 2, 4, 7, 5, 2, 1, 2, 5, 2,\n",
       "       2, 2, 0, 5, 1, 1, 3, 6, 5, 2, 3, 2, 4, 2, 4, 1, 7, 3, 7, 5, 1, 1,\n",
       "       2, 3, 3, 3, 5, 2, 2, 1, 2, 5, 6, 0, 6, 5, 3, 4, 7, 5, 4, 2, 4, 1,\n",
       "       4, 4, 2, 3, 2, 2, 2, 3, 4, 4, 4, 5, 0, 3, 6, 6, 1, 5, 2, 7, 4, 3,\n",
       "       7, 4, 5, 5, 2, 0, 0, 1, 5, 5, 4, 3, 5, 2, 1, 1, 3, 4, 7, 6, 2, 6,\n",
       "       7, 3, 6, 4, 2, 1, 5, 5, 6, 0, 1, 5, 3, 4, 4, 1, 1, 4, 4, 4, 3, 7,\n",
       "       2, 1, 5, 1, 6, 7, 1, 5, 1, 5, 1, 0, 1, 2, 3, 3, 1, 3, 4, 1, 3, 4,\n",
       "       5, 3, 6, 6, 6, 0, 5, 1, 4, 3, 1, 1, 3, 4, 3, 1, 6, 2, 3, 7, 1, 3,\n",
       "       3, 2, 4, 2, 5, 2, 6, 1, 1, 0, 7, 6, 2, 0, 2, 3, 7, 3, 5, 1, 5, 5,\n",
       "       5, 6, 5, 1, 2, 5, 3, 4, 4, 5, 0, 0, 1, 5, 5, 0, 2, 0, 7, 6, 2, 5,\n",
       "       5, 4, 5, 7, 5, 1, 7, 6, 4, 5, 5, 5, 4, 4, 3, 4, 1, 5, 1, 5, 2, 0,\n",
       "       3, 3, 2, 5, 4, 6, 0, 2, 6, 6, 5, 1, 2, 4, 2, 3, 1, 5, 5, 2, 1, 4,\n",
       "       5, 7, 0, 1, 2, 3, 5, 4, 2, 0, 6, 4, 5, 0, 2, 7, 3, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hu1S5IowfSDG"
   },
   "source": [
    "Now, the confusion matrix: it will show us the misclassified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "fdy09SCEd7Cl",
    "outputId": "4fd020f5-74c5-40e7-8b54-076c944901be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44  2  0  5  0  3  3  0]\n",
      " [11 96  7  9  1  1  5  0]\n",
      " [ 5  6 86  1  7 13  2  6]\n",
      " [ 4  8  4 80  3 16  3  5]\n",
      " [ 4  0  6  1 93  9  4  5]\n",
      " [ 0  1  9 19  7 86  0  2]\n",
      " [ 6  2  3  2  6  5 37  2]\n",
      " [ 2  0  4  1  1  3  9 45]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(new_Ytest, predictions)\n",
    "print (matrix)\n",
    "\n",
    "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.77      0.66        57\n",
      "           1       0.83      0.74      0.78       130\n",
      "           2       0.72      0.68      0.70       126\n",
      "           3       0.68      0.65      0.66       123\n",
      "           4       0.79      0.76      0.78       122\n",
      "           5       0.63      0.69      0.66       124\n",
      "           6       0.59      0.59      0.59        63\n",
      "           7       0.69      0.69      0.69        65\n",
      "\n",
      "    accuracy                           0.70       810\n",
      "   macro avg       0.69      0.70      0.69       810\n",
      "weighted avg       0.71      0.70      0.70       810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_ySPOyHxkZ3"
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f5kRmoD-sdHj",
    "outputId": "99ad6a5b-a4a6-42bc-ed78-229864c33d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at ../model/Emotion_Voice_Detection_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = '../model/'\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EmotionsRecognition.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "43a24ecce625020f2d6631fb4cfb730bba30d877e9fe9ec2e0d85cb5a52a2b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
