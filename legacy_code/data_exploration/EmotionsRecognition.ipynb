{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjWvnaQUrZmD"
   },
   "source": [
    "# Emotion classification using the RAVDESS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDNbxj45rkvB"
   },
   "source": [
    "# Analysis\n",
    "\n",
    "We are will first install LibROSA, a python package for music and audio analysis.\n",
    "\n",
    "After the import, we will plot the signal of the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "EgFwaDhMbJVm",
    "outputId": "d1f5d32b-177d-4858-c366-d43a6b8783ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.9.2)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.0.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.7.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (0.3.1)\n",
      "Requirement already satisfied: numba>=0.45.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (0.55.2)\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from numba>=0.45.1->librosa) (0.38.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from numba>=0.45.1->librosa) (62.3.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from packaging>=20.0->librosa) (3.0.9)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pooch>=1.0->librosa) (2.27.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxI4xzngdS-e"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "from librosa import display\n",
    "\n",
    "data, sampling_rate = librosa.load('../../features/Actor_01/03-01-01-01-01-01-01.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCtNuVWlr5jL"
   },
   "source": [
    "# Load all files\n",
    "\n",
    "We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AKvuF--gd6F-",
    "outputId": "4fbbbdc4-3bce-47b3-812c-1dd9e3938159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data loaded. Loading time: 836.6764433383942 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "path = '../../features/'\n",
    "lst = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "  for file in files:\n",
    "      try:\n",
    "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
    "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
    "        file = int(file[7:8]) - 1 \n",
    "        arr = mfccs, file\n",
    "        lst.append(arr)\n",
    "      # If the file is not valid, skip it\n",
    "      except ValueError:\n",
    "        continue\n",
    "\n",
    "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLSggnF7kKY1"
   },
   "outputs": [],
   "source": [
    "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
    "X, y = zip(*lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VzvBRTJIlIE9",
    "outputId": "6eb806ec-f065-4420-d526-c1fa8d00c26c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2452, 40), (2452,))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOutQiAlCjOY"
   },
   "outputs": [],
   "source": [
    "# Saving joblib files to not load them again with the loop above\n",
    "\n",
    "import joblib\n",
    "\n",
    "X_name = 'X.joblib'\n",
    "y_name = 'y.joblib'\n",
    "save_dir = '../../legacy_code/model/'\n",
    "\n",
    "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
    "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIoFdycUXMxA"
   },
   "outputs": [],
   "source": [
    "# Loading saved models\n",
    "\n",
    "X = joblib.load('../../legacy_code/model/X.joblib')\n",
    "y = joblib.load('../../legacy_code/model/y.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Agw-3KN1sDhh"
   },
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "To make a first attempt in accomplishing this classification task I chose a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-Xgb5NslTBO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UshLOC1ClWL3"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BnCR52nlXw0"
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "qWyTownblZM0",
    "outputId": "61708923-f907-442b-86d7-b3e5b2840c29"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEuw6TUQlr7C"
   },
   "outputs": [],
   "source": [
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1v0i0V7sMw7"
   },
   "source": [
    "Let's go with our classification report.\n",
    "\n",
    "Before we start, a quick reminder of the classes we are trying to predict:\n",
    "\n",
    "emotions = {\n",
    "    \"neutral\": \"0\",\n",
    "    \"calm\": \"1\",\n",
    "    \"happy\": \"2\",\n",
    "    \"sad\": \"3\",\n",
    "    \"angry\": \"4\", \n",
    "    \"fearful\": \"5\", \n",
    "    \"disgust\": \"6\", \n",
    "    \"surprised\": \"7\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "c4kNSYkAleIv",
    "outputId": "fb407f71-b7de-4a15-cee9-527a69d45c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.46      0.40        57\n",
      "           1       0.68      0.61      0.64       130\n",
      "           2       0.43      0.44      0.43       126\n",
      "           3       0.41      0.39      0.40       123\n",
      "           4       0.59      0.55      0.57       122\n",
      "           5       0.44      0.46      0.45       124\n",
      "           6       0.25      0.27      0.26        63\n",
      "           7       0.45      0.46      0.45        65\n",
      "\n",
      "    accuracy                           0.47       810\n",
      "   macro avg       0.45      0.45      0.45       810\n",
      "weighted avg       0.48      0.47      0.47       810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCVgjLj-gwE2"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jfaTxzZ1w__y"
   },
   "source": [
    "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
    "\n",
    "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcov_DCXgs7v"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eo0ljqzg-KM"
   },
   "outputs": [],
   "source": [
    "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
    "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
    "                                 n_estimators= 22000, random_state= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Tg45qSOfg-26",
    "outputId": "c31df1c1-9342-4485-b7f8-211cc4077e7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10, max_features=&#x27;log2&#x27;, max_leaf_nodes=100,\n",
       "                       min_samples_leaf=3, min_samples_split=20,\n",
       "                       n_estimators=22000, random_state=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, max_features=&#x27;log2&#x27;, max_leaf_nodes=100,\n",
       "                       min_samples_leaf=3, min_samples_split=20,\n",
       "                       n_estimators=22000, random_state=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
       "                       min_samples_leaf=3, min_samples_split=20,\n",
       "                       n_estimators=22000, random_state=5)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aM8KU3qxhGBM"
   },
   "outputs": [],
   "source": [
    "predictions = rforest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "296FW5sBdanI",
    "outputId": "a9fbfcc2-f9c5-4a3d-9bc0-2a6f513ba609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.44      0.60        57\n",
      "           1       0.64      0.88      0.74       130\n",
      "           2       0.72      0.51      0.60       126\n",
      "           3       0.55      0.63      0.59       123\n",
      "           4       0.68      0.77      0.72       122\n",
      "           5       0.58      0.54      0.56       124\n",
      "           6       0.47      0.30      0.37        63\n",
      "           7       0.46      0.57      0.51        65\n",
      "\n",
      "    accuracy                           0.62       810\n",
      "   macro avg       0.63      0.58      0.59       810\n",
      "weighted avg       0.63      0.62      0.61       810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9eqMHV3S8i6"
   },
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-QscoyMxQtn"
   },
   "source": [
    "Let's build our neural network!\n",
    "\n",
    "To do so, we need to expand the dimensions of our array, adding a third one using the numpy \"expand_dims\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4i187-Pe-w5"
   },
   "outputs": [],
   "source": [
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vnvoCRX1gQCh",
    "outputId": "cc9d5f67-0c48-443c-e6b1-6798d200529e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1642, 40, 1), (810, 40, 1))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_traincnn.shape, x_testcnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "HZOGIpuefCd3",
    "outputId": "4fe2802a-3147-4725-d79b-3c9cbe993e16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(40,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LphftMIZzUvz"
   },
   "source": [
    "With *model.summary* we can see a recap of what we have build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "pIWPB4Zgfic7",
    "outputId": "49b5d344-637a-452e-c730-76f5471ea889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 40, 128)           768       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 5128      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 87,944\n",
      "Trainable params: 87,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qQSBeBhzcLu"
   },
   "source": [
    "Now we can compile and fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "iNI1znbsfpTx",
    "outputId": "872a1dbe-5206-4d6c-a7ce-943b585f4a9e"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ktdF-nJKfq6F",
    "outputId": "a05f0852-1564-4425-8885-bcea35dd0e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "103/103 [==============================] - 2s 9ms/step - loss: 7.4414 - accuracy: 0.1370 - val_loss: 2.3340 - val_accuracy: 0.1765\n",
      "Epoch 2/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 6.1774 - accuracy: 0.1468 - val_loss: 2.6674 - val_accuracy: 0.1568\n",
      "Epoch 3/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 5.3499 - accuracy: 0.1535 - val_loss: 2.2466 - val_accuracy: 0.2407\n",
      "Epoch 4/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 4.8967 - accuracy: 0.1492 - val_loss: 2.0228 - val_accuracy: 0.2494\n",
      "Epoch 5/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 4.2203 - accuracy: 0.1577 - val_loss: 2.6446 - val_accuracy: 0.1815\n",
      "Epoch 6/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 3.7436 - accuracy: 0.1784 - val_loss: 2.4542 - val_accuracy: 0.2086\n",
      "Epoch 7/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 3.3514 - accuracy: 0.1943 - val_loss: 1.9579 - val_accuracy: 0.2358\n",
      "Epoch 8/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 2.9810 - accuracy: 0.1876 - val_loss: 1.8829 - val_accuracy: 0.2790\n",
      "Epoch 9/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 2.7432 - accuracy: 0.2016 - val_loss: 1.8711 - val_accuracy: 0.2543\n",
      "Epoch 10/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 2.4804 - accuracy: 0.2052 - val_loss: 1.8975 - val_accuracy: 0.2333\n",
      "Epoch 11/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 2.3221 - accuracy: 0.2339 - val_loss: 1.7890 - val_accuracy: 0.2802\n",
      "Epoch 12/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 2.1961 - accuracy: 0.2296 - val_loss: 1.8506 - val_accuracy: 0.2580\n",
      "Epoch 13/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 2.0707 - accuracy: 0.2625 - val_loss: 1.7987 - val_accuracy: 0.2358\n",
      "Epoch 14/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 2.0001 - accuracy: 0.2540 - val_loss: 1.7818 - val_accuracy: 0.3309\n",
      "Epoch 15/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.9425 - accuracy: 0.2649 - val_loss: 1.7893 - val_accuracy: 0.2951\n",
      "Epoch 16/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 1.9449 - accuracy: 0.2600 - val_loss: 1.7615 - val_accuracy: 0.3383\n",
      "Epoch 17/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.9022 - accuracy: 0.2838 - val_loss: 1.7899 - val_accuracy: 0.2914\n",
      "Epoch 18/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.8826 - accuracy: 0.2801 - val_loss: 1.7409 - val_accuracy: 0.3852\n",
      "Epoch 19/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.8622 - accuracy: 0.2814 - val_loss: 1.7453 - val_accuracy: 0.3395\n",
      "Epoch 20/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.8130 - accuracy: 0.2978 - val_loss: 1.7604 - val_accuracy: 0.3358\n",
      "Epoch 21/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.8137 - accuracy: 0.2929 - val_loss: 1.6968 - val_accuracy: 0.3951\n",
      "Epoch 22/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.7940 - accuracy: 0.3130 - val_loss: 1.7059 - val_accuracy: 0.3346\n",
      "Epoch 23/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.7550 - accuracy: 0.3222 - val_loss: 1.6665 - val_accuracy: 0.3864\n",
      "Epoch 24/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.7523 - accuracy: 0.3484 - val_loss: 1.6992 - val_accuracy: 0.3309\n",
      "Epoch 25/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.7519 - accuracy: 0.3301 - val_loss: 1.6401 - val_accuracy: 0.4012\n",
      "Epoch 26/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.7420 - accuracy: 0.3264 - val_loss: 1.6399 - val_accuracy: 0.4111\n",
      "Epoch 27/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.7052 - accuracy: 0.3599 - val_loss: 1.6152 - val_accuracy: 0.4247\n",
      "Epoch 28/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.6899 - accuracy: 0.3435 - val_loss: 1.6223 - val_accuracy: 0.4012\n",
      "Epoch 29/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.6800 - accuracy: 0.3526 - val_loss: 1.6292 - val_accuracy: 0.4148\n",
      "Epoch 30/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.6646 - accuracy: 0.3770 - val_loss: 1.6102 - val_accuracy: 0.3975\n",
      "Epoch 31/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.6705 - accuracy: 0.3532 - val_loss: 1.6008 - val_accuracy: 0.4123\n",
      "Epoch 32/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.6541 - accuracy: 0.3752 - val_loss: 1.5702 - val_accuracy: 0.4395\n",
      "Epoch 33/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.6573 - accuracy: 0.3782 - val_loss: 1.5557 - val_accuracy: 0.4321\n",
      "Epoch 34/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.6565 - accuracy: 0.3776 - val_loss: 1.5731 - val_accuracy: 0.4296\n",
      "Epoch 35/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.6181 - accuracy: 0.3910 - val_loss: 1.5421 - val_accuracy: 0.4444\n",
      "Epoch 36/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.6058 - accuracy: 0.4032 - val_loss: 1.5876 - val_accuracy: 0.4000\n",
      "Epoch 37/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.6024 - accuracy: 0.4086 - val_loss: 1.5252 - val_accuracy: 0.4407\n",
      "Epoch 38/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.5920 - accuracy: 0.3910 - val_loss: 1.5234 - val_accuracy: 0.4370\n",
      "Epoch 39/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.5718 - accuracy: 0.4123 - val_loss: 1.5161 - val_accuracy: 0.4383\n",
      "Epoch 40/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.5778 - accuracy: 0.4166 - val_loss: 1.5169 - val_accuracy: 0.4457\n",
      "Epoch 41/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.5622 - accuracy: 0.4245 - val_loss: 1.5310 - val_accuracy: 0.4222\n",
      "Epoch 42/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.5645 - accuracy: 0.4239 - val_loss: 1.5018 - val_accuracy: 0.4506\n",
      "Epoch 43/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.5283 - accuracy: 0.4415 - val_loss: 1.5182 - val_accuracy: 0.4210\n",
      "Epoch 44/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.5416 - accuracy: 0.4312 - val_loss: 1.4815 - val_accuracy: 0.4605\n",
      "Epoch 45/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.5184 - accuracy: 0.4336 - val_loss: 1.4689 - val_accuracy: 0.4679\n",
      "Epoch 46/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.5097 - accuracy: 0.4403 - val_loss: 1.4804 - val_accuracy: 0.4568\n",
      "Epoch 47/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.5001 - accuracy: 0.4434 - val_loss: 1.4449 - val_accuracy: 0.4556\n",
      "Epoch 48/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.4961 - accuracy: 0.4495 - val_loss: 1.4415 - val_accuracy: 0.4630\n",
      "Epoch 49/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.5208 - accuracy: 0.4464 - val_loss: 1.4385 - val_accuracy: 0.4728\n",
      "Epoch 50/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.4855 - accuracy: 0.4415 - val_loss: 1.4188 - val_accuracy: 0.4815\n",
      "Epoch 51/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.4804 - accuracy: 0.4598 - val_loss: 1.4358 - val_accuracy: 0.4765\n",
      "Epoch 52/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.4883 - accuracy: 0.4549 - val_loss: 1.4392 - val_accuracy: 0.4654\n",
      "Epoch 53/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.4511 - accuracy: 0.4519 - val_loss: 1.4240 - val_accuracy: 0.4877\n",
      "Epoch 54/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.4613 - accuracy: 0.4568 - val_loss: 1.4215 - val_accuracy: 0.4790\n",
      "Epoch 55/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.4597 - accuracy: 0.4653 - val_loss: 1.4067 - val_accuracy: 0.4753\n",
      "Epoch 56/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.4670 - accuracy: 0.4677 - val_loss: 1.4275 - val_accuracy: 0.4617\n",
      "Epoch 57/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.4252 - accuracy: 0.4762 - val_loss: 1.3963 - val_accuracy: 0.4988\n",
      "Epoch 58/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.4431 - accuracy: 0.4665 - val_loss: 1.3859 - val_accuracy: 0.5037\n",
      "Epoch 59/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.4183 - accuracy: 0.4842 - val_loss: 1.3766 - val_accuracy: 0.4963\n",
      "Epoch 60/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.4155 - accuracy: 0.4781 - val_loss: 1.3736 - val_accuracy: 0.5037\n",
      "Epoch 61/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.4090 - accuracy: 0.4836 - val_loss: 1.3673 - val_accuracy: 0.5062\n",
      "Epoch 62/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.4057 - accuracy: 0.4781 - val_loss: 1.3694 - val_accuracy: 0.4901\n",
      "Epoch 63/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.3938 - accuracy: 0.4823 - val_loss: 1.3532 - val_accuracy: 0.5210\n",
      "Epoch 64/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.3943 - accuracy: 0.4903 - val_loss: 1.3730 - val_accuracy: 0.5000\n",
      "Epoch 65/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.3905 - accuracy: 0.4842 - val_loss: 1.3590 - val_accuracy: 0.5062\n",
      "Epoch 66/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.3784 - accuracy: 0.4933 - val_loss: 1.3512 - val_accuracy: 0.5198\n",
      "Epoch 67/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.3972 - accuracy: 0.4896 - val_loss: 1.3393 - val_accuracy: 0.5160\n",
      "Epoch 68/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.3706 - accuracy: 0.4945 - val_loss: 1.3722 - val_accuracy: 0.4988\n",
      "Epoch 69/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.3622 - accuracy: 0.4994 - val_loss: 1.3448 - val_accuracy: 0.5074\n",
      "Epoch 70/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.3604 - accuracy: 0.4884 - val_loss: 1.3473 - val_accuracy: 0.4938\n",
      "Epoch 71/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.3501 - accuracy: 0.5000 - val_loss: 1.3321 - val_accuracy: 0.5062\n",
      "Epoch 72/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.3587 - accuracy: 0.4994 - val_loss: 1.3301 - val_accuracy: 0.5099\n",
      "Epoch 73/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.3487 - accuracy: 0.4994 - val_loss: 1.3227 - val_accuracy: 0.5370\n",
      "Epoch 74/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.3609 - accuracy: 0.5134 - val_loss: 1.3161 - val_accuracy: 0.5284\n",
      "Epoch 75/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.3229 - accuracy: 0.5171 - val_loss: 1.3122 - val_accuracy: 0.5321\n",
      "Epoch 76/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.3350 - accuracy: 0.5037 - val_loss: 1.3302 - val_accuracy: 0.5062\n",
      "Epoch 77/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.3300 - accuracy: 0.5073 - val_loss: 1.3283 - val_accuracy: 0.5185\n",
      "Epoch 78/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.3297 - accuracy: 0.5122 - val_loss: 1.3159 - val_accuracy: 0.5198\n",
      "Epoch 79/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.3235 - accuracy: 0.5067 - val_loss: 1.3058 - val_accuracy: 0.5210\n",
      "Epoch 80/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2981 - accuracy: 0.5128 - val_loss: 1.3340 - val_accuracy: 0.5049\n",
      "Epoch 81/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.3161 - accuracy: 0.5219 - val_loss: 1.2915 - val_accuracy: 0.5358\n",
      "Epoch 82/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.3115 - accuracy: 0.5183 - val_loss: 1.2829 - val_accuracy: 0.5407\n",
      "Epoch 83/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.2891 - accuracy: 0.5359 - val_loss: 1.2906 - val_accuracy: 0.5407\n",
      "Epoch 84/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.2782 - accuracy: 0.5189 - val_loss: 1.2723 - val_accuracy: 0.5407\n",
      "Epoch 85/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2964 - accuracy: 0.5238 - val_loss: 1.2748 - val_accuracy: 0.5469\n",
      "Epoch 86/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2832 - accuracy: 0.5231 - val_loss: 1.2701 - val_accuracy: 0.5519\n",
      "Epoch 87/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2720 - accuracy: 0.5432 - val_loss: 1.2571 - val_accuracy: 0.5506\n",
      "Epoch 88/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2718 - accuracy: 0.5341 - val_loss: 1.2615 - val_accuracy: 0.5457\n",
      "Epoch 89/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2794 - accuracy: 0.5268 - val_loss: 1.2562 - val_accuracy: 0.5531\n",
      "Epoch 90/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2636 - accuracy: 0.5311 - val_loss: 1.2442 - val_accuracy: 0.5556\n",
      "Epoch 91/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.2688 - accuracy: 0.5323 - val_loss: 1.2767 - val_accuracy: 0.5383\n",
      "Epoch 92/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.2593 - accuracy: 0.5292 - val_loss: 1.2556 - val_accuracy: 0.5481\n",
      "Epoch 93/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.2494 - accuracy: 0.5390 - val_loss: 1.2470 - val_accuracy: 0.5457\n",
      "Epoch 94/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2535 - accuracy: 0.5323 - val_loss: 1.2590 - val_accuracy: 0.5617\n",
      "Epoch 95/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2462 - accuracy: 0.5438 - val_loss: 1.2523 - val_accuracy: 0.5481\n",
      "Epoch 96/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.2396 - accuracy: 0.5408 - val_loss: 1.2343 - val_accuracy: 0.5580\n",
      "Epoch 97/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2267 - accuracy: 0.5438 - val_loss: 1.2769 - val_accuracy: 0.5309\n",
      "Epoch 98/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2381 - accuracy: 0.5329 - val_loss: 1.2364 - val_accuracy: 0.5593\n",
      "Epoch 99/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2223 - accuracy: 0.5530 - val_loss: 1.2347 - val_accuracy: 0.5506\n",
      "Epoch 100/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2288 - accuracy: 0.5560 - val_loss: 1.2283 - val_accuracy: 0.5469\n",
      "Epoch 101/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.2486 - accuracy: 0.5286 - val_loss: 1.2242 - val_accuracy: 0.5654\n",
      "Epoch 102/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.2133 - accuracy: 0.5481 - val_loss: 1.2444 - val_accuracy: 0.5506\n",
      "Epoch 103/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2121 - accuracy: 0.5499 - val_loss: 1.2193 - val_accuracy: 0.5531\n",
      "Epoch 104/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.2242 - accuracy: 0.5469 - val_loss: 1.2255 - val_accuracy: 0.5531\n",
      "Epoch 105/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.2121 - accuracy: 0.5615 - val_loss: 1.2100 - val_accuracy: 0.5605\n",
      "Epoch 106/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1924 - accuracy: 0.5560 - val_loss: 1.2344 - val_accuracy: 0.5506\n",
      "Epoch 107/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1969 - accuracy: 0.5627 - val_loss: 1.2243 - val_accuracy: 0.5556\n",
      "Epoch 108/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.2015 - accuracy: 0.5652 - val_loss: 1.2151 - val_accuracy: 0.5630\n",
      "Epoch 109/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1885 - accuracy: 0.5548 - val_loss: 1.2356 - val_accuracy: 0.5407\n",
      "Epoch 110/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.1827 - accuracy: 0.5670 - val_loss: 1.2145 - val_accuracy: 0.5556\n",
      "Epoch 111/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.1996 - accuracy: 0.5530 - val_loss: 1.2265 - val_accuracy: 0.5568\n",
      "Epoch 112/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.1839 - accuracy: 0.5658 - val_loss: 1.1978 - val_accuracy: 0.5704\n",
      "Epoch 113/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.1749 - accuracy: 0.5609 - val_loss: 1.2050 - val_accuracy: 0.5642\n",
      "Epoch 114/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.1876 - accuracy: 0.5536 - val_loss: 1.1904 - val_accuracy: 0.5741\n",
      "Epoch 115/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1685 - accuracy: 0.5700 - val_loss: 1.2089 - val_accuracy: 0.5605\n",
      "Epoch 116/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1491 - accuracy: 0.5719 - val_loss: 1.2033 - val_accuracy: 0.5630\n",
      "Epoch 117/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1595 - accuracy: 0.5713 - val_loss: 1.1951 - val_accuracy: 0.5840\n",
      "Epoch 118/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1643 - accuracy: 0.5816 - val_loss: 1.1931 - val_accuracy: 0.5716\n",
      "Epoch 119/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.1675 - accuracy: 0.5706 - val_loss: 1.1807 - val_accuracy: 0.5815\n",
      "Epoch 120/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.1632 - accuracy: 0.5749 - val_loss: 1.1809 - val_accuracy: 0.5728\n",
      "Epoch 121/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.1542 - accuracy: 0.5731 - val_loss: 1.1897 - val_accuracy: 0.5654\n",
      "Epoch 122/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1645 - accuracy: 0.5621 - val_loss: 1.1857 - val_accuracy: 0.5877\n",
      "Epoch 123/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1506 - accuracy: 0.5840 - val_loss: 1.1823 - val_accuracy: 0.5778\n",
      "Epoch 124/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.1253 - accuracy: 0.5926 - val_loss: 1.2082 - val_accuracy: 0.5617\n",
      "Epoch 125/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.1392 - accuracy: 0.5780 - val_loss: 1.1739 - val_accuracy: 0.5877\n",
      "Epoch 126/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.1369 - accuracy: 0.5883 - val_loss: 1.1722 - val_accuracy: 0.5765\n",
      "Epoch 127/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.1251 - accuracy: 0.5798 - val_loss: 1.1745 - val_accuracy: 0.5691\n",
      "Epoch 128/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.1291 - accuracy: 0.5767 - val_loss: 1.1708 - val_accuracy: 0.5679\n",
      "Epoch 129/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 1.1206 - accuracy: 0.5865 - val_loss: 1.1796 - val_accuracy: 0.5642\n",
      "Epoch 130/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.1273 - accuracy: 0.5871 - val_loss: 1.1544 - val_accuracy: 0.5827\n",
      "Epoch 131/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.1063 - accuracy: 0.5907 - val_loss: 1.1668 - val_accuracy: 0.5716\n",
      "Epoch 132/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1076 - accuracy: 0.5810 - val_loss: 1.1683 - val_accuracy: 0.5790\n",
      "Epoch 133/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.1092 - accuracy: 0.5938 - val_loss: 1.1671 - val_accuracy: 0.5765\n",
      "Epoch 134/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1021 - accuracy: 0.6011 - val_loss: 1.1642 - val_accuracy: 0.5889\n",
      "Epoch 135/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0979 - accuracy: 0.5987 - val_loss: 1.1474 - val_accuracy: 0.5938\n",
      "Epoch 136/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.1015 - accuracy: 0.5993 - val_loss: 1.1605 - val_accuracy: 0.5753\n",
      "Epoch 137/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.1159 - accuracy: 0.5859 - val_loss: 1.1589 - val_accuracy: 0.5864\n",
      "Epoch 138/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.1083 - accuracy: 0.5920 - val_loss: 1.1506 - val_accuracy: 0.5889\n",
      "Epoch 139/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 1.1040 - accuracy: 0.5950 - val_loss: 1.1707 - val_accuracy: 0.5654\n",
      "Epoch 140/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.0622 - accuracy: 0.6096 - val_loss: 1.1835 - val_accuracy: 0.5593\n",
      "Epoch 141/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.0917 - accuracy: 0.6060 - val_loss: 1.1820 - val_accuracy: 0.5593\n",
      "Epoch 142/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.0884 - accuracy: 0.5993 - val_loss: 1.1565 - val_accuracy: 0.5741\n",
      "Epoch 143/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0947 - accuracy: 0.5956 - val_loss: 1.1372 - val_accuracy: 0.5877\n",
      "Epoch 144/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 1.0688 - accuracy: 0.5962 - val_loss: 1.1580 - val_accuracy: 0.5630\n",
      "Epoch 145/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0873 - accuracy: 0.6035 - val_loss: 1.1389 - val_accuracy: 0.5753\n",
      "Epoch 146/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0890 - accuracy: 0.6005 - val_loss: 1.1549 - val_accuracy: 0.5617\n",
      "Epoch 147/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0749 - accuracy: 0.6005 - val_loss: 1.1269 - val_accuracy: 0.5963\n",
      "Epoch 148/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.0592 - accuracy: 0.6175 - val_loss: 1.1297 - val_accuracy: 0.5914\n",
      "Epoch 149/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.0569 - accuracy: 0.6029 - val_loss: 1.1339 - val_accuracy: 0.5704\n",
      "Epoch 150/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.0755 - accuracy: 0.6078 - val_loss: 1.1423 - val_accuracy: 0.5852\n",
      "Epoch 151/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0788 - accuracy: 0.6029 - val_loss: 1.1487 - val_accuracy: 0.5704\n",
      "Epoch 152/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.0375 - accuracy: 0.6084 - val_loss: 1.1224 - val_accuracy: 0.5877\n",
      "Epoch 153/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.0547 - accuracy: 0.6145 - val_loss: 1.1311 - val_accuracy: 0.5852\n",
      "Epoch 154/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0494 - accuracy: 0.6291 - val_loss: 1.1531 - val_accuracy: 0.5531\n",
      "Epoch 155/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0524 - accuracy: 0.6090 - val_loss: 1.1071 - val_accuracy: 0.5988\n",
      "Epoch 156/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0390 - accuracy: 0.6194 - val_loss: 1.1338 - val_accuracy: 0.5864\n",
      "Epoch 157/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.0450 - accuracy: 0.6188 - val_loss: 1.1403 - val_accuracy: 0.5790\n",
      "Epoch 158/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.0462 - accuracy: 0.6121 - val_loss: 1.1195 - val_accuracy: 0.6049\n",
      "Epoch 159/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.0669 - accuracy: 0.6200 - val_loss: 1.1196 - val_accuracy: 0.5877\n",
      "Epoch 160/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0410 - accuracy: 0.6181 - val_loss: 1.1321 - val_accuracy: 0.5827\n",
      "Epoch 161/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0466 - accuracy: 0.6078 - val_loss: 1.1109 - val_accuracy: 0.6012\n",
      "Epoch 162/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0457 - accuracy: 0.6114 - val_loss: 1.1185 - val_accuracy: 0.5938\n",
      "Epoch 163/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.0297 - accuracy: 0.6212 - val_loss: 1.1246 - val_accuracy: 0.5765\n",
      "Epoch 164/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0197 - accuracy: 0.6188 - val_loss: 1.1062 - val_accuracy: 0.6037\n",
      "Epoch 165/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0251 - accuracy: 0.6230 - val_loss: 1.1184 - val_accuracy: 0.6062\n",
      "Epoch 166/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0068 - accuracy: 0.6364 - val_loss: 1.1197 - val_accuracy: 0.5914\n",
      "Epoch 167/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 1.0253 - accuracy: 0.6309 - val_loss: 1.1305 - val_accuracy: 0.5815\n",
      "Epoch 168/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 1.0215 - accuracy: 0.6273 - val_loss: 1.1056 - val_accuracy: 0.6000\n",
      "Epoch 169/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0197 - accuracy: 0.6212 - val_loss: 1.1232 - val_accuracy: 0.5790\n",
      "Epoch 170/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0334 - accuracy: 0.6242 - val_loss: 1.1078 - val_accuracy: 0.5815\n",
      "Epoch 171/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0081 - accuracy: 0.6389 - val_loss: 1.0943 - val_accuracy: 0.5864\n",
      "Epoch 172/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0118 - accuracy: 0.6340 - val_loss: 1.0998 - val_accuracy: 0.5963\n",
      "Epoch 173/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0014 - accuracy: 0.6456 - val_loss: 1.1071 - val_accuracy: 0.5877\n",
      "Epoch 174/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 1.0038 - accuracy: 0.6395 - val_loss: 1.0867 - val_accuracy: 0.5988\n",
      "Epoch 175/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.9979 - accuracy: 0.6395 - val_loss: 1.1038 - val_accuracy: 0.6000\n",
      "Epoch 176/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.9888 - accuracy: 0.6352 - val_loss: 1.0964 - val_accuracy: 0.6012\n",
      "Epoch 177/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 1.0008 - accuracy: 0.6419 - val_loss: 1.0778 - val_accuracy: 0.6062\n",
      "Epoch 178/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.0035 - accuracy: 0.6303 - val_loss: 1.0877 - val_accuracy: 0.6074\n",
      "Epoch 179/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 1.0023 - accuracy: 0.6352 - val_loss: 1.0865 - val_accuracy: 0.5988\n",
      "Epoch 180/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9900 - accuracy: 0.6523 - val_loss: 1.0726 - val_accuracy: 0.6111\n",
      "Epoch 181/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 1.0006 - accuracy: 0.6322 - val_loss: 1.0732 - val_accuracy: 0.5938\n",
      "Epoch 182/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.9825 - accuracy: 0.6401 - val_loss: 1.1082 - val_accuracy: 0.5914\n",
      "Epoch 183/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.9686 - accuracy: 0.6492 - val_loss: 1.0847 - val_accuracy: 0.6049\n",
      "Epoch 184/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.9871 - accuracy: 0.6413 - val_loss: 1.0771 - val_accuracy: 0.5938\n",
      "Epoch 185/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.9992 - accuracy: 0.6389 - val_loss: 1.0768 - val_accuracy: 0.6000\n",
      "Epoch 186/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.9798 - accuracy: 0.6437 - val_loss: 1.0785 - val_accuracy: 0.6025\n",
      "Epoch 187/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.9893 - accuracy: 0.6456 - val_loss: 1.1010 - val_accuracy: 0.5802\n",
      "Epoch 188/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9715 - accuracy: 0.6498 - val_loss: 1.0888 - val_accuracy: 0.5988\n",
      "Epoch 189/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9829 - accuracy: 0.6419 - val_loss: 1.0832 - val_accuracy: 0.6049\n",
      "Epoch 190/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9576 - accuracy: 0.6565 - val_loss: 1.0690 - val_accuracy: 0.6173\n",
      "Epoch 191/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9526 - accuracy: 0.6553 - val_loss: 1.0747 - val_accuracy: 0.6049\n",
      "Epoch 192/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9596 - accuracy: 0.6419 - val_loss: 1.0657 - val_accuracy: 0.6049\n",
      "Epoch 193/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9733 - accuracy: 0.6571 - val_loss: 1.0752 - val_accuracy: 0.6099\n",
      "Epoch 194/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9680 - accuracy: 0.6443 - val_loss: 1.0602 - val_accuracy: 0.5975\n",
      "Epoch 195/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.9413 - accuracy: 0.6583 - val_loss: 1.0636 - val_accuracy: 0.6037\n",
      "Epoch 196/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.9536 - accuracy: 0.6553 - val_loss: 1.0583 - val_accuracy: 0.6025\n",
      "Epoch 197/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.9570 - accuracy: 0.6583 - val_loss: 1.0714 - val_accuracy: 0.6099\n",
      "Epoch 198/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9555 - accuracy: 0.6577 - val_loss: 1.0683 - val_accuracy: 0.6062\n",
      "Epoch 199/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.9529 - accuracy: 0.6535 - val_loss: 1.0600 - val_accuracy: 0.6247\n",
      "Epoch 200/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.9347 - accuracy: 0.6565 - val_loss: 1.0616 - val_accuracy: 0.6222\n",
      "Epoch 201/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.9426 - accuracy: 0.6474 - val_loss: 1.0701 - val_accuracy: 0.6012\n",
      "Epoch 202/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9458 - accuracy: 0.6675 - val_loss: 1.0469 - val_accuracy: 0.6160\n",
      "Epoch 203/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9362 - accuracy: 0.6711 - val_loss: 1.0580 - val_accuracy: 0.6062\n",
      "Epoch 204/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.9324 - accuracy: 0.6523 - val_loss: 1.0563 - val_accuracy: 0.6086\n",
      "Epoch 205/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.9292 - accuracy: 0.6474 - val_loss: 1.0530 - val_accuracy: 0.6037\n",
      "Epoch 206/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.9365 - accuracy: 0.6456 - val_loss: 1.0603 - val_accuracy: 0.6198\n",
      "Epoch 207/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.9258 - accuracy: 0.6614 - val_loss: 1.0963 - val_accuracy: 0.5938\n",
      "Epoch 208/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.9420 - accuracy: 0.6620 - val_loss: 1.0568 - val_accuracy: 0.6012\n",
      "Epoch 209/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9072 - accuracy: 0.6754 - val_loss: 1.0518 - val_accuracy: 0.5877\n",
      "Epoch 210/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.9395 - accuracy: 0.6376 - val_loss: 1.0478 - val_accuracy: 0.6111\n",
      "Epoch 211/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9371 - accuracy: 0.6504 - val_loss: 1.0534 - val_accuracy: 0.6111\n",
      "Epoch 212/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.9285 - accuracy: 0.6614 - val_loss: 1.0313 - val_accuracy: 0.6222\n",
      "Epoch 213/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.9293 - accuracy: 0.6547 - val_loss: 1.0411 - val_accuracy: 0.6136\n",
      "Epoch 214/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.9255 - accuracy: 0.6559 - val_loss: 1.0437 - val_accuracy: 0.6062\n",
      "Epoch 215/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.9310 - accuracy: 0.6492 - val_loss: 1.0489 - val_accuracy: 0.6086\n",
      "Epoch 216/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.9056 - accuracy: 0.6797 - val_loss: 1.0623 - val_accuracy: 0.6025\n",
      "Epoch 217/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.9117 - accuracy: 0.6620 - val_loss: 1.0501 - val_accuracy: 0.6222\n",
      "Epoch 218/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.9238 - accuracy: 0.6778 - val_loss: 1.0468 - val_accuracy: 0.6074\n",
      "Epoch 219/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9114 - accuracy: 0.6717 - val_loss: 1.0479 - val_accuracy: 0.6086\n",
      "Epoch 220/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.9015 - accuracy: 0.6766 - val_loss: 1.0419 - val_accuracy: 0.6136\n",
      "Epoch 221/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.9039 - accuracy: 0.6699 - val_loss: 1.0738 - val_accuracy: 0.5840\n",
      "Epoch 222/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.9044 - accuracy: 0.6742 - val_loss: 1.0302 - val_accuracy: 0.6309\n",
      "Epoch 223/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.8975 - accuracy: 0.6705 - val_loss: 1.0295 - val_accuracy: 0.6247\n",
      "Epoch 224/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.8939 - accuracy: 0.6821 - val_loss: 1.0286 - val_accuracy: 0.6173\n",
      "Epoch 225/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.8846 - accuracy: 0.6760 - val_loss: 1.0258 - val_accuracy: 0.6235\n",
      "Epoch 226/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.8763 - accuracy: 0.6937 - val_loss: 1.0413 - val_accuracy: 0.6198\n",
      "Epoch 227/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.9076 - accuracy: 0.6632 - val_loss: 1.0345 - val_accuracy: 0.6173\n",
      "Epoch 228/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.8826 - accuracy: 0.6754 - val_loss: 1.0800 - val_accuracy: 0.5975\n",
      "Epoch 229/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.8733 - accuracy: 0.6918 - val_loss: 1.0396 - val_accuracy: 0.6136\n",
      "Epoch 230/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.8776 - accuracy: 0.6797 - val_loss: 1.0308 - val_accuracy: 0.6012\n",
      "Epoch 231/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.8877 - accuracy: 0.6778 - val_loss: 1.0363 - val_accuracy: 0.6185\n",
      "Epoch 232/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.9015 - accuracy: 0.6583 - val_loss: 1.0513 - val_accuracy: 0.6062\n",
      "Epoch 233/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.8685 - accuracy: 0.6912 - val_loss: 1.0234 - val_accuracy: 0.6123\n",
      "Epoch 234/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.8731 - accuracy: 0.6760 - val_loss: 1.0349 - val_accuracy: 0.6123\n",
      "Epoch 235/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.8717 - accuracy: 0.6857 - val_loss: 1.0382 - val_accuracy: 0.6012\n",
      "Epoch 236/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.8834 - accuracy: 0.6790 - val_loss: 1.0316 - val_accuracy: 0.6160\n",
      "Epoch 237/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.8918 - accuracy: 0.6626 - val_loss: 1.0113 - val_accuracy: 0.6296\n",
      "Epoch 238/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.8682 - accuracy: 0.6833 - val_loss: 1.0209 - val_accuracy: 0.6198\n",
      "Epoch 239/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8587 - accuracy: 0.6937 - val_loss: 1.0330 - val_accuracy: 0.6333\n",
      "Epoch 240/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.8620 - accuracy: 0.6833 - val_loss: 1.0275 - val_accuracy: 0.6111\n",
      "Epoch 241/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.8692 - accuracy: 0.6876 - val_loss: 1.0220 - val_accuracy: 0.6173\n",
      "Epoch 242/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.8684 - accuracy: 0.6784 - val_loss: 1.0220 - val_accuracy: 0.6185\n",
      "Epoch 243/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.8670 - accuracy: 0.6912 - val_loss: 1.0172 - val_accuracy: 0.6284\n",
      "Epoch 244/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.8464 - accuracy: 0.6870 - val_loss: 1.0595 - val_accuracy: 0.6160\n",
      "Epoch 245/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.8555 - accuracy: 0.6894 - val_loss: 1.0137 - val_accuracy: 0.6247\n",
      "Epoch 246/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.8628 - accuracy: 0.6864 - val_loss: 1.0146 - val_accuracy: 0.6198\n",
      "Epoch 247/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.8535 - accuracy: 0.6900 - val_loss: 1.0477 - val_accuracy: 0.6259\n",
      "Epoch 248/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.8463 - accuracy: 0.6876 - val_loss: 1.0182 - val_accuracy: 0.6272\n",
      "Epoch 249/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8589 - accuracy: 0.6973 - val_loss: 1.0274 - val_accuracy: 0.6173\n",
      "Epoch 250/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.8449 - accuracy: 0.6821 - val_loss: 1.0321 - val_accuracy: 0.6136\n",
      "Epoch 251/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.8346 - accuracy: 0.6943 - val_loss: 1.0335 - val_accuracy: 0.6049\n",
      "Epoch 252/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.8561 - accuracy: 0.6985 - val_loss: 1.0097 - val_accuracy: 0.6333\n",
      "Epoch 253/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8334 - accuracy: 0.6937 - val_loss: 0.9871 - val_accuracy: 0.6296\n",
      "Epoch 254/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.8229 - accuracy: 0.7058 - val_loss: 1.0092 - val_accuracy: 0.6210\n",
      "Epoch 255/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.8418 - accuracy: 0.6961 - val_loss: 1.0255 - val_accuracy: 0.6037\n",
      "Epoch 256/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.8403 - accuracy: 0.6900 - val_loss: 0.9933 - val_accuracy: 0.6284\n",
      "Epoch 257/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.8441 - accuracy: 0.7077 - val_loss: 1.0074 - val_accuracy: 0.6284\n",
      "Epoch 258/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.8263 - accuracy: 0.6961 - val_loss: 0.9934 - val_accuracy: 0.6321\n",
      "Epoch 259/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.8311 - accuracy: 0.7010 - val_loss: 1.0185 - val_accuracy: 0.6321\n",
      "Epoch 260/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.8191 - accuracy: 0.7156 - val_loss: 1.0134 - val_accuracy: 0.6284\n",
      "Epoch 261/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.8198 - accuracy: 0.7004 - val_loss: 1.0058 - val_accuracy: 0.6185\n",
      "Epoch 262/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.8383 - accuracy: 0.6888 - val_loss: 1.0170 - val_accuracy: 0.6173\n",
      "Epoch 263/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.8306 - accuracy: 0.7052 - val_loss: 1.0049 - val_accuracy: 0.6333\n",
      "Epoch 264/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.8304 - accuracy: 0.6949 - val_loss: 0.9944 - val_accuracy: 0.6309\n",
      "Epoch 265/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.8183 - accuracy: 0.7022 - val_loss: 1.0063 - val_accuracy: 0.6185\n",
      "Epoch 266/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.8150 - accuracy: 0.7004 - val_loss: 1.0006 - val_accuracy: 0.6086\n",
      "Epoch 267/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8179 - accuracy: 0.7089 - val_loss: 0.9935 - val_accuracy: 0.6333\n",
      "Epoch 268/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.8046 - accuracy: 0.7077 - val_loss: 0.9799 - val_accuracy: 0.6346\n",
      "Epoch 269/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.8071 - accuracy: 0.7046 - val_loss: 1.0066 - val_accuracy: 0.6272\n",
      "Epoch 270/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.8074 - accuracy: 0.7022 - val_loss: 0.9957 - val_accuracy: 0.6185\n",
      "Epoch 271/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.8096 - accuracy: 0.7107 - val_loss: 0.9948 - val_accuracy: 0.6259\n",
      "Epoch 272/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.8075 - accuracy: 0.7150 - val_loss: 1.0071 - val_accuracy: 0.6284\n",
      "Epoch 273/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.8137 - accuracy: 0.7065 - val_loss: 0.9859 - val_accuracy: 0.6358\n",
      "Epoch 274/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.8027 - accuracy: 0.6924 - val_loss: 1.0525 - val_accuracy: 0.6012\n",
      "Epoch 275/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7917 - accuracy: 0.7223 - val_loss: 0.9780 - val_accuracy: 0.6383\n",
      "Epoch 276/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.7936 - accuracy: 0.7071 - val_loss: 0.9982 - val_accuracy: 0.6272\n",
      "Epoch 277/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.7996 - accuracy: 0.7046 - val_loss: 0.9870 - val_accuracy: 0.6506\n",
      "Epoch 278/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7991 - accuracy: 0.7083 - val_loss: 0.9897 - val_accuracy: 0.6333\n",
      "Epoch 279/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7825 - accuracy: 0.7046 - val_loss: 0.9796 - val_accuracy: 0.6407\n",
      "Epoch 280/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7777 - accuracy: 0.7241 - val_loss: 0.9810 - val_accuracy: 0.6284\n",
      "Epoch 281/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.7783 - accuracy: 0.7089 - val_loss: 0.9772 - val_accuracy: 0.6395\n",
      "Epoch 282/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7705 - accuracy: 0.7205 - val_loss: 0.9787 - val_accuracy: 0.6333\n",
      "Epoch 283/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7758 - accuracy: 0.7192 - val_loss: 1.0097 - val_accuracy: 0.6296\n",
      "Epoch 284/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.7971 - accuracy: 0.7077 - val_loss: 0.9844 - val_accuracy: 0.6407\n",
      "Epoch 285/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7855 - accuracy: 0.7211 - val_loss: 0.9800 - val_accuracy: 0.6469\n",
      "Epoch 286/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.7939 - accuracy: 0.7138 - val_loss: 0.9784 - val_accuracy: 0.6519\n",
      "Epoch 287/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7722 - accuracy: 0.7144 - val_loss: 0.9823 - val_accuracy: 0.6296\n",
      "Epoch 288/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7699 - accuracy: 0.7284 - val_loss: 0.9706 - val_accuracy: 0.6457\n",
      "Epoch 289/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7686 - accuracy: 0.7168 - val_loss: 1.0277 - val_accuracy: 0.6136\n",
      "Epoch 290/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7816 - accuracy: 0.7132 - val_loss: 0.9740 - val_accuracy: 0.6370\n",
      "Epoch 291/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7507 - accuracy: 0.7162 - val_loss: 0.9845 - val_accuracy: 0.6370\n",
      "Epoch 292/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.7768 - accuracy: 0.7211 - val_loss: 0.9782 - val_accuracy: 0.6284\n",
      "Epoch 293/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.7666 - accuracy: 0.7162 - val_loss: 0.9858 - val_accuracy: 0.6444\n",
      "Epoch 294/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7949 - accuracy: 0.7107 - val_loss: 0.9835 - val_accuracy: 0.6494\n",
      "Epoch 295/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.7802 - accuracy: 0.7199 - val_loss: 0.9755 - val_accuracy: 0.6407\n",
      "Epoch 296/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7796 - accuracy: 0.7229 - val_loss: 0.9836 - val_accuracy: 0.6210\n",
      "Epoch 297/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.7695 - accuracy: 0.7125 - val_loss: 0.9685 - val_accuracy: 0.6444\n",
      "Epoch 298/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7702 - accuracy: 0.7162 - val_loss: 0.9681 - val_accuracy: 0.6543\n",
      "Epoch 299/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.7767 - accuracy: 0.7266 - val_loss: 0.9607 - val_accuracy: 0.6494\n",
      "Epoch 300/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.7754 - accuracy: 0.7235 - val_loss: 0.9772 - val_accuracy: 0.6346\n",
      "Epoch 301/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.7406 - accuracy: 0.7192 - val_loss: 0.9641 - val_accuracy: 0.6481\n",
      "Epoch 302/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7301 - accuracy: 0.7467 - val_loss: 0.9888 - val_accuracy: 0.6444\n",
      "Epoch 303/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7469 - accuracy: 0.7278 - val_loss: 0.9573 - val_accuracy: 0.6543\n",
      "Epoch 304/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7414 - accuracy: 0.7375 - val_loss: 0.9702 - val_accuracy: 0.6395\n",
      "Epoch 305/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7666 - accuracy: 0.7180 - val_loss: 0.9631 - val_accuracy: 0.6383\n",
      "Epoch 306/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7547 - accuracy: 0.7412 - val_loss: 0.9724 - val_accuracy: 0.6333\n",
      "Epoch 307/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7236 - accuracy: 0.7393 - val_loss: 0.9846 - val_accuracy: 0.6222\n",
      "Epoch 308/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.7417 - accuracy: 0.7363 - val_loss: 0.9513 - val_accuracy: 0.6568\n",
      "Epoch 309/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7503 - accuracy: 0.7241 - val_loss: 0.9818 - val_accuracy: 0.6358\n",
      "Epoch 310/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.7394 - accuracy: 0.7393 - val_loss: 0.9684 - val_accuracy: 0.6333\n",
      "Epoch 311/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.7355 - accuracy: 0.7412 - val_loss: 0.9821 - val_accuracy: 0.6506\n",
      "Epoch 312/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7320 - accuracy: 0.7339 - val_loss: 0.9733 - val_accuracy: 0.6321\n",
      "Epoch 313/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.7565 - accuracy: 0.7278 - val_loss: 0.9478 - val_accuracy: 0.6506\n",
      "Epoch 314/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7476 - accuracy: 0.7363 - val_loss: 0.9777 - val_accuracy: 0.6395\n",
      "Epoch 315/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.7359 - accuracy: 0.7314 - val_loss: 0.9744 - val_accuracy: 0.6543\n",
      "Epoch 316/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.7453 - accuracy: 0.7320 - val_loss: 0.9517 - val_accuracy: 0.6494\n",
      "Epoch 317/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.7288 - accuracy: 0.7333 - val_loss: 0.9697 - val_accuracy: 0.6506\n",
      "Epoch 318/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.7238 - accuracy: 0.7290 - val_loss: 0.9517 - val_accuracy: 0.6284\n",
      "Epoch 319/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7090 - accuracy: 0.7442 - val_loss: 0.9713 - val_accuracy: 0.6420\n",
      "Epoch 320/1000\n",
      "103/103 [==============================] - 2s 16ms/step - loss: 0.7210 - accuracy: 0.7393 - val_loss: 0.9520 - val_accuracy: 0.6432\n",
      "Epoch 321/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7304 - accuracy: 0.7357 - val_loss: 0.9867 - val_accuracy: 0.6370\n",
      "Epoch 322/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.7204 - accuracy: 0.7503 - val_loss: 0.9957 - val_accuracy: 0.6235\n",
      "Epoch 323/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.7219 - accuracy: 0.7363 - val_loss: 0.9551 - val_accuracy: 0.6506\n",
      "Epoch 324/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7091 - accuracy: 0.7600 - val_loss: 0.9513 - val_accuracy: 0.6407\n",
      "Epoch 325/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7328 - accuracy: 0.7406 - val_loss: 0.9488 - val_accuracy: 0.6469\n",
      "Epoch 326/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7200 - accuracy: 0.7369 - val_loss: 0.9488 - val_accuracy: 0.6506\n",
      "Epoch 327/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.7340 - accuracy: 0.7369 - val_loss: 0.9586 - val_accuracy: 0.6506\n",
      "Epoch 328/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7066 - accuracy: 0.7473 - val_loss: 0.9512 - val_accuracy: 0.6568\n",
      "Epoch 329/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.7038 - accuracy: 0.7491 - val_loss: 0.9516 - val_accuracy: 0.6556\n",
      "Epoch 330/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.7159 - accuracy: 0.7339 - val_loss: 0.9467 - val_accuracy: 0.6481\n",
      "Epoch 331/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7178 - accuracy: 0.7393 - val_loss: 0.9476 - val_accuracy: 0.6407\n",
      "Epoch 332/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6989 - accuracy: 0.7460 - val_loss: 0.9420 - val_accuracy: 0.6469\n",
      "Epoch 333/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.7180 - accuracy: 0.7436 - val_loss: 0.9519 - val_accuracy: 0.6469\n",
      "Epoch 334/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.7054 - accuracy: 0.7503 - val_loss: 0.9499 - val_accuracy: 0.6420\n",
      "Epoch 335/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6976 - accuracy: 0.7570 - val_loss: 0.9595 - val_accuracy: 0.6407\n",
      "Epoch 336/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.6960 - accuracy: 0.7515 - val_loss: 0.9547 - val_accuracy: 0.6617\n",
      "Epoch 337/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.6936 - accuracy: 0.7625 - val_loss: 0.9493 - val_accuracy: 0.6444\n",
      "Epoch 338/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7027 - accuracy: 0.7527 - val_loss: 0.9482 - val_accuracy: 0.6420\n",
      "Epoch 339/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.6899 - accuracy: 0.7533 - val_loss: 0.9864 - val_accuracy: 0.6346\n",
      "Epoch 340/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7001 - accuracy: 0.7546 - val_loss: 0.9623 - val_accuracy: 0.6395\n",
      "Epoch 341/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.7012 - accuracy: 0.7442 - val_loss: 0.9588 - val_accuracy: 0.6333\n",
      "Epoch 342/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6829 - accuracy: 0.7607 - val_loss: 0.9433 - val_accuracy: 0.6481\n",
      "Epoch 343/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6968 - accuracy: 0.7509 - val_loss: 0.9692 - val_accuracy: 0.6259\n",
      "Epoch 344/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6928 - accuracy: 0.7509 - val_loss: 0.9501 - val_accuracy: 0.6506\n",
      "Epoch 345/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.7009 - accuracy: 0.7540 - val_loss: 0.9560 - val_accuracy: 0.6469\n",
      "Epoch 346/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6898 - accuracy: 0.7527 - val_loss: 0.9591 - val_accuracy: 0.6395\n",
      "Epoch 347/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6884 - accuracy: 0.7540 - val_loss: 0.9322 - val_accuracy: 0.6580\n",
      "Epoch 348/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.6778 - accuracy: 0.7582 - val_loss: 0.9601 - val_accuracy: 0.6506\n",
      "Epoch 349/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6791 - accuracy: 0.7582 - val_loss: 0.9242 - val_accuracy: 0.6593\n",
      "Epoch 350/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.6733 - accuracy: 0.7576 - val_loss: 0.9597 - val_accuracy: 0.6370\n",
      "Epoch 351/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6685 - accuracy: 0.7643 - val_loss: 0.9230 - val_accuracy: 0.6617\n",
      "Epoch 352/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6794 - accuracy: 0.7448 - val_loss: 0.9359 - val_accuracy: 0.6519\n",
      "Epoch 353/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.6781 - accuracy: 0.7540 - val_loss: 0.9308 - val_accuracy: 0.6642\n",
      "Epoch 354/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6724 - accuracy: 0.7667 - val_loss: 0.9330 - val_accuracy: 0.6654\n",
      "Epoch 355/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6951 - accuracy: 0.7527 - val_loss: 0.9273 - val_accuracy: 0.6691\n",
      "Epoch 356/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6510 - accuracy: 0.7759 - val_loss: 0.9418 - val_accuracy: 0.6469\n",
      "Epoch 357/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.6769 - accuracy: 0.7600 - val_loss: 0.9454 - val_accuracy: 0.6432\n",
      "Epoch 358/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.6712 - accuracy: 0.7546 - val_loss: 0.9368 - val_accuracy: 0.6654\n",
      "Epoch 359/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6638 - accuracy: 0.7692 - val_loss: 0.9363 - val_accuracy: 0.6642\n",
      "Epoch 360/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6681 - accuracy: 0.7637 - val_loss: 0.9240 - val_accuracy: 0.6593\n",
      "Epoch 361/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6646 - accuracy: 0.7637 - val_loss: 0.9441 - val_accuracy: 0.6617\n",
      "Epoch 362/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6669 - accuracy: 0.7655 - val_loss: 0.9318 - val_accuracy: 0.6605\n",
      "Epoch 363/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.6686 - accuracy: 0.7588 - val_loss: 0.9463 - val_accuracy: 0.6469\n",
      "Epoch 364/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6547 - accuracy: 0.7698 - val_loss: 0.9324 - val_accuracy: 0.6568\n",
      "Epoch 365/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6662 - accuracy: 0.7710 - val_loss: 0.9342 - val_accuracy: 0.6556\n",
      "Epoch 366/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6512 - accuracy: 0.7637 - val_loss: 0.9241 - val_accuracy: 0.6556\n",
      "Epoch 367/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6430 - accuracy: 0.7783 - val_loss: 0.9406 - val_accuracy: 0.6543\n",
      "Epoch 368/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6491 - accuracy: 0.7753 - val_loss: 0.9278 - val_accuracy: 0.6494\n",
      "Epoch 369/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6629 - accuracy: 0.7588 - val_loss: 0.9388 - val_accuracy: 0.6667\n",
      "Epoch 370/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6405 - accuracy: 0.7747 - val_loss: 0.9256 - val_accuracy: 0.6506\n",
      "Epoch 371/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.6688 - accuracy: 0.7485 - val_loss: 0.9210 - val_accuracy: 0.6568\n",
      "Epoch 372/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.6636 - accuracy: 0.7576 - val_loss: 0.9485 - val_accuracy: 0.6481\n",
      "Epoch 373/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6495 - accuracy: 0.7856 - val_loss: 0.9253 - val_accuracy: 0.6593\n",
      "Epoch 374/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6503 - accuracy: 0.7704 - val_loss: 0.9247 - val_accuracy: 0.6642\n",
      "Epoch 375/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6438 - accuracy: 0.7704 - val_loss: 0.9195 - val_accuracy: 0.6617\n",
      "Epoch 376/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6358 - accuracy: 0.7722 - val_loss: 0.9272 - val_accuracy: 0.6679\n",
      "Epoch 377/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6446 - accuracy: 0.7826 - val_loss: 0.9607 - val_accuracy: 0.6469\n",
      "Epoch 378/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6376 - accuracy: 0.7686 - val_loss: 0.9636 - val_accuracy: 0.6235\n",
      "Epoch 379/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6343 - accuracy: 0.7728 - val_loss: 0.9567 - val_accuracy: 0.6605\n",
      "Epoch 380/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.6587 - accuracy: 0.7686 - val_loss: 0.9432 - val_accuracy: 0.6580\n",
      "Epoch 381/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6464 - accuracy: 0.7667 - val_loss: 0.9524 - val_accuracy: 0.6568\n",
      "Epoch 382/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.6468 - accuracy: 0.7649 - val_loss: 0.9339 - val_accuracy: 0.6556\n",
      "Epoch 383/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.6559 - accuracy: 0.7728 - val_loss: 0.9327 - val_accuracy: 0.6580\n",
      "Epoch 384/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.6402 - accuracy: 0.7789 - val_loss: 0.9140 - val_accuracy: 0.6654\n",
      "Epoch 385/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6302 - accuracy: 0.7789 - val_loss: 0.9236 - val_accuracy: 0.6556\n",
      "Epoch 386/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6385 - accuracy: 0.7716 - val_loss: 0.9079 - val_accuracy: 0.6543\n",
      "Epoch 387/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6330 - accuracy: 0.7716 - val_loss: 0.9171 - val_accuracy: 0.6642\n",
      "Epoch 388/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6283 - accuracy: 0.7838 - val_loss: 0.9242 - val_accuracy: 0.6667\n",
      "Epoch 389/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.6340 - accuracy: 0.7716 - val_loss: 0.9267 - val_accuracy: 0.6617\n",
      "Epoch 390/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.6402 - accuracy: 0.7710 - val_loss: 0.9163 - val_accuracy: 0.6654\n",
      "Epoch 391/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.6264 - accuracy: 0.7716 - val_loss: 0.9150 - val_accuracy: 0.6556\n",
      "Epoch 392/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6250 - accuracy: 0.7826 - val_loss: 0.9469 - val_accuracy: 0.6654\n",
      "Epoch 393/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6124 - accuracy: 0.7795 - val_loss: 0.9140 - val_accuracy: 0.6691\n",
      "Epoch 394/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6164 - accuracy: 0.7820 - val_loss: 0.9294 - val_accuracy: 0.6506\n",
      "Epoch 395/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6257 - accuracy: 0.7759 - val_loss: 0.9403 - val_accuracy: 0.6383\n",
      "Epoch 396/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6220 - accuracy: 0.7795 - val_loss: 0.9188 - val_accuracy: 0.6481\n",
      "Epoch 397/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6400 - accuracy: 0.7783 - val_loss: 0.9190 - val_accuracy: 0.6679\n",
      "Epoch 398/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5927 - accuracy: 0.7899 - val_loss: 0.9399 - val_accuracy: 0.6543\n",
      "Epoch 399/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6311 - accuracy: 0.7741 - val_loss: 0.9218 - val_accuracy: 0.6556\n",
      "Epoch 400/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6145 - accuracy: 0.7838 - val_loss: 0.9275 - val_accuracy: 0.6556\n",
      "Epoch 401/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5980 - accuracy: 0.7942 - val_loss: 0.9295 - val_accuracy: 0.6704\n",
      "Epoch 402/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6164 - accuracy: 0.7814 - val_loss: 0.9170 - val_accuracy: 0.6667\n",
      "Epoch 403/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6048 - accuracy: 0.7814 - val_loss: 0.9480 - val_accuracy: 0.6580\n",
      "Epoch 404/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6196 - accuracy: 0.7850 - val_loss: 0.9092 - val_accuracy: 0.6494\n",
      "Epoch 405/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.6036 - accuracy: 0.7875 - val_loss: 0.9343 - val_accuracy: 0.6506\n",
      "Epoch 406/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.5999 - accuracy: 0.7826 - val_loss: 0.9563 - val_accuracy: 0.6444\n",
      "Epoch 407/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.6274 - accuracy: 0.7631 - val_loss: 0.9223 - val_accuracy: 0.6691\n",
      "Epoch 408/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5930 - accuracy: 0.8015 - val_loss: 0.9210 - val_accuracy: 0.6617\n",
      "Epoch 409/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5923 - accuracy: 0.7923 - val_loss: 0.9346 - val_accuracy: 0.6704\n",
      "Epoch 410/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.6010 - accuracy: 0.7935 - val_loss: 0.9294 - val_accuracy: 0.6605\n",
      "Epoch 411/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6166 - accuracy: 0.7808 - val_loss: 0.9224 - val_accuracy: 0.6605\n",
      "Epoch 412/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5967 - accuracy: 0.7850 - val_loss: 0.9129 - val_accuracy: 0.6605\n",
      "Epoch 413/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.6003 - accuracy: 0.7996 - val_loss: 0.9199 - val_accuracy: 0.6654\n",
      "Epoch 414/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5890 - accuracy: 0.7948 - val_loss: 0.9210 - val_accuracy: 0.6407\n",
      "Epoch 415/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5985 - accuracy: 0.7923 - val_loss: 0.9255 - val_accuracy: 0.6605\n",
      "Epoch 416/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.6005 - accuracy: 0.7838 - val_loss: 0.9583 - val_accuracy: 0.6457\n",
      "Epoch 417/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5937 - accuracy: 0.7948 - val_loss: 0.9025 - val_accuracy: 0.6741\n",
      "Epoch 418/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.6015 - accuracy: 0.7862 - val_loss: 0.9237 - val_accuracy: 0.6617\n",
      "Epoch 419/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5947 - accuracy: 0.7814 - val_loss: 0.9210 - val_accuracy: 0.6654\n",
      "Epoch 420/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.5704 - accuracy: 0.7996 - val_loss: 0.9143 - val_accuracy: 0.6654\n",
      "Epoch 421/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5773 - accuracy: 0.7984 - val_loss: 0.9098 - val_accuracy: 0.6593\n",
      "Epoch 422/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.5782 - accuracy: 0.7935 - val_loss: 0.9413 - val_accuracy: 0.6519\n",
      "Epoch 423/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.6021 - accuracy: 0.7996 - val_loss: 0.9102 - val_accuracy: 0.6630\n",
      "Epoch 424/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5757 - accuracy: 0.7917 - val_loss: 0.9098 - val_accuracy: 0.6667\n",
      "Epoch 425/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5681 - accuracy: 0.7972 - val_loss: 0.9655 - val_accuracy: 0.6457\n",
      "Epoch 426/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5951 - accuracy: 0.7826 - val_loss: 0.9069 - val_accuracy: 0.6790\n",
      "Epoch 427/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5812 - accuracy: 0.7899 - val_loss: 0.9268 - val_accuracy: 0.6728\n",
      "Epoch 428/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5665 - accuracy: 0.7960 - val_loss: 0.9124 - val_accuracy: 0.6642\n",
      "Epoch 429/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5906 - accuracy: 0.7954 - val_loss: 0.9217 - val_accuracy: 0.6580\n",
      "Epoch 430/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.5863 - accuracy: 0.7972 - val_loss: 0.9265 - val_accuracy: 0.6642\n",
      "Epoch 431/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5930 - accuracy: 0.7862 - val_loss: 0.9132 - val_accuracy: 0.6741\n",
      "Epoch 432/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5748 - accuracy: 0.8027 - val_loss: 0.9033 - val_accuracy: 0.6765\n",
      "Epoch 433/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.5763 - accuracy: 0.8002 - val_loss: 0.9013 - val_accuracy: 0.6716\n",
      "Epoch 434/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5749 - accuracy: 0.8015 - val_loss: 0.9046 - val_accuracy: 0.6630\n",
      "Epoch 435/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5845 - accuracy: 0.7929 - val_loss: 0.9149 - val_accuracy: 0.6667\n",
      "Epoch 436/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5709 - accuracy: 0.8082 - val_loss: 0.9028 - val_accuracy: 0.6753\n",
      "Epoch 437/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5702 - accuracy: 0.7978 - val_loss: 0.9153 - val_accuracy: 0.6728\n",
      "Epoch 438/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5725 - accuracy: 0.7990 - val_loss: 0.8956 - val_accuracy: 0.6691\n",
      "Epoch 439/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5724 - accuracy: 0.7972 - val_loss: 0.9107 - val_accuracy: 0.6654\n",
      "Epoch 440/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5617 - accuracy: 0.7978 - val_loss: 0.9188 - val_accuracy: 0.6531\n",
      "Epoch 441/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5637 - accuracy: 0.7905 - val_loss: 0.9132 - val_accuracy: 0.6654\n",
      "Epoch 442/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5784 - accuracy: 0.7978 - val_loss: 0.9128 - val_accuracy: 0.6630\n",
      "Epoch 443/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.5642 - accuracy: 0.7978 - val_loss: 0.9511 - val_accuracy: 0.6617\n",
      "Epoch 444/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5778 - accuracy: 0.7954 - val_loss: 0.9054 - val_accuracy: 0.6679\n",
      "Epoch 445/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5705 - accuracy: 0.7923 - val_loss: 0.9282 - val_accuracy: 0.6679\n",
      "Epoch 446/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5605 - accuracy: 0.7905 - val_loss: 0.9148 - val_accuracy: 0.6667\n",
      "Epoch 447/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5401 - accuracy: 0.8057 - val_loss: 0.9117 - val_accuracy: 0.6642\n",
      "Epoch 448/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5563 - accuracy: 0.7948 - val_loss: 0.9242 - val_accuracy: 0.6728\n",
      "Epoch 449/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5691 - accuracy: 0.7954 - val_loss: 0.9133 - val_accuracy: 0.6630\n",
      "Epoch 450/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5517 - accuracy: 0.8112 - val_loss: 0.9089 - val_accuracy: 0.6617\n",
      "Epoch 451/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5651 - accuracy: 0.7990 - val_loss: 0.9197 - val_accuracy: 0.6580\n",
      "Epoch 452/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5290 - accuracy: 0.8100 - val_loss: 0.9542 - val_accuracy: 0.6383\n",
      "Epoch 453/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5558 - accuracy: 0.8045 - val_loss: 0.8911 - val_accuracy: 0.6864\n",
      "Epoch 454/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5367 - accuracy: 0.8045 - val_loss: 0.9592 - val_accuracy: 0.6654\n",
      "Epoch 455/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5416 - accuracy: 0.8124 - val_loss: 0.9183 - val_accuracy: 0.6778\n",
      "Epoch 456/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5594 - accuracy: 0.7917 - val_loss: 0.8938 - val_accuracy: 0.6741\n",
      "Epoch 457/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5489 - accuracy: 0.8027 - val_loss: 0.9061 - val_accuracy: 0.6741\n",
      "Epoch 458/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.5607 - accuracy: 0.7990 - val_loss: 0.8951 - val_accuracy: 0.6667\n",
      "Epoch 459/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5495 - accuracy: 0.8149 - val_loss: 0.8961 - val_accuracy: 0.6642\n",
      "Epoch 460/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5395 - accuracy: 0.8057 - val_loss: 0.8919 - val_accuracy: 0.6704\n",
      "Epoch 461/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5326 - accuracy: 0.8112 - val_loss: 0.9101 - val_accuracy: 0.6765\n",
      "Epoch 462/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5624 - accuracy: 0.8057 - val_loss: 0.8978 - val_accuracy: 0.6815\n",
      "Epoch 463/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5573 - accuracy: 0.8021 - val_loss: 0.8938 - val_accuracy: 0.6679\n",
      "Epoch 464/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5435 - accuracy: 0.8149 - val_loss: 0.9172 - val_accuracy: 0.6654\n",
      "Epoch 465/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5287 - accuracy: 0.8173 - val_loss: 0.9184 - val_accuracy: 0.6605\n",
      "Epoch 466/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.5535 - accuracy: 0.8076 - val_loss: 0.9048 - val_accuracy: 0.6642\n",
      "Epoch 467/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5397 - accuracy: 0.7948 - val_loss: 0.9193 - val_accuracy: 0.6691\n",
      "Epoch 468/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.5425 - accuracy: 0.8039 - val_loss: 0.9040 - val_accuracy: 0.6494\n",
      "Epoch 469/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5369 - accuracy: 0.8185 - val_loss: 0.9014 - val_accuracy: 0.6667\n",
      "Epoch 470/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5220 - accuracy: 0.8094 - val_loss: 0.9102 - val_accuracy: 0.6704\n",
      "Epoch 471/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5239 - accuracy: 0.8143 - val_loss: 0.9078 - val_accuracy: 0.6704\n",
      "Epoch 472/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5344 - accuracy: 0.8094 - val_loss: 0.9109 - val_accuracy: 0.6630\n",
      "Epoch 473/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5298 - accuracy: 0.8191 - val_loss: 0.8982 - val_accuracy: 0.6852\n",
      "Epoch 474/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.5189 - accuracy: 0.8228 - val_loss: 0.9019 - val_accuracy: 0.6753\n",
      "Epoch 475/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5329 - accuracy: 0.8069 - val_loss: 0.9303 - val_accuracy: 0.6481\n",
      "Epoch 476/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5252 - accuracy: 0.8149 - val_loss: 0.9077 - val_accuracy: 0.6654\n",
      "Epoch 477/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5194 - accuracy: 0.8185 - val_loss: 0.9167 - val_accuracy: 0.6679\n",
      "Epoch 478/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5422 - accuracy: 0.8100 - val_loss: 0.9097 - val_accuracy: 0.6728\n",
      "Epoch 479/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5318 - accuracy: 0.8197 - val_loss: 0.9082 - val_accuracy: 0.6679\n",
      "Epoch 480/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5062 - accuracy: 0.8167 - val_loss: 0.9043 - val_accuracy: 0.6790\n",
      "Epoch 481/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5272 - accuracy: 0.8161 - val_loss: 0.9189 - val_accuracy: 0.6778\n",
      "Epoch 482/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.5198 - accuracy: 0.8246 - val_loss: 0.9081 - val_accuracy: 0.6691\n",
      "Epoch 483/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5200 - accuracy: 0.8136 - val_loss: 0.9099 - val_accuracy: 0.6481\n",
      "Epoch 484/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.5185 - accuracy: 0.8210 - val_loss: 0.9187 - val_accuracy: 0.6642\n",
      "Epoch 485/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5104 - accuracy: 0.8082 - val_loss: 0.9097 - val_accuracy: 0.6667\n",
      "Epoch 486/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5199 - accuracy: 0.8161 - val_loss: 0.8960 - val_accuracy: 0.6852\n",
      "Epoch 487/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.5335 - accuracy: 0.8045 - val_loss: 0.9068 - val_accuracy: 0.6741\n",
      "Epoch 488/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5042 - accuracy: 0.8283 - val_loss: 0.8978 - val_accuracy: 0.6741\n",
      "Epoch 489/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5040 - accuracy: 0.8228 - val_loss: 0.9087 - val_accuracy: 0.6580\n",
      "Epoch 490/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5041 - accuracy: 0.8270 - val_loss: 0.9109 - val_accuracy: 0.6642\n",
      "Epoch 491/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5352 - accuracy: 0.8106 - val_loss: 0.8980 - val_accuracy: 0.6790\n",
      "Epoch 492/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5189 - accuracy: 0.8094 - val_loss: 0.9066 - val_accuracy: 0.6704\n",
      "Epoch 493/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.4969 - accuracy: 0.8283 - val_loss: 0.9048 - val_accuracy: 0.6654\n",
      "Epoch 494/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.5207 - accuracy: 0.8136 - val_loss: 0.9060 - val_accuracy: 0.6593\n",
      "Epoch 495/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.5190 - accuracy: 0.8118 - val_loss: 0.8967 - val_accuracy: 0.6741\n",
      "Epoch 496/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5024 - accuracy: 0.8222 - val_loss: 0.9114 - val_accuracy: 0.6741\n",
      "Epoch 497/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5176 - accuracy: 0.8222 - val_loss: 0.9217 - val_accuracy: 0.6704\n",
      "Epoch 498/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.5154 - accuracy: 0.8179 - val_loss: 0.9171 - val_accuracy: 0.6753\n",
      "Epoch 499/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.5172 - accuracy: 0.8210 - val_loss: 0.9023 - val_accuracy: 0.6630\n",
      "Epoch 500/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.5258 - accuracy: 0.8185 - val_loss: 0.9001 - val_accuracy: 0.6815\n",
      "Epoch 501/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4934 - accuracy: 0.8295 - val_loss: 0.9144 - val_accuracy: 0.6691\n",
      "Epoch 502/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.5019 - accuracy: 0.8185 - val_loss: 0.9161 - val_accuracy: 0.6691\n",
      "Epoch 503/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5061 - accuracy: 0.8197 - val_loss: 0.9000 - val_accuracy: 0.6704\n",
      "Epoch 504/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4888 - accuracy: 0.8173 - val_loss: 0.9015 - val_accuracy: 0.6704\n",
      "Epoch 505/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.5127 - accuracy: 0.8100 - val_loss: 0.9030 - val_accuracy: 0.6765\n",
      "Epoch 506/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4975 - accuracy: 0.8283 - val_loss: 0.9003 - val_accuracy: 0.6667\n",
      "Epoch 507/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4848 - accuracy: 0.8319 - val_loss: 0.9047 - val_accuracy: 0.6753\n",
      "Epoch 508/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.4958 - accuracy: 0.8301 - val_loss: 0.8955 - val_accuracy: 0.6716\n",
      "Epoch 509/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5027 - accuracy: 0.8252 - val_loss: 0.8886 - val_accuracy: 0.6827\n",
      "Epoch 510/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4974 - accuracy: 0.8295 - val_loss: 0.9071 - val_accuracy: 0.6556\n",
      "Epoch 511/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4984 - accuracy: 0.8228 - val_loss: 0.8861 - val_accuracy: 0.6802\n",
      "Epoch 512/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4833 - accuracy: 0.8301 - val_loss: 0.9034 - val_accuracy: 0.6753\n",
      "Epoch 513/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4869 - accuracy: 0.8319 - val_loss: 0.9054 - val_accuracy: 0.6827\n",
      "Epoch 514/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4704 - accuracy: 0.8289 - val_loss: 0.8983 - val_accuracy: 0.6765\n",
      "Epoch 515/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4902 - accuracy: 0.8246 - val_loss: 0.8997 - val_accuracy: 0.6765\n",
      "Epoch 516/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4813 - accuracy: 0.8343 - val_loss: 0.8972 - val_accuracy: 0.6704\n",
      "Epoch 517/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5058 - accuracy: 0.8161 - val_loss: 0.9165 - val_accuracy: 0.6457\n",
      "Epoch 518/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4846 - accuracy: 0.8289 - val_loss: 0.9011 - val_accuracy: 0.6580\n",
      "Epoch 519/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.5066 - accuracy: 0.8240 - val_loss: 0.9133 - val_accuracy: 0.6506\n",
      "Epoch 520/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4799 - accuracy: 0.8423 - val_loss: 0.8950 - val_accuracy: 0.6642\n",
      "Epoch 521/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4768 - accuracy: 0.8313 - val_loss: 0.9098 - val_accuracy: 0.6642\n",
      "Epoch 522/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4771 - accuracy: 0.8210 - val_loss: 0.8921 - val_accuracy: 0.6765\n",
      "Epoch 523/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4729 - accuracy: 0.8362 - val_loss: 0.8937 - val_accuracy: 0.6691\n",
      "Epoch 524/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4756 - accuracy: 0.8350 - val_loss: 0.9260 - val_accuracy: 0.6778\n",
      "Epoch 525/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4946 - accuracy: 0.8289 - val_loss: 0.8857 - val_accuracy: 0.6765\n",
      "Epoch 526/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.4849 - accuracy: 0.8350 - val_loss: 0.9342 - val_accuracy: 0.6654\n",
      "Epoch 527/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4789 - accuracy: 0.8350 - val_loss: 0.9194 - val_accuracy: 0.6654\n",
      "Epoch 528/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4731 - accuracy: 0.8301 - val_loss: 0.9104 - val_accuracy: 0.6889\n",
      "Epoch 529/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4833 - accuracy: 0.8276 - val_loss: 0.8919 - val_accuracy: 0.6691\n",
      "Epoch 530/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4476 - accuracy: 0.8398 - val_loss: 0.9294 - val_accuracy: 0.6852\n",
      "Epoch 531/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.4827 - accuracy: 0.8289 - val_loss: 0.8866 - val_accuracy: 0.6667\n",
      "Epoch 532/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4656 - accuracy: 0.8356 - val_loss: 0.8919 - val_accuracy: 0.6753\n",
      "Epoch 533/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4629 - accuracy: 0.8490 - val_loss: 0.9033 - val_accuracy: 0.6728\n",
      "Epoch 534/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4787 - accuracy: 0.8380 - val_loss: 0.9146 - val_accuracy: 0.6778\n",
      "Epoch 535/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4718 - accuracy: 0.8307 - val_loss: 0.8852 - val_accuracy: 0.6778\n",
      "Epoch 536/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4748 - accuracy: 0.8307 - val_loss: 0.8914 - val_accuracy: 0.6827\n",
      "Epoch 537/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4684 - accuracy: 0.8295 - val_loss: 0.9016 - val_accuracy: 0.6765\n",
      "Epoch 538/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.4737 - accuracy: 0.8270 - val_loss: 0.9023 - val_accuracy: 0.6642\n",
      "Epoch 539/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.4771 - accuracy: 0.8417 - val_loss: 0.9237 - val_accuracy: 0.6679\n",
      "Epoch 540/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4670 - accuracy: 0.8307 - val_loss: 0.9107 - val_accuracy: 0.6827\n",
      "Epoch 541/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4585 - accuracy: 0.8404 - val_loss: 0.8950 - val_accuracy: 0.6704\n",
      "Epoch 542/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4554 - accuracy: 0.8459 - val_loss: 0.9112 - val_accuracy: 0.6667\n",
      "Epoch 543/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4459 - accuracy: 0.8453 - val_loss: 0.9011 - val_accuracy: 0.6790\n",
      "Epoch 544/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4628 - accuracy: 0.8386 - val_loss: 0.9421 - val_accuracy: 0.6580\n",
      "Epoch 545/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4737 - accuracy: 0.8319 - val_loss: 0.9061 - val_accuracy: 0.6519\n",
      "Epoch 546/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4582 - accuracy: 0.8362 - val_loss: 0.8874 - val_accuracy: 0.6852\n",
      "Epoch 547/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4542 - accuracy: 0.8380 - val_loss: 0.8972 - val_accuracy: 0.6938\n",
      "Epoch 548/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4586 - accuracy: 0.8417 - val_loss: 0.9092 - val_accuracy: 0.6802\n",
      "Epoch 549/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4349 - accuracy: 0.8526 - val_loss: 0.9189 - val_accuracy: 0.6877\n",
      "Epoch 550/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.4590 - accuracy: 0.8447 - val_loss: 0.8859 - val_accuracy: 0.6889\n",
      "Epoch 551/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4499 - accuracy: 0.8441 - val_loss: 0.8996 - val_accuracy: 0.6728\n",
      "Epoch 552/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4659 - accuracy: 0.8270 - val_loss: 0.9005 - val_accuracy: 0.6630\n",
      "Epoch 553/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4463 - accuracy: 0.8471 - val_loss: 0.9096 - val_accuracy: 0.6704\n",
      "Epoch 554/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4390 - accuracy: 0.8471 - val_loss: 0.8961 - val_accuracy: 0.6778\n",
      "Epoch 555/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4549 - accuracy: 0.8319 - val_loss: 0.8957 - val_accuracy: 0.6827\n",
      "Epoch 556/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4657 - accuracy: 0.8417 - val_loss: 0.8952 - val_accuracy: 0.6790\n",
      "Epoch 557/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4522 - accuracy: 0.8362 - val_loss: 0.8915 - val_accuracy: 0.6877\n",
      "Epoch 558/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4508 - accuracy: 0.8343 - val_loss: 0.8894 - val_accuracy: 0.6889\n",
      "Epoch 559/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4647 - accuracy: 0.8380 - val_loss: 0.9037 - val_accuracy: 0.6667\n",
      "Epoch 560/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4551 - accuracy: 0.8471 - val_loss: 0.9241 - val_accuracy: 0.6827\n",
      "Epoch 561/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4557 - accuracy: 0.8447 - val_loss: 0.8880 - val_accuracy: 0.6704\n",
      "Epoch 562/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4413 - accuracy: 0.8459 - val_loss: 0.8848 - val_accuracy: 0.6778\n",
      "Epoch 563/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4296 - accuracy: 0.8496 - val_loss: 0.9004 - val_accuracy: 0.6840\n",
      "Epoch 564/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4383 - accuracy: 0.8453 - val_loss: 0.9081 - val_accuracy: 0.6741\n",
      "Epoch 565/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.4421 - accuracy: 0.8441 - val_loss: 0.8952 - val_accuracy: 0.6827\n",
      "Epoch 566/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4449 - accuracy: 0.8392 - val_loss: 0.8984 - val_accuracy: 0.6765\n",
      "Epoch 567/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4369 - accuracy: 0.8465 - val_loss: 0.9137 - val_accuracy: 0.6815\n",
      "Epoch 568/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4337 - accuracy: 0.8508 - val_loss: 0.8957 - val_accuracy: 0.6864\n",
      "Epoch 569/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4564 - accuracy: 0.8325 - val_loss: 0.8892 - val_accuracy: 0.6926\n",
      "Epoch 570/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4269 - accuracy: 0.8429 - val_loss: 0.9113 - val_accuracy: 0.6654\n",
      "Epoch 571/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4221 - accuracy: 0.8471 - val_loss: 0.8782 - val_accuracy: 0.6852\n",
      "Epoch 572/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4294 - accuracy: 0.8569 - val_loss: 0.9189 - val_accuracy: 0.6802\n",
      "Epoch 573/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4559 - accuracy: 0.8374 - val_loss: 0.8873 - val_accuracy: 0.6926\n",
      "Epoch 574/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4310 - accuracy: 0.8484 - val_loss: 0.9080 - val_accuracy: 0.6679\n",
      "Epoch 575/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4355 - accuracy: 0.8496 - val_loss: 0.9064 - val_accuracy: 0.6840\n",
      "Epoch 576/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4353 - accuracy: 0.8471 - val_loss: 0.9121 - val_accuracy: 0.6840\n",
      "Epoch 577/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4268 - accuracy: 0.8490 - val_loss: 0.9041 - val_accuracy: 0.6741\n",
      "Epoch 578/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4220 - accuracy: 0.8410 - val_loss: 0.9112 - val_accuracy: 0.6642\n",
      "Epoch 579/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4318 - accuracy: 0.8575 - val_loss: 0.9024 - val_accuracy: 0.6802\n",
      "Epoch 580/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4214 - accuracy: 0.8459 - val_loss: 0.8944 - val_accuracy: 0.6889\n",
      "Epoch 581/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4432 - accuracy: 0.8423 - val_loss: 0.8898 - val_accuracy: 0.6840\n",
      "Epoch 582/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4383 - accuracy: 0.8447 - val_loss: 0.9017 - val_accuracy: 0.6728\n",
      "Epoch 583/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4279 - accuracy: 0.8508 - val_loss: 0.8988 - val_accuracy: 0.6753\n",
      "Epoch 584/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4165 - accuracy: 0.8611 - val_loss: 0.8952 - val_accuracy: 0.6815\n",
      "Epoch 585/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4248 - accuracy: 0.8514 - val_loss: 0.9045 - val_accuracy: 0.6716\n",
      "Epoch 586/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4198 - accuracy: 0.8508 - val_loss: 0.8915 - val_accuracy: 0.6765\n",
      "Epoch 587/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.4228 - accuracy: 0.8514 - val_loss: 0.8953 - val_accuracy: 0.6790\n",
      "Epoch 588/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4255 - accuracy: 0.8563 - val_loss: 0.9132 - val_accuracy: 0.6790\n",
      "Epoch 589/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.4282 - accuracy: 0.8423 - val_loss: 0.9022 - val_accuracy: 0.6704\n",
      "Epoch 590/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4164 - accuracy: 0.8569 - val_loss: 0.8975 - val_accuracy: 0.6877\n",
      "Epoch 591/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4289 - accuracy: 0.8435 - val_loss: 0.8997 - val_accuracy: 0.6938\n",
      "Epoch 592/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.4056 - accuracy: 0.8575 - val_loss: 0.8894 - val_accuracy: 0.6877\n",
      "Epoch 593/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4363 - accuracy: 0.8398 - val_loss: 0.8980 - val_accuracy: 0.6802\n",
      "Epoch 594/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4208 - accuracy: 0.8520 - val_loss: 0.9192 - val_accuracy: 0.6654\n",
      "Epoch 595/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.4223 - accuracy: 0.8581 - val_loss: 0.8758 - val_accuracy: 0.6889\n",
      "Epoch 596/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4247 - accuracy: 0.8386 - val_loss: 0.8752 - val_accuracy: 0.6840\n",
      "Epoch 597/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4285 - accuracy: 0.8508 - val_loss: 0.9017 - val_accuracy: 0.6802\n",
      "Epoch 598/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4279 - accuracy: 0.8599 - val_loss: 0.8930 - val_accuracy: 0.6728\n",
      "Epoch 599/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4121 - accuracy: 0.8563 - val_loss: 0.9055 - val_accuracy: 0.6877\n",
      "Epoch 600/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4154 - accuracy: 0.8551 - val_loss: 0.8922 - val_accuracy: 0.6716\n",
      "Epoch 601/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3942 - accuracy: 0.8727 - val_loss: 0.8991 - val_accuracy: 0.6914\n",
      "Epoch 602/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4330 - accuracy: 0.8465 - val_loss: 0.9005 - val_accuracy: 0.6840\n",
      "Epoch 603/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4098 - accuracy: 0.8575 - val_loss: 0.9014 - val_accuracy: 0.6753\n",
      "Epoch 604/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4134 - accuracy: 0.8477 - val_loss: 0.9092 - val_accuracy: 0.6679\n",
      "Epoch 605/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.4125 - accuracy: 0.8569 - val_loss: 0.9208 - val_accuracy: 0.6704\n",
      "Epoch 606/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4065 - accuracy: 0.8593 - val_loss: 0.8972 - val_accuracy: 0.6802\n",
      "Epoch 607/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4162 - accuracy: 0.8514 - val_loss: 0.8909 - val_accuracy: 0.6827\n",
      "Epoch 608/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.4212 - accuracy: 0.8477 - val_loss: 0.8864 - val_accuracy: 0.6914\n",
      "Epoch 609/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.4079 - accuracy: 0.8654 - val_loss: 0.8901 - val_accuracy: 0.6790\n",
      "Epoch 610/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4057 - accuracy: 0.8575 - val_loss: 0.9178 - val_accuracy: 0.6580\n",
      "Epoch 611/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.4019 - accuracy: 0.8685 - val_loss: 0.9146 - val_accuracy: 0.6877\n",
      "Epoch 612/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4080 - accuracy: 0.8575 - val_loss: 0.8990 - val_accuracy: 0.6765\n",
      "Epoch 613/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3979 - accuracy: 0.8672 - val_loss: 0.9069 - val_accuracy: 0.6864\n",
      "Epoch 614/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3994 - accuracy: 0.8630 - val_loss: 0.8965 - val_accuracy: 0.6864\n",
      "Epoch 615/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.4063 - accuracy: 0.8532 - val_loss: 0.8793 - val_accuracy: 0.6852\n",
      "Epoch 616/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3959 - accuracy: 0.8703 - val_loss: 0.9072 - val_accuracy: 0.6802\n",
      "Epoch 617/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3968 - accuracy: 0.8605 - val_loss: 0.8934 - val_accuracy: 0.6815\n",
      "Epoch 618/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3777 - accuracy: 0.8752 - val_loss: 0.9235 - val_accuracy: 0.6827\n",
      "Epoch 619/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.4035 - accuracy: 0.8630 - val_loss: 0.9043 - val_accuracy: 0.6568\n",
      "Epoch 620/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3934 - accuracy: 0.8599 - val_loss: 0.8887 - val_accuracy: 0.6901\n",
      "Epoch 621/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.4016 - accuracy: 0.8599 - val_loss: 0.9142 - val_accuracy: 0.6802\n",
      "Epoch 622/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.4046 - accuracy: 0.8581 - val_loss: 0.8863 - val_accuracy: 0.6877\n",
      "Epoch 623/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3829 - accuracy: 0.8648 - val_loss: 0.8898 - val_accuracy: 0.6877\n",
      "Epoch 624/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3826 - accuracy: 0.8666 - val_loss: 0.8826 - val_accuracy: 0.6827\n",
      "Epoch 625/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3967 - accuracy: 0.8502 - val_loss: 0.8983 - val_accuracy: 0.6975\n",
      "Epoch 626/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3941 - accuracy: 0.8691 - val_loss: 0.9042 - val_accuracy: 0.6765\n",
      "Epoch 627/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3964 - accuracy: 0.8569 - val_loss: 0.9328 - val_accuracy: 0.6815\n",
      "Epoch 628/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3916 - accuracy: 0.8654 - val_loss: 0.8897 - val_accuracy: 0.6963\n",
      "Epoch 629/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3973 - accuracy: 0.8642 - val_loss: 0.8973 - val_accuracy: 0.6901\n",
      "Epoch 630/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3927 - accuracy: 0.8642 - val_loss: 0.9112 - val_accuracy: 0.6877\n",
      "Epoch 631/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3969 - accuracy: 0.8691 - val_loss: 0.9154 - val_accuracy: 0.6802\n",
      "Epoch 632/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3917 - accuracy: 0.8672 - val_loss: 0.8888 - val_accuracy: 0.6864\n",
      "Epoch 633/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3738 - accuracy: 0.8678 - val_loss: 0.8962 - val_accuracy: 0.7000\n",
      "Epoch 634/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3995 - accuracy: 0.8630 - val_loss: 0.9175 - val_accuracy: 0.6778\n",
      "Epoch 635/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3845 - accuracy: 0.8526 - val_loss: 0.9012 - val_accuracy: 0.6914\n",
      "Epoch 636/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3990 - accuracy: 0.8685 - val_loss: 0.8787 - val_accuracy: 0.6963\n",
      "Epoch 637/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3784 - accuracy: 0.8745 - val_loss: 0.8940 - val_accuracy: 0.6840\n",
      "Epoch 638/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.4069 - accuracy: 0.8575 - val_loss: 0.8861 - val_accuracy: 0.6938\n",
      "Epoch 639/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3986 - accuracy: 0.8697 - val_loss: 0.8986 - val_accuracy: 0.6827\n",
      "Epoch 640/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3860 - accuracy: 0.8618 - val_loss: 0.9275 - val_accuracy: 0.6667\n",
      "Epoch 641/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3765 - accuracy: 0.8703 - val_loss: 0.8993 - val_accuracy: 0.6975\n",
      "Epoch 642/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3860 - accuracy: 0.8618 - val_loss: 0.8883 - val_accuracy: 0.7074\n",
      "Epoch 643/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3779 - accuracy: 0.8636 - val_loss: 0.9082 - val_accuracy: 0.6914\n",
      "Epoch 644/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3993 - accuracy: 0.8624 - val_loss: 0.8905 - val_accuracy: 0.6889\n",
      "Epoch 645/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3942 - accuracy: 0.8648 - val_loss: 0.9115 - val_accuracy: 0.6864\n",
      "Epoch 646/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3906 - accuracy: 0.8660 - val_loss: 0.9106 - val_accuracy: 0.6951\n",
      "Epoch 647/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3817 - accuracy: 0.8611 - val_loss: 0.8840 - val_accuracy: 0.6938\n",
      "Epoch 648/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3911 - accuracy: 0.8642 - val_loss: 0.8961 - val_accuracy: 0.6827\n",
      "Epoch 649/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3854 - accuracy: 0.8630 - val_loss: 0.9089 - val_accuracy: 0.6827\n",
      "Epoch 650/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3943 - accuracy: 0.8599 - val_loss: 0.8952 - val_accuracy: 0.6827\n",
      "Epoch 651/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3798 - accuracy: 0.8745 - val_loss: 0.9096 - val_accuracy: 0.6852\n",
      "Epoch 652/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3719 - accuracy: 0.8727 - val_loss: 0.9013 - val_accuracy: 0.6864\n",
      "Epoch 653/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3721 - accuracy: 0.8685 - val_loss: 0.9057 - val_accuracy: 0.6815\n",
      "Epoch 654/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3734 - accuracy: 0.8715 - val_loss: 0.8783 - val_accuracy: 0.6889\n",
      "Epoch 655/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3640 - accuracy: 0.8770 - val_loss: 0.9023 - val_accuracy: 0.6815\n",
      "Epoch 656/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3739 - accuracy: 0.8703 - val_loss: 0.9336 - val_accuracy: 0.6790\n",
      "Epoch 657/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3804 - accuracy: 0.8642 - val_loss: 0.8988 - val_accuracy: 0.6889\n",
      "Epoch 658/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3633 - accuracy: 0.8727 - val_loss: 0.9046 - val_accuracy: 0.6914\n",
      "Epoch 659/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3786 - accuracy: 0.8648 - val_loss: 0.8892 - val_accuracy: 0.6926\n",
      "Epoch 660/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3530 - accuracy: 0.8849 - val_loss: 0.9110 - val_accuracy: 0.6852\n",
      "Epoch 661/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3894 - accuracy: 0.8544 - val_loss: 0.9046 - val_accuracy: 0.6889\n",
      "Epoch 662/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3867 - accuracy: 0.8703 - val_loss: 0.9015 - val_accuracy: 0.6877\n",
      "Epoch 663/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3782 - accuracy: 0.8691 - val_loss: 0.8828 - val_accuracy: 0.6951\n",
      "Epoch 664/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3480 - accuracy: 0.8758 - val_loss: 0.9169 - val_accuracy: 0.6926\n",
      "Epoch 665/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3650 - accuracy: 0.8770 - val_loss: 0.9060 - val_accuracy: 0.6753\n",
      "Epoch 666/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3585 - accuracy: 0.8764 - val_loss: 0.9015 - val_accuracy: 0.6951\n",
      "Epoch 667/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3560 - accuracy: 0.8800 - val_loss: 0.9003 - val_accuracy: 0.6889\n",
      "Epoch 668/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3734 - accuracy: 0.8733 - val_loss: 0.9061 - val_accuracy: 0.6864\n",
      "Epoch 669/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3532 - accuracy: 0.8800 - val_loss: 0.8942 - val_accuracy: 0.6975\n",
      "Epoch 670/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3455 - accuracy: 0.8825 - val_loss: 0.9284 - val_accuracy: 0.6642\n",
      "Epoch 671/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3641 - accuracy: 0.8685 - val_loss: 0.8931 - val_accuracy: 0.6914\n",
      "Epoch 672/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3783 - accuracy: 0.8599 - val_loss: 0.8975 - val_accuracy: 0.7062\n",
      "Epoch 673/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3751 - accuracy: 0.8660 - val_loss: 0.8917 - val_accuracy: 0.6877\n",
      "Epoch 674/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3605 - accuracy: 0.8776 - val_loss: 0.9152 - val_accuracy: 0.6864\n",
      "Epoch 675/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3652 - accuracy: 0.8727 - val_loss: 0.9492 - val_accuracy: 0.6827\n",
      "Epoch 676/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3589 - accuracy: 0.8685 - val_loss: 0.9033 - val_accuracy: 0.6975\n",
      "Epoch 677/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3489 - accuracy: 0.8861 - val_loss: 0.9245 - val_accuracy: 0.6852\n",
      "Epoch 678/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3472 - accuracy: 0.8764 - val_loss: 0.9169 - val_accuracy: 0.6790\n",
      "Epoch 679/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3531 - accuracy: 0.8745 - val_loss: 0.8859 - val_accuracy: 0.6938\n",
      "Epoch 680/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3441 - accuracy: 0.8898 - val_loss: 0.9087 - val_accuracy: 0.6914\n",
      "Epoch 681/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3630 - accuracy: 0.8691 - val_loss: 0.8897 - val_accuracy: 0.7025\n",
      "Epoch 682/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.3741 - accuracy: 0.8685 - val_loss: 0.8966 - val_accuracy: 0.7025\n",
      "Epoch 683/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3724 - accuracy: 0.8685 - val_loss: 0.8927 - val_accuracy: 0.6938\n",
      "Epoch 684/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3575 - accuracy: 0.8764 - val_loss: 0.8956 - val_accuracy: 0.6840\n",
      "Epoch 685/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3506 - accuracy: 0.8855 - val_loss: 0.8885 - val_accuracy: 0.7049\n",
      "Epoch 686/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3462 - accuracy: 0.8831 - val_loss: 0.8952 - val_accuracy: 0.6877\n",
      "Epoch 687/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3364 - accuracy: 0.8819 - val_loss: 0.9046 - val_accuracy: 0.6914\n",
      "Epoch 688/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3605 - accuracy: 0.8758 - val_loss: 0.8914 - val_accuracy: 0.6815\n",
      "Epoch 689/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3530 - accuracy: 0.8806 - val_loss: 0.9045 - val_accuracy: 0.6778\n",
      "Epoch 690/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3512 - accuracy: 0.8812 - val_loss: 0.8938 - val_accuracy: 0.6951\n",
      "Epoch 691/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3449 - accuracy: 0.8831 - val_loss: 0.8700 - val_accuracy: 0.7025\n",
      "Epoch 692/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3403 - accuracy: 0.8837 - val_loss: 0.9093 - val_accuracy: 0.6914\n",
      "Epoch 693/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3257 - accuracy: 0.8928 - val_loss: 0.9034 - val_accuracy: 0.6951\n",
      "Epoch 694/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3608 - accuracy: 0.8739 - val_loss: 0.9022 - val_accuracy: 0.6975\n",
      "Epoch 695/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3525 - accuracy: 0.8812 - val_loss: 0.9065 - val_accuracy: 0.6938\n",
      "Epoch 696/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3353 - accuracy: 0.8922 - val_loss: 0.9055 - val_accuracy: 0.6790\n",
      "Epoch 697/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3457 - accuracy: 0.8745 - val_loss: 0.9045 - val_accuracy: 0.6951\n",
      "Epoch 698/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3319 - accuracy: 0.8825 - val_loss: 0.9067 - val_accuracy: 0.6827\n",
      "Epoch 699/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3413 - accuracy: 0.8867 - val_loss: 0.8768 - val_accuracy: 0.6914\n",
      "Epoch 700/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3355 - accuracy: 0.8800 - val_loss: 0.9002 - val_accuracy: 0.6963\n",
      "Epoch 701/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3500 - accuracy: 0.8618 - val_loss: 0.9079 - val_accuracy: 0.6975\n",
      "Epoch 702/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3361 - accuracy: 0.8904 - val_loss: 0.8900 - val_accuracy: 0.6963\n",
      "Epoch 703/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3326 - accuracy: 0.8837 - val_loss: 0.8955 - val_accuracy: 0.6889\n",
      "Epoch 704/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3353 - accuracy: 0.8782 - val_loss: 0.9226 - val_accuracy: 0.6753\n",
      "Epoch 705/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3420 - accuracy: 0.8800 - val_loss: 0.9136 - val_accuracy: 0.6901\n",
      "Epoch 706/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3431 - accuracy: 0.8898 - val_loss: 0.9235 - val_accuracy: 0.6914\n",
      "Epoch 707/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3270 - accuracy: 0.8849 - val_loss: 0.9340 - val_accuracy: 0.6741\n",
      "Epoch 708/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3239 - accuracy: 0.8922 - val_loss: 0.9231 - val_accuracy: 0.6901\n",
      "Epoch 709/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3275 - accuracy: 0.8843 - val_loss: 0.9102 - val_accuracy: 0.6852\n",
      "Epoch 710/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3196 - accuracy: 0.8959 - val_loss: 0.9070 - val_accuracy: 0.7025\n",
      "Epoch 711/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3349 - accuracy: 0.8837 - val_loss: 0.9401 - val_accuracy: 0.6790\n",
      "Epoch 712/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3316 - accuracy: 0.8764 - val_loss: 0.9296 - val_accuracy: 0.6963\n",
      "Epoch 713/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3464 - accuracy: 0.8800 - val_loss: 0.9026 - val_accuracy: 0.7012\n",
      "Epoch 714/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3194 - accuracy: 0.8916 - val_loss: 0.9045 - val_accuracy: 0.6901\n",
      "Epoch 715/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3588 - accuracy: 0.8654 - val_loss: 0.9032 - val_accuracy: 0.6975\n",
      "Epoch 716/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3363 - accuracy: 0.8867 - val_loss: 0.9141 - val_accuracy: 0.6790\n",
      "Epoch 717/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.3319 - accuracy: 0.8776 - val_loss: 0.9061 - val_accuracy: 0.6963\n",
      "Epoch 718/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3340 - accuracy: 0.8837 - val_loss: 0.9098 - val_accuracy: 0.7074\n",
      "Epoch 719/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.3383 - accuracy: 0.8812 - val_loss: 0.9223 - val_accuracy: 0.6901\n",
      "Epoch 720/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3197 - accuracy: 0.8916 - val_loss: 0.9097 - val_accuracy: 0.6975\n",
      "Epoch 721/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3221 - accuracy: 0.8855 - val_loss: 0.9078 - val_accuracy: 0.6951\n",
      "Epoch 722/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3304 - accuracy: 0.8831 - val_loss: 0.9046 - val_accuracy: 0.7037\n",
      "Epoch 723/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3272 - accuracy: 0.8837 - val_loss: 0.9262 - val_accuracy: 0.6889\n",
      "Epoch 724/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3222 - accuracy: 0.8879 - val_loss: 0.9257 - val_accuracy: 0.6951\n",
      "Epoch 725/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3274 - accuracy: 0.8837 - val_loss: 0.9014 - val_accuracy: 0.6901\n",
      "Epoch 726/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3339 - accuracy: 0.8825 - val_loss: 0.8892 - val_accuracy: 0.7000\n",
      "Epoch 727/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3079 - accuracy: 0.8892 - val_loss: 0.8912 - val_accuracy: 0.6877\n",
      "Epoch 728/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3382 - accuracy: 0.8837 - val_loss: 0.8849 - val_accuracy: 0.6988\n",
      "Epoch 729/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3273 - accuracy: 0.8855 - val_loss: 0.9005 - val_accuracy: 0.6852\n",
      "Epoch 730/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3352 - accuracy: 0.8861 - val_loss: 0.9079 - val_accuracy: 0.6951\n",
      "Epoch 731/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3415 - accuracy: 0.8861 - val_loss: 0.9260 - val_accuracy: 0.7000\n",
      "Epoch 732/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3073 - accuracy: 0.8971 - val_loss: 0.8986 - val_accuracy: 0.6852\n",
      "Epoch 733/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3322 - accuracy: 0.8788 - val_loss: 0.9059 - val_accuracy: 0.6753\n",
      "Epoch 734/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3200 - accuracy: 0.8855 - val_loss: 0.8954 - val_accuracy: 0.6963\n",
      "Epoch 735/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3011 - accuracy: 0.8983 - val_loss: 0.9047 - val_accuracy: 0.6926\n",
      "Epoch 736/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3230 - accuracy: 0.8873 - val_loss: 0.8850 - val_accuracy: 0.6889\n",
      "Epoch 737/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3193 - accuracy: 0.8916 - val_loss: 0.9161 - val_accuracy: 0.6901\n",
      "Epoch 738/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3066 - accuracy: 0.8971 - val_loss: 0.9164 - val_accuracy: 0.7037\n",
      "Epoch 739/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3425 - accuracy: 0.8806 - val_loss: 0.9083 - val_accuracy: 0.6963\n",
      "Epoch 740/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3250 - accuracy: 0.8898 - val_loss: 0.8946 - val_accuracy: 0.6988\n",
      "Epoch 741/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3002 - accuracy: 0.8971 - val_loss: 0.9335 - val_accuracy: 0.6679\n",
      "Epoch 742/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3277 - accuracy: 0.8873 - val_loss: 0.9342 - val_accuracy: 0.7086\n",
      "Epoch 743/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3105 - accuracy: 0.8946 - val_loss: 0.9462 - val_accuracy: 0.6802\n",
      "Epoch 744/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.3034 - accuracy: 0.8959 - val_loss: 0.9193 - val_accuracy: 0.6901\n",
      "Epoch 745/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3067 - accuracy: 0.8928 - val_loss: 0.8940 - val_accuracy: 0.6926\n",
      "Epoch 746/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3158 - accuracy: 0.8916 - val_loss: 0.8988 - val_accuracy: 0.6963\n",
      "Epoch 747/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3159 - accuracy: 0.8904 - val_loss: 0.9219 - val_accuracy: 0.6951\n",
      "Epoch 748/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3055 - accuracy: 0.8989 - val_loss: 0.9255 - val_accuracy: 0.6926\n",
      "Epoch 749/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3107 - accuracy: 0.8898 - val_loss: 0.8881 - val_accuracy: 0.6988\n",
      "Epoch 750/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3219 - accuracy: 0.8928 - val_loss: 0.8997 - val_accuracy: 0.7099\n",
      "Epoch 751/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3126 - accuracy: 0.8886 - val_loss: 0.8978 - val_accuracy: 0.6963\n",
      "Epoch 752/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3125 - accuracy: 0.8886 - val_loss: 0.9099 - val_accuracy: 0.6951\n",
      "Epoch 753/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3242 - accuracy: 0.8867 - val_loss: 0.9233 - val_accuracy: 0.6963\n",
      "Epoch 754/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3052 - accuracy: 0.8946 - val_loss: 0.9434 - val_accuracy: 0.6852\n",
      "Epoch 755/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.3057 - accuracy: 0.8989 - val_loss: 0.9265 - val_accuracy: 0.6914\n",
      "Epoch 756/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2997 - accuracy: 0.8946 - val_loss: 0.9152 - val_accuracy: 0.6975\n",
      "Epoch 757/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3125 - accuracy: 0.8928 - val_loss: 0.9091 - val_accuracy: 0.7099\n",
      "Epoch 758/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3067 - accuracy: 0.8946 - val_loss: 0.9146 - val_accuracy: 0.7037\n",
      "Epoch 759/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3132 - accuracy: 0.8959 - val_loss: 0.9230 - val_accuracy: 0.7000\n",
      "Epoch 760/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.3079 - accuracy: 0.8952 - val_loss: 0.8911 - val_accuracy: 0.7012\n",
      "Epoch 761/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3054 - accuracy: 0.8946 - val_loss: 0.9215 - val_accuracy: 0.7000\n",
      "Epoch 762/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3060 - accuracy: 0.9026 - val_loss: 0.9128 - val_accuracy: 0.6864\n",
      "Epoch 763/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.3054 - accuracy: 0.9001 - val_loss: 0.9018 - val_accuracy: 0.6975\n",
      "Epoch 764/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.3057 - accuracy: 0.8940 - val_loss: 0.9069 - val_accuracy: 0.6975\n",
      "Epoch 765/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.3078 - accuracy: 0.8965 - val_loss: 0.9259 - val_accuracy: 0.6963\n",
      "Epoch 766/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2822 - accuracy: 0.9044 - val_loss: 0.9224 - val_accuracy: 0.7025\n",
      "Epoch 767/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2994 - accuracy: 0.9001 - val_loss: 0.9259 - val_accuracy: 0.6840\n",
      "Epoch 768/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2930 - accuracy: 0.8989 - val_loss: 0.9108 - val_accuracy: 0.6926\n",
      "Epoch 769/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3101 - accuracy: 0.8922 - val_loss: 0.9345 - val_accuracy: 0.7025\n",
      "Epoch 770/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2960 - accuracy: 0.8983 - val_loss: 0.9024 - val_accuracy: 0.6938\n",
      "Epoch 771/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2948 - accuracy: 0.8995 - val_loss: 0.9244 - val_accuracy: 0.6889\n",
      "Epoch 772/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3050 - accuracy: 0.9019 - val_loss: 0.9102 - val_accuracy: 0.7123\n",
      "Epoch 773/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.3028 - accuracy: 0.8971 - val_loss: 0.8984 - val_accuracy: 0.7111\n",
      "Epoch 774/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2991 - accuracy: 0.8989 - val_loss: 0.9257 - val_accuracy: 0.6864\n",
      "Epoch 775/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2833 - accuracy: 0.9032 - val_loss: 0.9112 - val_accuracy: 0.6914\n",
      "Epoch 776/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2823 - accuracy: 0.9074 - val_loss: 0.9223 - val_accuracy: 0.7012\n",
      "Epoch 777/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2990 - accuracy: 0.8965 - val_loss: 0.9325 - val_accuracy: 0.6840\n",
      "Epoch 778/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.3129 - accuracy: 0.8879 - val_loss: 0.9192 - val_accuracy: 0.6914\n",
      "Epoch 779/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.3030 - accuracy: 0.9001 - val_loss: 0.9207 - val_accuracy: 0.7037\n",
      "Epoch 780/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.3065 - accuracy: 0.9026 - val_loss: 0.9176 - val_accuracy: 0.6988\n",
      "Epoch 781/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2895 - accuracy: 0.9068 - val_loss: 0.9241 - val_accuracy: 0.7086\n",
      "Epoch 782/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2945 - accuracy: 0.8934 - val_loss: 0.9359 - val_accuracy: 0.6975\n",
      "Epoch 783/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2676 - accuracy: 0.9153 - val_loss: 0.9278 - val_accuracy: 0.6852\n",
      "Epoch 784/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2812 - accuracy: 0.9019 - val_loss: 0.9159 - val_accuracy: 0.7000\n",
      "Epoch 785/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2762 - accuracy: 0.9068 - val_loss: 0.9218 - val_accuracy: 0.6889\n",
      "Epoch 786/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2841 - accuracy: 0.9056 - val_loss: 0.9184 - val_accuracy: 0.7062\n",
      "Epoch 787/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2789 - accuracy: 0.9117 - val_loss: 0.9220 - val_accuracy: 0.7086\n",
      "Epoch 788/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2910 - accuracy: 0.8940 - val_loss: 0.9370 - val_accuracy: 0.7074\n",
      "Epoch 789/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2884 - accuracy: 0.8977 - val_loss: 0.9497 - val_accuracy: 0.6951\n",
      "Epoch 790/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2975 - accuracy: 0.8940 - val_loss: 0.9467 - val_accuracy: 0.6988\n",
      "Epoch 791/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.3023 - accuracy: 0.8952 - val_loss: 0.9359 - val_accuracy: 0.7049\n",
      "Epoch 792/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2970 - accuracy: 0.8959 - val_loss: 0.9067 - val_accuracy: 0.7037\n",
      "Epoch 793/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2746 - accuracy: 0.9068 - val_loss: 0.9272 - val_accuracy: 0.7062\n",
      "Epoch 794/1000\n",
      "103/103 [==============================] - 2s 16ms/step - loss: 0.2981 - accuracy: 0.8934 - val_loss: 0.9267 - val_accuracy: 0.6938\n",
      "Epoch 795/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2848 - accuracy: 0.9044 - val_loss: 0.9374 - val_accuracy: 0.6864\n",
      "Epoch 796/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2737 - accuracy: 0.9062 - val_loss: 0.9419 - val_accuracy: 0.6975\n",
      "Epoch 797/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2850 - accuracy: 0.9111 - val_loss: 0.9326 - val_accuracy: 0.6914\n",
      "Epoch 798/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2811 - accuracy: 0.9099 - val_loss: 0.8970 - val_accuracy: 0.7111\n",
      "Epoch 799/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2804 - accuracy: 0.9026 - val_loss: 0.9065 - val_accuracy: 0.7012\n",
      "Epoch 800/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2639 - accuracy: 0.9184 - val_loss: 0.9174 - val_accuracy: 0.7049\n",
      "Epoch 801/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2841 - accuracy: 0.9026 - val_loss: 0.8989 - val_accuracy: 0.6938\n",
      "Epoch 802/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2809 - accuracy: 0.8995 - val_loss: 0.9012 - val_accuracy: 0.6988\n",
      "Epoch 803/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2867 - accuracy: 0.9050 - val_loss: 0.9088 - val_accuracy: 0.6889\n",
      "Epoch 804/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2932 - accuracy: 0.9001 - val_loss: 0.9122 - val_accuracy: 0.6852\n",
      "Epoch 805/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2844 - accuracy: 0.9019 - val_loss: 0.9363 - val_accuracy: 0.7025\n",
      "Epoch 806/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2740 - accuracy: 0.9013 - val_loss: 0.9365 - val_accuracy: 0.7099\n",
      "Epoch 807/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2844 - accuracy: 0.9013 - val_loss: 0.9074 - val_accuracy: 0.6889\n",
      "Epoch 808/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2804 - accuracy: 0.9062 - val_loss: 0.9024 - val_accuracy: 0.7000\n",
      "Epoch 809/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2835 - accuracy: 0.8995 - val_loss: 0.8970 - val_accuracy: 0.7000\n",
      "Epoch 810/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2712 - accuracy: 0.9080 - val_loss: 0.9200 - val_accuracy: 0.6975\n",
      "Epoch 811/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2799 - accuracy: 0.9117 - val_loss: 0.9092 - val_accuracy: 0.6963\n",
      "Epoch 812/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2848 - accuracy: 0.8965 - val_loss: 0.8877 - val_accuracy: 0.7049\n",
      "Epoch 813/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2692 - accuracy: 0.9141 - val_loss: 0.9071 - val_accuracy: 0.6975\n",
      "Epoch 814/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2776 - accuracy: 0.9038 - val_loss: 0.9239 - val_accuracy: 0.7074\n",
      "Epoch 815/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2806 - accuracy: 0.9019 - val_loss: 0.9047 - val_accuracy: 0.7025\n",
      "Epoch 816/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2697 - accuracy: 0.9111 - val_loss: 0.9374 - val_accuracy: 0.6963\n",
      "Epoch 817/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2889 - accuracy: 0.9044 - val_loss: 0.9189 - val_accuracy: 0.7025\n",
      "Epoch 818/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2748 - accuracy: 0.9038 - val_loss: 0.8973 - val_accuracy: 0.7160\n",
      "Epoch 819/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2744 - accuracy: 0.8995 - val_loss: 0.9363 - val_accuracy: 0.6877\n",
      "Epoch 820/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2756 - accuracy: 0.8989 - val_loss: 0.9234 - val_accuracy: 0.6988\n",
      "Epoch 821/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2714 - accuracy: 0.9111 - val_loss: 0.9111 - val_accuracy: 0.7062\n",
      "Epoch 822/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2780 - accuracy: 0.9129 - val_loss: 0.9158 - val_accuracy: 0.6926\n",
      "Epoch 823/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2775 - accuracy: 0.9007 - val_loss: 0.9320 - val_accuracy: 0.6889\n",
      "Epoch 824/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2590 - accuracy: 0.9147 - val_loss: 0.9277 - val_accuracy: 0.7037\n",
      "Epoch 825/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2758 - accuracy: 0.9032 - val_loss: 0.9257 - val_accuracy: 0.7136\n",
      "Epoch 826/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2811 - accuracy: 0.8983 - val_loss: 0.9157 - val_accuracy: 0.7025\n",
      "Epoch 827/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2849 - accuracy: 0.8898 - val_loss: 0.9100 - val_accuracy: 0.6877\n",
      "Epoch 828/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2707 - accuracy: 0.9147 - val_loss: 0.9272 - val_accuracy: 0.7086\n",
      "Epoch 829/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2863 - accuracy: 0.8989 - val_loss: 0.9226 - val_accuracy: 0.6963\n",
      "Epoch 830/1000\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.2659 - accuracy: 0.9093 - val_loss: 0.9504 - val_accuracy: 0.6988\n",
      "Epoch 831/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2773 - accuracy: 0.9038 - val_loss: 0.9095 - val_accuracy: 0.7074\n",
      "Epoch 832/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2689 - accuracy: 0.9044 - val_loss: 0.9244 - val_accuracy: 0.7086\n",
      "Epoch 833/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2742 - accuracy: 0.9013 - val_loss: 0.9385 - val_accuracy: 0.6963\n",
      "Epoch 834/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2690 - accuracy: 0.9099 - val_loss: 0.9322 - val_accuracy: 0.6877\n",
      "Epoch 835/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2809 - accuracy: 0.9019 - val_loss: 0.9155 - val_accuracy: 0.6975\n",
      "Epoch 836/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2672 - accuracy: 0.9093 - val_loss: 0.9132 - val_accuracy: 0.7111\n",
      "Epoch 837/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2651 - accuracy: 0.9123 - val_loss: 0.9331 - val_accuracy: 0.7111\n",
      "Epoch 838/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2720 - accuracy: 0.9086 - val_loss: 0.9335 - val_accuracy: 0.7025\n",
      "Epoch 839/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2508 - accuracy: 0.9117 - val_loss: 0.9256 - val_accuracy: 0.7037\n",
      "Epoch 840/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2604 - accuracy: 0.9117 - val_loss: 0.9427 - val_accuracy: 0.6963\n",
      "Epoch 841/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2586 - accuracy: 0.9135 - val_loss: 0.9179 - val_accuracy: 0.6889\n",
      "Epoch 842/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2622 - accuracy: 0.9074 - val_loss: 0.9282 - val_accuracy: 0.7062\n",
      "Epoch 843/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2664 - accuracy: 0.9129 - val_loss: 0.9326 - val_accuracy: 0.6951\n",
      "Epoch 844/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2527 - accuracy: 0.9202 - val_loss: 0.9253 - val_accuracy: 0.6975\n",
      "Epoch 845/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2452 - accuracy: 0.9160 - val_loss: 0.9191 - val_accuracy: 0.7086\n",
      "Epoch 846/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2713 - accuracy: 0.9093 - val_loss: 0.9389 - val_accuracy: 0.7099\n",
      "Epoch 847/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2623 - accuracy: 0.9099 - val_loss: 0.9142 - val_accuracy: 0.7049\n",
      "Epoch 848/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2561 - accuracy: 0.9117 - val_loss: 0.9542 - val_accuracy: 0.6778\n",
      "Epoch 849/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2734 - accuracy: 0.9019 - val_loss: 0.9497 - val_accuracy: 0.6852\n",
      "Epoch 850/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2666 - accuracy: 0.9099 - val_loss: 0.9111 - val_accuracy: 0.6975\n",
      "Epoch 851/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2633 - accuracy: 0.9147 - val_loss: 0.9320 - val_accuracy: 0.6889\n",
      "Epoch 852/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2609 - accuracy: 0.9068 - val_loss: 0.9113 - val_accuracy: 0.7025\n",
      "Epoch 853/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2506 - accuracy: 0.9196 - val_loss: 0.9271 - val_accuracy: 0.7012\n",
      "Epoch 854/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2663 - accuracy: 0.9074 - val_loss: 0.9219 - val_accuracy: 0.7049\n",
      "Epoch 855/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2630 - accuracy: 0.9038 - val_loss: 0.9193 - val_accuracy: 0.7037\n",
      "Epoch 856/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2676 - accuracy: 0.9044 - val_loss: 0.9292 - val_accuracy: 0.7086\n",
      "Epoch 857/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2623 - accuracy: 0.9093 - val_loss: 0.9471 - val_accuracy: 0.6951\n",
      "Epoch 858/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2618 - accuracy: 0.9050 - val_loss: 0.9216 - val_accuracy: 0.7012\n",
      "Epoch 859/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2470 - accuracy: 0.9153 - val_loss: 0.9292 - val_accuracy: 0.7074\n",
      "Epoch 860/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2620 - accuracy: 0.9135 - val_loss: 0.9307 - val_accuracy: 0.7000\n",
      "Epoch 861/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2541 - accuracy: 0.9141 - val_loss: 0.9172 - val_accuracy: 0.7037\n",
      "Epoch 862/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2619 - accuracy: 0.9135 - val_loss: 0.9270 - val_accuracy: 0.7000\n",
      "Epoch 863/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2365 - accuracy: 0.9263 - val_loss: 0.9508 - val_accuracy: 0.6926\n",
      "Epoch 864/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2594 - accuracy: 0.9038 - val_loss: 0.9455 - val_accuracy: 0.6864\n",
      "Epoch 865/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2523 - accuracy: 0.9153 - val_loss: 0.9317 - val_accuracy: 0.7111\n",
      "Epoch 866/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2694 - accuracy: 0.9080 - val_loss: 0.9155 - val_accuracy: 0.7037\n",
      "Epoch 867/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2574 - accuracy: 0.9129 - val_loss: 0.9318 - val_accuracy: 0.7037\n",
      "Epoch 868/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2635 - accuracy: 0.9160 - val_loss: 0.9165 - val_accuracy: 0.7099\n",
      "Epoch 869/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2470 - accuracy: 0.9214 - val_loss: 0.9196 - val_accuracy: 0.7025\n",
      "Epoch 870/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2335 - accuracy: 0.9178 - val_loss: 0.9487 - val_accuracy: 0.6864\n",
      "Epoch 871/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2571 - accuracy: 0.9074 - val_loss: 0.9418 - val_accuracy: 0.7000\n",
      "Epoch 872/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2408 - accuracy: 0.9141 - val_loss: 0.9376 - val_accuracy: 0.7173\n",
      "Epoch 873/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2403 - accuracy: 0.9190 - val_loss: 0.9173 - val_accuracy: 0.7025\n",
      "Epoch 874/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2413 - accuracy: 0.9220 - val_loss: 0.9688 - val_accuracy: 0.6877\n",
      "Epoch 875/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2428 - accuracy: 0.9196 - val_loss: 0.9509 - val_accuracy: 0.7025\n",
      "Epoch 876/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2329 - accuracy: 0.9239 - val_loss: 0.9497 - val_accuracy: 0.6988\n",
      "Epoch 877/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2324 - accuracy: 0.9251 - val_loss: 0.9396 - val_accuracy: 0.7074\n",
      "Epoch 878/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2431 - accuracy: 0.9141 - val_loss: 0.9503 - val_accuracy: 0.7037\n",
      "Epoch 879/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2439 - accuracy: 0.9172 - val_loss: 0.9303 - val_accuracy: 0.7037\n",
      "Epoch 880/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2417 - accuracy: 0.9220 - val_loss: 0.9560 - val_accuracy: 0.7025\n",
      "Epoch 881/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2535 - accuracy: 0.9056 - val_loss: 0.9236 - val_accuracy: 0.7062\n",
      "Epoch 882/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2359 - accuracy: 0.9184 - val_loss: 0.9355 - val_accuracy: 0.7074\n",
      "Epoch 883/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2329 - accuracy: 0.9184 - val_loss: 0.9516 - val_accuracy: 0.7012\n",
      "Epoch 884/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2534 - accuracy: 0.9123 - val_loss: 0.9275 - val_accuracy: 0.7148\n",
      "Epoch 885/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2436 - accuracy: 0.9208 - val_loss: 0.9262 - val_accuracy: 0.6963\n",
      "Epoch 886/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2410 - accuracy: 0.9160 - val_loss: 0.9434 - val_accuracy: 0.7049\n",
      "Epoch 887/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2312 - accuracy: 0.9208 - val_loss: 0.9224 - val_accuracy: 0.7148\n",
      "Epoch 888/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2337 - accuracy: 0.9275 - val_loss: 0.9128 - val_accuracy: 0.7210\n",
      "Epoch 889/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2469 - accuracy: 0.9202 - val_loss: 0.9338 - val_accuracy: 0.7012\n",
      "Epoch 890/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2226 - accuracy: 0.9373 - val_loss: 0.9428 - val_accuracy: 0.7062\n",
      "Epoch 891/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2419 - accuracy: 0.9117 - val_loss: 0.9521 - val_accuracy: 0.6975\n",
      "Epoch 892/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2122 - accuracy: 0.9275 - val_loss: 0.9293 - val_accuracy: 0.7160\n",
      "Epoch 893/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2314 - accuracy: 0.9208 - val_loss: 0.9319 - val_accuracy: 0.7062\n",
      "Epoch 894/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2234 - accuracy: 0.9312 - val_loss: 0.9615 - val_accuracy: 0.6963\n",
      "Epoch 895/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2379 - accuracy: 0.9129 - val_loss: 0.9445 - val_accuracy: 0.6914\n",
      "Epoch 896/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2342 - accuracy: 0.9178 - val_loss: 0.9365 - val_accuracy: 0.6914\n",
      "Epoch 897/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2489 - accuracy: 0.9117 - val_loss: 0.9497 - val_accuracy: 0.7000\n",
      "Epoch 898/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2571 - accuracy: 0.9153 - val_loss: 0.9364 - val_accuracy: 0.6975\n",
      "Epoch 899/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2339 - accuracy: 0.9239 - val_loss: 0.9192 - val_accuracy: 0.6988\n",
      "Epoch 900/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2302 - accuracy: 0.9281 - val_loss: 0.9321 - val_accuracy: 0.7086\n",
      "Epoch 901/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2458 - accuracy: 0.9123 - val_loss: 0.9359 - val_accuracy: 0.7000\n",
      "Epoch 902/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2529 - accuracy: 0.9202 - val_loss: 0.9187 - val_accuracy: 0.7074\n",
      "Epoch 903/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2366 - accuracy: 0.9160 - val_loss: 0.9195 - val_accuracy: 0.7012\n",
      "Epoch 904/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2286 - accuracy: 0.9214 - val_loss: 0.9305 - val_accuracy: 0.7259\n",
      "Epoch 905/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2297 - accuracy: 0.9178 - val_loss: 0.9373 - val_accuracy: 0.7198\n",
      "Epoch 906/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2336 - accuracy: 0.9220 - val_loss: 0.9356 - val_accuracy: 0.7160\n",
      "Epoch 907/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2261 - accuracy: 0.9233 - val_loss: 0.9434 - val_accuracy: 0.6975\n",
      "Epoch 908/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2504 - accuracy: 0.9080 - val_loss: 0.9312 - val_accuracy: 0.7025\n",
      "Epoch 909/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2274 - accuracy: 0.9233 - val_loss: 0.9523 - val_accuracy: 0.6963\n",
      "Epoch 910/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2327 - accuracy: 0.9208 - val_loss: 0.9603 - val_accuracy: 0.6975\n",
      "Epoch 911/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2632 - accuracy: 0.9026 - val_loss: 0.9319 - val_accuracy: 0.7086\n",
      "Epoch 912/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2415 - accuracy: 0.9166 - val_loss: 0.9303 - val_accuracy: 0.7148\n",
      "Epoch 913/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2272 - accuracy: 0.9220 - val_loss: 0.9779 - val_accuracy: 0.6926\n",
      "Epoch 914/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2409 - accuracy: 0.9196 - val_loss: 0.9512 - val_accuracy: 0.6975\n",
      "Epoch 915/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2333 - accuracy: 0.9233 - val_loss: 0.9835 - val_accuracy: 0.7123\n",
      "Epoch 916/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2351 - accuracy: 0.9135 - val_loss: 0.9399 - val_accuracy: 0.7123\n",
      "Epoch 917/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2415 - accuracy: 0.9135 - val_loss: 0.9462 - val_accuracy: 0.7000\n",
      "Epoch 918/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2193 - accuracy: 0.9287 - val_loss: 0.9451 - val_accuracy: 0.6852\n",
      "Epoch 919/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2368 - accuracy: 0.9172 - val_loss: 0.9632 - val_accuracy: 0.6840\n",
      "Epoch 920/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2266 - accuracy: 0.9196 - val_loss: 0.9273 - val_accuracy: 0.7049\n",
      "Epoch 921/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2130 - accuracy: 0.9312 - val_loss: 0.9202 - val_accuracy: 0.7062\n",
      "Epoch 922/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2368 - accuracy: 0.9202 - val_loss: 0.9527 - val_accuracy: 0.7000\n",
      "Epoch 923/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2303 - accuracy: 0.9208 - val_loss: 0.9413 - val_accuracy: 0.7012\n",
      "Epoch 924/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2283 - accuracy: 0.9214 - val_loss: 0.9303 - val_accuracy: 0.7123\n",
      "Epoch 925/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2379 - accuracy: 0.9190 - val_loss: 0.9498 - val_accuracy: 0.7062\n",
      "Epoch 926/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2444 - accuracy: 0.9172 - val_loss: 0.9330 - val_accuracy: 0.7099\n",
      "Epoch 927/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2245 - accuracy: 0.9251 - val_loss: 0.9358 - val_accuracy: 0.7049\n",
      "Epoch 928/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2258 - accuracy: 0.9281 - val_loss: 0.9227 - val_accuracy: 0.7148\n",
      "Epoch 929/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2461 - accuracy: 0.9080 - val_loss: 0.9878 - val_accuracy: 0.6951\n",
      "Epoch 930/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2232 - accuracy: 0.9202 - val_loss: 0.9572 - val_accuracy: 0.7074\n",
      "Epoch 931/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2174 - accuracy: 0.9312 - val_loss: 0.9643 - val_accuracy: 0.6988\n",
      "Epoch 932/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2198 - accuracy: 0.9184 - val_loss: 0.9579 - val_accuracy: 0.7025\n",
      "Epoch 933/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2345 - accuracy: 0.9172 - val_loss: 0.9276 - val_accuracy: 0.7025\n",
      "Epoch 934/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2260 - accuracy: 0.9208 - val_loss: 0.9533 - val_accuracy: 0.7025\n",
      "Epoch 935/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2383 - accuracy: 0.9178 - val_loss: 0.9355 - val_accuracy: 0.7099\n",
      "Epoch 936/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2233 - accuracy: 0.9287 - val_loss: 0.9472 - val_accuracy: 0.7136\n",
      "Epoch 937/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2259 - accuracy: 0.9135 - val_loss: 0.9453 - val_accuracy: 0.7074\n",
      "Epoch 938/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2111 - accuracy: 0.9220 - val_loss: 0.9260 - val_accuracy: 0.7062\n",
      "Epoch 939/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2234 - accuracy: 0.9251 - val_loss: 0.9477 - val_accuracy: 0.7099\n",
      "Epoch 940/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2088 - accuracy: 0.9330 - val_loss: 0.9805 - val_accuracy: 0.7037\n",
      "Epoch 941/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2217 - accuracy: 0.9269 - val_loss: 0.9389 - val_accuracy: 0.6840\n",
      "Epoch 942/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2158 - accuracy: 0.9330 - val_loss: 0.9445 - val_accuracy: 0.7111\n",
      "Epoch 943/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.1964 - accuracy: 0.9361 - val_loss: 0.9501 - val_accuracy: 0.6988\n",
      "Epoch 944/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2233 - accuracy: 0.9251 - val_loss: 0.9666 - val_accuracy: 0.7086\n",
      "Epoch 945/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2093 - accuracy: 0.9348 - val_loss: 0.9419 - val_accuracy: 0.7185\n",
      "Epoch 946/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2210 - accuracy: 0.9306 - val_loss: 0.9300 - val_accuracy: 0.7173\n",
      "Epoch 947/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2278 - accuracy: 0.9312 - val_loss: 0.9523 - val_accuracy: 0.7000\n",
      "Epoch 948/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2125 - accuracy: 0.9214 - val_loss: 0.9356 - val_accuracy: 0.7074\n",
      "Epoch 949/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2097 - accuracy: 0.9281 - val_loss: 0.9575 - val_accuracy: 0.7099\n",
      "Epoch 950/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.1963 - accuracy: 0.9379 - val_loss: 0.9558 - val_accuracy: 0.7173\n",
      "Epoch 951/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2321 - accuracy: 0.9245 - val_loss: 0.9428 - val_accuracy: 0.7173\n",
      "Epoch 952/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2110 - accuracy: 0.9257 - val_loss: 0.9643 - val_accuracy: 0.6864\n",
      "Epoch 953/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2084 - accuracy: 0.9324 - val_loss: 0.9385 - val_accuracy: 0.7099\n",
      "Epoch 954/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2012 - accuracy: 0.9336 - val_loss: 0.9636 - val_accuracy: 0.7037\n",
      "Epoch 955/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2080 - accuracy: 0.9354 - val_loss: 0.9800 - val_accuracy: 0.6926\n",
      "Epoch 956/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2052 - accuracy: 0.9294 - val_loss: 0.9697 - val_accuracy: 0.7012\n",
      "Epoch 957/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2046 - accuracy: 0.9361 - val_loss: 0.9317 - val_accuracy: 0.7173\n",
      "Epoch 958/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2187 - accuracy: 0.9257 - val_loss: 0.9751 - val_accuracy: 0.7123\n",
      "Epoch 959/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2137 - accuracy: 0.9220 - val_loss: 0.9341 - val_accuracy: 0.6963\n",
      "Epoch 960/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2225 - accuracy: 0.9239 - val_loss: 0.9611 - val_accuracy: 0.7099\n",
      "Epoch 961/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2265 - accuracy: 0.9172 - val_loss: 0.9504 - val_accuracy: 0.7185\n",
      "Epoch 962/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2080 - accuracy: 0.9300 - val_loss: 0.9284 - val_accuracy: 0.7111\n",
      "Epoch 963/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2203 - accuracy: 0.9263 - val_loss: 0.9496 - val_accuracy: 0.7136\n",
      "Epoch 964/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2203 - accuracy: 0.9239 - val_loss: 0.9739 - val_accuracy: 0.7123\n",
      "Epoch 965/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2037 - accuracy: 0.9281 - val_loss: 0.9300 - val_accuracy: 0.7086\n",
      "Epoch 966/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.1991 - accuracy: 0.9348 - val_loss: 0.9420 - val_accuracy: 0.7099\n",
      "Epoch 967/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2120 - accuracy: 0.9269 - val_loss: 0.9587 - val_accuracy: 0.6963\n",
      "Epoch 968/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.1997 - accuracy: 0.9324 - val_loss: 0.9754 - val_accuracy: 0.6852\n",
      "Epoch 969/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2040 - accuracy: 0.9348 - val_loss: 0.9538 - val_accuracy: 0.7049\n",
      "Epoch 970/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2046 - accuracy: 0.9300 - val_loss: 0.9628 - val_accuracy: 0.7086\n",
      "Epoch 971/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2145 - accuracy: 0.9312 - val_loss: 0.9545 - val_accuracy: 0.7000\n",
      "Epoch 972/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2071 - accuracy: 0.9275 - val_loss: 0.9637 - val_accuracy: 0.6963\n",
      "Epoch 973/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2277 - accuracy: 0.9220 - val_loss: 0.9198 - val_accuracy: 0.7148\n",
      "Epoch 974/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.1965 - accuracy: 0.9403 - val_loss: 0.9516 - val_accuracy: 0.7012\n",
      "Epoch 975/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2006 - accuracy: 0.9312 - val_loss: 0.9475 - val_accuracy: 0.6938\n",
      "Epoch 976/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2030 - accuracy: 0.9312 - val_loss: 0.9415 - val_accuracy: 0.7247\n",
      "Epoch 977/1000\n",
      "103/103 [==============================] - 1s 12ms/step - loss: 0.2136 - accuracy: 0.9312 - val_loss: 0.9742 - val_accuracy: 0.6951\n",
      "Epoch 978/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2185 - accuracy: 0.9300 - val_loss: 0.9655 - val_accuracy: 0.7148\n",
      "Epoch 979/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.2093 - accuracy: 0.9287 - val_loss: 0.9577 - val_accuracy: 0.7210\n",
      "Epoch 980/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2023 - accuracy: 0.9324 - val_loss: 0.9502 - val_accuracy: 0.7148\n",
      "Epoch 981/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.2137 - accuracy: 0.9196 - val_loss: 0.9601 - val_accuracy: 0.6938\n",
      "Epoch 982/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.1950 - accuracy: 0.9452 - val_loss: 0.9461 - val_accuracy: 0.7049\n",
      "Epoch 983/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.1983 - accuracy: 0.9421 - val_loss: 0.9938 - val_accuracy: 0.7012\n",
      "Epoch 984/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.2263 - accuracy: 0.9239 - val_loss: 0.9587 - val_accuracy: 0.7099\n",
      "Epoch 985/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.1918 - accuracy: 0.9342 - val_loss: 0.9685 - val_accuracy: 0.6926\n",
      "Epoch 986/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2043 - accuracy: 0.9251 - val_loss: 0.9695 - val_accuracy: 0.7074\n",
      "Epoch 987/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.1983 - accuracy: 0.9300 - val_loss: 0.9432 - val_accuracy: 0.6938\n",
      "Epoch 988/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2083 - accuracy: 0.9312 - val_loss: 0.9527 - val_accuracy: 0.6988\n",
      "Epoch 989/1000\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.2115 - accuracy: 0.9300 - val_loss: 0.9500 - val_accuracy: 0.6963\n",
      "Epoch 990/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2029 - accuracy: 0.9391 - val_loss: 0.9626 - val_accuracy: 0.7086\n",
      "Epoch 991/1000\n",
      "103/103 [==============================] - 1s 15ms/step - loss: 0.2066 - accuracy: 0.9300 - val_loss: 0.9639 - val_accuracy: 0.7062\n",
      "Epoch 992/1000\n",
      "103/103 [==============================] - 2s 15ms/step - loss: 0.1981 - accuracy: 0.9354 - val_loss: 0.9577 - val_accuracy: 0.7086\n",
      "Epoch 993/1000\n",
      "103/103 [==============================] - 1s 13ms/step - loss: 0.2096 - accuracy: 0.9324 - val_loss: 0.9477 - val_accuracy: 0.7049\n",
      "Epoch 994/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2016 - accuracy: 0.9300 - val_loss: 0.9598 - val_accuracy: 0.7136\n",
      "Epoch 995/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.2071 - accuracy: 0.9257 - val_loss: 0.9766 - val_accuracy: 0.7037\n",
      "Epoch 996/1000\n",
      "103/103 [==============================] - 1s 14ms/step - loss: 0.2205 - accuracy: 0.9245 - val_loss: 0.9510 - val_accuracy: 0.7074\n",
      "Epoch 997/1000\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.1956 - accuracy: 0.9367 - val_loss: 0.9618 - val_accuracy: 0.7049\n",
      "Epoch 998/1000\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.1814 - accuracy: 0.9367 - val_loss: 0.9951 - val_accuracy: 0.7074\n",
      "Epoch 999/1000\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.2061 - accuracy: 0.9275 - val_loss: 0.9786 - val_accuracy: 0.7086\n",
      "Epoch 1000/1000\n",
      "103/103 [==============================] - 1s 11ms/step - loss: 0.1943 - accuracy: 0.9385 - val_loss: 0.9756 - val_accuracy: 0.6975\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFytY6LDzgJ0"
   },
   "source": [
    "Let's plot the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "TFz4ClZov9gZ",
    "outputId": "e3fdf6e2-f249-4b36-a063-683c88ab705f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vf1W7LgP2DA5"
   },
   "source": [
    "\n",
    "\n",
    "And now let's plot the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "8yyFBt7ASPUe",
    "outputId": "d149ff38-7f2f-4eb4-d62e-08d8683ede2d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['accuracy'])\n",
    "plt.plot(cnnhistory.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gaZONl1mD8XD"
   },
   "source": [
    "Let's now create a classification report to review the f1-score of the model per class.\n",
    "To do so, we have to:\n",
    "- Create a variable predictions that will contain the model.predict_classes outcome\n",
    "- Convert our y_test (array of strings with our classes) to an array of int called new_Ytest, otherwise it will not be comparable to the predictions by the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EO25uIL-9vqx"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_testcnn)\n",
    "predictions=np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1i06grlBBSrn",
    "outputId": "af34893b-827c-4355-a92a-17b53b80ad3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 1, 2, 2, 0, 2, 4, 4, 3, 6, 2, 2, 1, 5, 4, 4, 5, 1, 6, 4,\n",
       "       7, 1, 6, 7, 5, 6, 7, 0, 4, 5, 0, 4, 0, 4, 1, 3, 4, 3, 3, 3, 2, 2,\n",
       "       2, 4, 5, 3, 6, 3, 0, 4, 6, 5, 2, 4, 3, 0, 5, 2, 1, 3, 5, 3, 3, 3,\n",
       "       2, 4, 0, 3, 4, 0, 4, 5, 5, 2, 2, 0, 5, 3, 2, 3, 3, 1, 4, 7, 0, 4,\n",
       "       3, 2, 1, 5, 4, 2, 2, 0, 5, 3, 4, 7, 6, 1, 1, 7, 3, 3, 7, 2, 3, 4,\n",
       "       7, 4, 4, 5, 1, 1, 0, 1, 7, 1, 2, 1, 4, 0, 2, 2, 3, 7, 2, 3, 5, 0,\n",
       "       2, 7, 0, 7, 5, 7, 7, 3, 3, 1, 3, 6, 4, 4, 3, 2, 2, 4, 6, 4, 3, 0,\n",
       "       6, 5, 3, 4, 5, 2, 4, 1, 4, 5, 3, 1, 6, 0, 5, 1, 5, 2, 1, 3, 1, 5,\n",
       "       1, 7, 7, 2, 3, 1, 1, 2, 3, 4, 1, 4, 7, 1, 3, 3, 5, 1, 0, 5, 1, 0,\n",
       "       4, 3, 1, 4, 3, 3, 7, 4, 3, 1, 3, 5, 7, 3, 3, 3, 5, 0, 2, 1, 1, 2,\n",
       "       6, 3, 5, 3, 4, 5, 1, 1, 2, 4, 3, 7, 5, 4, 2, 6, 7, 5, 6, 3, 3, 7,\n",
       "       5, 2, 3, 0, 5, 2, 3, 3, 5, 5, 2, 5, 3, 4, 2, 2, 3, 0, 3, 3, 2, 3,\n",
       "       1, 3, 1, 2, 7, 7, 1, 7, 6, 2, 1, 2, 0, 0, 2, 1, 4, 7, 3, 2, 7, 5,\n",
       "       2, 3, 3, 0, 4, 0, 2, 1, 5, 6, 6, 3, 1, 4, 5, 3, 3, 6, 6, 5, 7, 5,\n",
       "       1, 6, 2, 0, 0, 1, 7, 1, 3, 2, 2, 3, 5, 5, 3, 1, 4, 7, 2, 1, 4, 5,\n",
       "       2, 5, 6, 2, 3, 1, 1, 0, 5, 3, 3, 1, 4, 2, 1, 7, 5, 2, 1, 5, 2, 7,\n",
       "       1, 2, 3, 5, 1, 7, 2, 1, 0, 4, 2, 6, 4, 6, 7, 1, 3, 1, 5, 1, 2, 2,\n",
       "       3, 3, 3, 3, 3, 1, 4, 4, 1, 5, 2, 0, 7, 5, 7, 4, 2, 4, 4, 4, 3, 2,\n",
       "       0, 2, 7, 4, 2, 7, 0, 7, 3, 7, 5, 5, 1, 5, 3, 5, 3, 5, 3, 5, 4, 4,\n",
       "       1, 6, 5, 7, 3, 0, 1, 1, 3, 0, 1, 4, 4, 7, 0, 3, 3, 4, 3, 6, 2, 4,\n",
       "       5, 0, 4, 2, 0, 2, 3, 5, 5, 1, 1, 6, 7, 0, 1, 1, 4, 1, 5, 3, 4, 4,\n",
       "       5, 6, 4, 6, 4, 2, 2, 3, 5, 3, 3, 7, 1, 7, 2, 7, 5, 6, 6, 5, 5, 0,\n",
       "       5, 6, 2, 4, 3, 1, 2, 3, 4, 2, 1, 0, 3, 7, 3, 7, 2, 3, 3, 2, 7, 5,\n",
       "       7, 3, 1, 3, 5, 3, 1, 2, 1, 3, 1, 4, 5, 3, 1, 1, 5, 2, 4, 7, 5, 0,\n",
       "       4, 7, 1, 4, 3, 6, 5, 2, 7, 2, 0, 0, 3, 2, 1, 7, 5, 5, 1, 2, 5, 2,\n",
       "       2, 5, 5, 5, 1, 1, 3, 5, 2, 2, 3, 5, 4, 1, 4, 3, 7, 1, 0, 5, 1, 1,\n",
       "       5, 3, 5, 7, 5, 2, 2, 1, 0, 5, 0, 0, 3, 5, 3, 5, 2, 5, 4, 2, 7, 0,\n",
       "       6, 4, 0, 3, 2, 0, 2, 5, 4, 4, 5, 5, 0, 3, 0, 6, 1, 5, 4, 7, 0, 3,\n",
       "       2, 2, 5, 5, 2, 0, 0, 1, 5, 5, 0, 3, 3, 2, 1, 1, 3, 2, 7, 6, 2, 6,\n",
       "       7, 3, 7, 4, 2, 1, 5, 3, 5, 0, 1, 5, 6, 3, 4, 1, 1, 3, 7, 2, 0, 7,\n",
       "       2, 1, 5, 0, 2, 7, 1, 3, 0, 5, 1, 0, 1, 2, 3, 3, 3, 3, 2, 1, 3, 4,\n",
       "       2, 3, 4, 5, 2, 3, 5, 1, 4, 3, 1, 3, 3, 4, 3, 1, 1, 2, 3, 7, 1, 3,\n",
       "       3, 2, 4, 4, 7, 2, 2, 1, 1, 0, 2, 3, 2, 0, 2, 3, 6, 3, 2, 1, 5, 5,\n",
       "       3, 7, 5, 1, 2, 5, 3, 3, 4, 5, 0, 0, 1, 5, 5, 0, 2, 0, 2, 6, 0, 5,\n",
       "       3, 2, 5, 7, 2, 1, 7, 6, 0, 3, 5, 5, 4, 4, 3, 4, 1, 5, 1, 3, 2, 0,\n",
       "       3, 3, 2, 5, 5, 7, 0, 6, 6, 3, 5, 1, 2, 4, 2, 5, 1, 5, 5, 2, 0, 4,\n",
       "       5, 2, 0, 1, 5, 3, 5, 4, 7, 5, 1, 4, 5, 7, 2, 7, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HUHshx93CM_6",
    "outputId": "5b33758e-9a1a-403d-9679-19c0e6a2c0e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 1, 1, 1, 1, 2, 4, 4, 0, 6, 5, 2, 1, 3, 2, 4, 5, 3, 6, 4,\n",
       "       0, 1, 6, 4, 2, 6, 7, 7, 4, 5, 3, 7, 4, 4, 1, 3, 4, 3, 3, 5, 2, 2,\n",
       "       7, 4, 5, 3, 6, 3, 1, 4, 6, 3, 1, 4, 5, 0, 5, 2, 1, 3, 5, 3, 1, 3,\n",
       "       2, 4, 6, 3, 4, 2, 6, 5, 5, 2, 2, 0, 5, 3, 7, 3, 3, 1, 4, 4, 0, 4,\n",
       "       3, 5, 1, 5, 4, 2, 2, 1, 3, 5, 4, 7, 6, 2, 1, 7, 3, 2, 7, 2, 3, 4,\n",
       "       7, 4, 4, 5, 1, 1, 0, 1, 7, 1, 2, 1, 6, 0, 2, 2, 3, 2, 2, 3, 5, 0,\n",
       "       4, 6, 0, 7, 3, 7, 7, 1, 5, 1, 2, 7, 4, 4, 5, 2, 2, 4, 6, 7, 7, 2,\n",
       "       6, 5, 3, 6, 2, 4, 4, 1, 4, 4, 3, 1, 6, 0, 5, 1, 5, 2, 1, 5, 1, 2,\n",
       "       1, 7, 2, 2, 6, 1, 2, 2, 1, 1, 1, 4, 7, 1, 3, 3, 5, 3, 0, 5, 1, 0,\n",
       "       4, 1, 1, 4, 3, 3, 7, 4, 3, 1, 3, 4, 7, 0, 5, 3, 5, 0, 2, 1, 1, 2,\n",
       "       6, 1, 5, 3, 4, 2, 3, 0, 2, 4, 3, 5, 5, 4, 2, 7, 4, 5, 3, 3, 0, 7,\n",
       "       1, 5, 3, 0, 4, 3, 3, 3, 2, 5, 2, 4, 2, 4, 3, 2, 3, 0, 3, 4, 5, 3,\n",
       "       1, 3, 1, 5, 2, 7, 1, 7, 4, 1, 1, 2, 0, 0, 2, 1, 4, 5, 5, 2, 6, 5,\n",
       "       5, 3, 3, 0, 4, 0, 2, 1, 5, 6, 6, 5, 1, 4, 2, 0, 0, 1, 6, 3, 7, 0,\n",
       "       1, 7, 2, 1, 2, 1, 6, 1, 3, 2, 6, 3, 5, 5, 3, 3, 4, 7, 7, 1, 4, 5,\n",
       "       2, 5, 6, 2, 4, 1, 1, 4, 5, 3, 0, 1, 4, 2, 1, 7, 5, 2, 1, 5, 2, 7,\n",
       "       1, 2, 3, 5, 1, 7, 2, 4, 0, 4, 3, 6, 4, 6, 6, 1, 3, 1, 5, 1, 2, 2,\n",
       "       3, 2, 3, 3, 3, 1, 4, 4, 1, 2, 2, 0, 7, 5, 4, 4, 2, 4, 4, 4, 7, 4,\n",
       "       1, 2, 7, 2, 2, 3, 7, 7, 3, 4, 5, 5, 1, 5, 3, 5, 3, 4, 1, 5, 4, 4,\n",
       "       3, 6, 5, 7, 3, 0, 1, 0, 3, 0, 1, 4, 4, 7, 0, 1, 3, 2, 3, 6, 2, 4,\n",
       "       2, 6, 4, 2, 7, 2, 3, 5, 1, 1, 1, 7, 5, 0, 1, 1, 4, 1, 5, 3, 4, 4,\n",
       "       5, 6, 4, 6, 4, 2, 2, 0, 5, 3, 5, 7, 3, 7, 4, 3, 5, 6, 6, 5, 5, 0,\n",
       "       5, 6, 4, 4, 3, 1, 2, 3, 6, 2, 0, 1, 3, 4, 3, 5, 7, 5, 3, 2, 2, 5,\n",
       "       7, 0, 1, 1, 2, 3, 6, 2, 1, 3, 1, 4, 5, 3, 1, 1, 5, 7, 5, 7, 4, 7,\n",
       "       5, 4, 1, 4, 1, 6, 5, 2, 3, 2, 6, 0, 3, 2, 4, 7, 5, 2, 1, 2, 5, 2,\n",
       "       2, 2, 0, 5, 1, 1, 3, 6, 5, 2, 3, 2, 4, 2, 4, 1, 7, 3, 7, 5, 1, 1,\n",
       "       2, 3, 3, 3, 5, 2, 2, 1, 2, 5, 6, 0, 6, 5, 3, 4, 7, 5, 4, 2, 4, 1,\n",
       "       4, 4, 2, 3, 2, 2, 2, 3, 4, 4, 4, 5, 0, 3, 6, 6, 1, 5, 2, 7, 4, 3,\n",
       "       7, 4, 5, 5, 2, 0, 0, 1, 5, 5, 4, 3, 5, 2, 1, 1, 3, 4, 7, 6, 2, 6,\n",
       "       7, 3, 6, 4, 2, 1, 5, 5, 6, 0, 1, 5, 3, 4, 4, 1, 1, 4, 4, 4, 3, 7,\n",
       "       2, 1, 5, 1, 6, 7, 1, 5, 1, 5, 1, 0, 1, 2, 3, 3, 1, 3, 4, 1, 3, 4,\n",
       "       5, 3, 6, 6, 6, 0, 5, 1, 4, 3, 1, 1, 3, 4, 3, 1, 6, 2, 3, 7, 1, 3,\n",
       "       3, 2, 4, 2, 5, 2, 6, 1, 1, 0, 7, 6, 2, 0, 2, 3, 7, 3, 5, 1, 5, 5,\n",
       "       5, 6, 5, 1, 2, 5, 3, 4, 4, 5, 0, 0, 1, 5, 5, 0, 2, 0, 7, 6, 2, 5,\n",
       "       5, 4, 5, 7, 5, 1, 7, 6, 4, 5, 5, 5, 4, 4, 3, 4, 1, 5, 1, 5, 2, 0,\n",
       "       3, 3, 2, 5, 4, 6, 0, 2, 6, 6, 5, 1, 2, 4, 2, 3, 1, 5, 5, 2, 1, 4,\n",
       "       5, 7, 0, 1, 2, 3, 5, 4, 2, 0, 6, 4, 5, 0, 2, 7, 3, 6])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMxojpvWCxOs"
   },
   "outputs": [],
   "source": [
    "new_Ytest = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W07EQaC8DE6i",
    "outputId": "9e7d7f0f-8cd3-4068-e42e-4e23c22f8a71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 1, 1, 1, 1, 2, 4, 4, 0, 6, 5, 2, 1, 3, 2, 4, 5, 3, 6, 4,\n",
       "       0, 1, 6, 4, 2, 6, 7, 7, 4, 5, 3, 7, 4, 4, 1, 3, 4, 3, 3, 5, 2, 2,\n",
       "       7, 4, 5, 3, 6, 3, 1, 4, 6, 3, 1, 4, 5, 0, 5, 2, 1, 3, 5, 3, 1, 3,\n",
       "       2, 4, 6, 3, 4, 2, 6, 5, 5, 2, 2, 0, 5, 3, 7, 3, 3, 1, 4, 4, 0, 4,\n",
       "       3, 5, 1, 5, 4, 2, 2, 1, 3, 5, 4, 7, 6, 2, 1, 7, 3, 2, 7, 2, 3, 4,\n",
       "       7, 4, 4, 5, 1, 1, 0, 1, 7, 1, 2, 1, 6, 0, 2, 2, 3, 2, 2, 3, 5, 0,\n",
       "       4, 6, 0, 7, 3, 7, 7, 1, 5, 1, 2, 7, 4, 4, 5, 2, 2, 4, 6, 7, 7, 2,\n",
       "       6, 5, 3, 6, 2, 4, 4, 1, 4, 4, 3, 1, 6, 0, 5, 1, 5, 2, 1, 5, 1, 2,\n",
       "       1, 7, 2, 2, 6, 1, 2, 2, 1, 1, 1, 4, 7, 1, 3, 3, 5, 3, 0, 5, 1, 0,\n",
       "       4, 1, 1, 4, 3, 3, 7, 4, 3, 1, 3, 4, 7, 0, 5, 3, 5, 0, 2, 1, 1, 2,\n",
       "       6, 1, 5, 3, 4, 2, 3, 0, 2, 4, 3, 5, 5, 4, 2, 7, 4, 5, 3, 3, 0, 7,\n",
       "       1, 5, 3, 0, 4, 3, 3, 3, 2, 5, 2, 4, 2, 4, 3, 2, 3, 0, 3, 4, 5, 3,\n",
       "       1, 3, 1, 5, 2, 7, 1, 7, 4, 1, 1, 2, 0, 0, 2, 1, 4, 5, 5, 2, 6, 5,\n",
       "       5, 3, 3, 0, 4, 0, 2, 1, 5, 6, 6, 5, 1, 4, 2, 0, 0, 1, 6, 3, 7, 0,\n",
       "       1, 7, 2, 1, 2, 1, 6, 1, 3, 2, 6, 3, 5, 5, 3, 3, 4, 7, 7, 1, 4, 5,\n",
       "       2, 5, 6, 2, 4, 1, 1, 4, 5, 3, 0, 1, 4, 2, 1, 7, 5, 2, 1, 5, 2, 7,\n",
       "       1, 2, 3, 5, 1, 7, 2, 4, 0, 4, 3, 6, 4, 6, 6, 1, 3, 1, 5, 1, 2, 2,\n",
       "       3, 2, 3, 3, 3, 1, 4, 4, 1, 2, 2, 0, 7, 5, 4, 4, 2, 4, 4, 4, 7, 4,\n",
       "       1, 2, 7, 2, 2, 3, 7, 7, 3, 4, 5, 5, 1, 5, 3, 5, 3, 4, 1, 5, 4, 4,\n",
       "       3, 6, 5, 7, 3, 0, 1, 0, 3, 0, 1, 4, 4, 7, 0, 1, 3, 2, 3, 6, 2, 4,\n",
       "       2, 6, 4, 2, 7, 2, 3, 5, 1, 1, 1, 7, 5, 0, 1, 1, 4, 1, 5, 3, 4, 4,\n",
       "       5, 6, 4, 6, 4, 2, 2, 0, 5, 3, 5, 7, 3, 7, 4, 3, 5, 6, 6, 5, 5, 0,\n",
       "       5, 6, 4, 4, 3, 1, 2, 3, 6, 2, 0, 1, 3, 4, 3, 5, 7, 5, 3, 2, 2, 5,\n",
       "       7, 0, 1, 1, 2, 3, 6, 2, 1, 3, 1, 4, 5, 3, 1, 1, 5, 7, 5, 7, 4, 7,\n",
       "       5, 4, 1, 4, 1, 6, 5, 2, 3, 2, 6, 0, 3, 2, 4, 7, 5, 2, 1, 2, 5, 2,\n",
       "       2, 2, 0, 5, 1, 1, 3, 6, 5, 2, 3, 2, 4, 2, 4, 1, 7, 3, 7, 5, 1, 1,\n",
       "       2, 3, 3, 3, 5, 2, 2, 1, 2, 5, 6, 0, 6, 5, 3, 4, 7, 5, 4, 2, 4, 1,\n",
       "       4, 4, 2, 3, 2, 2, 2, 3, 4, 4, 4, 5, 0, 3, 6, 6, 1, 5, 2, 7, 4, 3,\n",
       "       7, 4, 5, 5, 2, 0, 0, 1, 5, 5, 4, 3, 5, 2, 1, 1, 3, 4, 7, 6, 2, 6,\n",
       "       7, 3, 6, 4, 2, 1, 5, 5, 6, 0, 1, 5, 3, 4, 4, 1, 1, 4, 4, 4, 3, 7,\n",
       "       2, 1, 5, 1, 6, 7, 1, 5, 1, 5, 1, 0, 1, 2, 3, 3, 1, 3, 4, 1, 3, 4,\n",
       "       5, 3, 6, 6, 6, 0, 5, 1, 4, 3, 1, 1, 3, 4, 3, 1, 6, 2, 3, 7, 1, 3,\n",
       "       3, 2, 4, 2, 5, 2, 6, 1, 1, 0, 7, 6, 2, 0, 2, 3, 7, 3, 5, 1, 5, 5,\n",
       "       5, 6, 5, 1, 2, 5, 3, 4, 4, 5, 0, 0, 1, 5, 5, 0, 2, 0, 7, 6, 2, 5,\n",
       "       5, 4, 5, 7, 5, 1, 7, 6, 4, 5, 5, 5, 4, 4, 3, 4, 1, 5, 1, 5, 2, 0,\n",
       "       3, 3, 2, 5, 4, 6, 0, 2, 6, 6, 5, 1, 2, 4, 2, 3, 1, 5, 5, 2, 1, 4,\n",
       "       5, 7, 0, 1, 2, 3, 5, 4, 2, 0, 6, 4, 5, 0, 2, 7, 3, 6])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hu1S5IowfSDG"
   },
   "source": [
    "Now, the confusion matrix: it will show us the misclassified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "fdy09SCEd7Cl",
    "outputId": "4fd020f5-74c5-40e7-8b54-076c944901be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 40   3   0   9   0   3   0   2]\n",
      " [ 10 100   4  12   1   2   1   0]\n",
      " [  7   3  87   4   5  14   1   5]\n",
      " [  2   7   3  97   0   8   2   4]\n",
      " [  5   2  10   5  80   9   2   9]\n",
      " [  0   0  10  18   2  89   0   5]\n",
      " [  6   3   4   4   5   3  31   7]\n",
      " [  5   0  10   2   2   0   5  41]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(new_Ytest, predictions)\n",
    "print (matrix)\n",
    "\n",
    "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Recall Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.70      0.61        57\n",
      "           1       0.85      0.77      0.81       130\n",
      "           2       0.68      0.69      0.69       126\n",
      "           3       0.64      0.79      0.71       123\n",
      "           4       0.84      0.66      0.74       122\n",
      "           5       0.70      0.72      0.71       124\n",
      "           6       0.74      0.49      0.59        63\n",
      "           7       0.56      0.63      0.59        65\n",
      "\n",
      "    accuracy                           0.70       810\n",
      "   macro avg       0.69      0.68      0.68       810\n",
      "weighted avg       0.71      0.70      0.70       810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(new_Ytest, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_ySPOyHxkZ3"
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f5kRmoD-sdHj",
    "outputId": "99ad6a5b-a4a6-42bc-ed78-229864c33d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at ../../model/Emotion_Voice_Detection_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = '../../model/'\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EmotionsRecognition.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "43a24ecce625020f2d6631fb4cfb730bba30d877e9fe9ec2e0d85cb5a52a2b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
